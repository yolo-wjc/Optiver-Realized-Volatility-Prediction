{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75f62085",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-05-25T04:14:40.099408Z",
     "iopub.status.busy": "2023-05-25T04:14:40.098804Z",
     "iopub.status.idle": "2023-05-25T04:14:43.677894Z",
     "shell.execute_reply": "2023-05-25T04:14:43.676274Z"
    },
    "papermill": {
     "duration": 3.596726,
     "end_time": "2023-05-25T04:14:43.680815",
     "exception": false,
     "start_time": "2023-05-25T04:14:40.084089",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "from joblib import Parallel, delayed\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sc\n",
    "from sklearn.model_selection import KFold\n",
    "import lightgbm as lgb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from tqdm import tqdm\n",
    "import joblib\n",
    "from catboost import CatBoostRegressor\n",
    "import catboost\n",
    "from scipy.stats import kurtosis\n",
    "from scipy.stats import skew\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2758ce23",
   "metadata": {
    "papermill": {
     "duration": 0.010721,
     "end_time": "2023-05-25T04:14:43.702468",
     "exception": false,
     "start_time": "2023-05-25T04:14:43.691747",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# LightGBM和CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2995103",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-25T04:14:43.728605Z",
     "iopub.status.busy": "2023-05-25T04:14:43.727785Z",
     "iopub.status.idle": "2023-05-25T04:14:43.813550Z",
     "shell.execute_reply": "2023-05-25T04:14:43.812144Z"
    },
    "papermill": {
     "duration": 0.103216,
     "end_time": "2023-05-25T04:14:43.816962",
     "exception": false,
     "start_time": "2023-05-25T04:14:43.713746",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 数据集目录\n",
    "data_dir = '../input/optiver-realized-volatility-prediction/'\n",
    "\n",
    "# 对数回报率\n",
    "def log_return(series):\n",
    "    return np.log(series).diff()\n",
    "\n",
    "# 基于最有竞争力价格和第二竞争力价格的加权平均价格\n",
    "def calc_wap1(df):\n",
    "    wap = (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']) / (df['bid_size1'] + df['ask_size1'])\n",
    "    return wap\n",
    "def calc_wap2(df):\n",
    "    wap = (df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2']) / (df['bid_size2'] + df['ask_size2'])\n",
    "    return wap\n",
    "\n",
    "# 已实现波动率\n",
    "def realized_volatility(series):\n",
    "    return np.sqrt(np.sum(series**2))\n",
    "\n",
    "# 返回数据基数\n",
    "def count_unique(series):\n",
    "    return len(np.unique(series))\n",
    "\n",
    "# 订单深度\n",
    "def calc_depth(df):\n",
    "    depth = df['bid_price1'] * df['bid_size1'] + df['ask_price1'] * df['ask_size1'] + df['bid_price2'] * df[\n",
    "               'bid_size2'] + df['ask_price2'] * df['ask_size2']\n",
    "    return depth\n",
    "\n",
    "# 订单斜率\n",
    "def calc_slope(df):\n",
    "    v0 = (df['bid_size1']+df['ask_size1'])/2\n",
    "    p0 = (df['bid_price1']+df['ask_price1'])/2\n",
    "    slope_bid = ((df['bid_size1']/v0)-1)/abs((df['bid_price1']/p0)-1)+(\n",
    "                (df['bid_size2']/df['bid_size1'])-1)/abs((df['bid_price2']/df['bid_price1'])-1)\n",
    "    slope_ask = ((df['ask_size1']/v0)-1)/abs((df['ask_price1']/p0)-1)+(\n",
    "                (df['ask_size2']/df['ask_size1'])-1)/abs((df['ask_price2']/df['ask_price1'])-1)\n",
    "    return (slope_bid+slope_ask)/2, abs(slope_bid-slope_ask)\n",
    "\n",
    "# 读取数据\n",
    "def read():\n",
    "    train = pd.read_csv('../input/optiver-realized-volatility-prediction/train.csv')\n",
    "    test = pd.read_csv('../input/optiver-realized-volatility-prediction/test.csv')\n",
    "    # 创建合并索引\n",
    "    train['row_id'] = train['stock_id'].astype(str) + '-' + train['time_id'].astype(str)\n",
    "    test['row_id'] = test['stock_id'].astype(str) + '-' + test['time_id'].astype(str)\n",
    "    print(f'训练集行数{train.shape[0]}')\n",
    "    print(f'测试集行数{test.shape[0]}')\n",
    "    return train, test\n",
    "\n",
    "# 处理订单本数据\n",
    "\n",
    "def book_preprocessor(file_path):\n",
    "    df = pd.read_parquet(file_path)\n",
    "    # 特征衍生\n",
    "    df['wap1'] = calc_wap1(df)\n",
    "    df['wap2'] = calc_wap2(df)\n",
    "    df['log_return1'] = df.groupby(['time_id'], group_keys=False)['wap1'].apply(log_return)\n",
    "    df['log_return2'] = df.groupby(['time_id'], group_keys=False)['wap2'].apply(log_return)\n",
    "    df['wap_balance'] = abs(df['wap1'] - df['wap2'])\n",
    "    df['price_spread_1'] = (df['ask_price1'] - df['bid_price1'])/df['ask_price1']\n",
    "    df['price_spread_2'] = (df['ask_price2'] - df['bid_price2'])/df['ask_price2']\n",
    "    df['log_ask1'] = np.log(df['ask_size1'])\n",
    "    df['log_ask2'] = np.log(df['ask_size2']) \n",
    "    df['log_bid1'] = np.log(df['bid_size1']) \n",
    "    df['log_bid2'] = np.log(df['bid_size2']) \n",
    "    df['log_ask_total'] = np.log(df['ask_size1']+df['ask_size2']) \n",
    "    df['log_bid_total'] = np.log(df['bid_size1']+df['bid_size2']) \n",
    "    df['log_askplusbid'] =  df['log_ask_total']+df['log_bid_total']\n",
    "    df['log_askdbid'] =  df['log_ask_total']/df['log_bid_total']\n",
    "    df['price_spread'] = (df['ask_price1'] - df['bid_price1']) / ((df['ask_price1'] + df['bid_price1']) / 2)\n",
    "    df['price_spread2'] = (df['ask_price2'] - df['bid_price2']) / ((df['ask_price2'] + df['bid_price2']) / 2)\n",
    "    df['bid_spread'] = df['bid_price1'] - df['bid_price2']\n",
    "    df['ask_spread'] = df['ask_price1'] - df['ask_price2']\n",
    "    df['total_volume'] = (df['ask_size1'] + df['ask_size2']) + (df['bid_size1'] + df['bid_size2'])\n",
    "    df['apply_require'] = (df['ask_size1'] + df['ask_size2']) - (df['bid_size1'] + df['bid_size2'])\n",
    "    df['volume_imbalance'] = abs((df['ask_size1'] + df['ask_size2']) - (df['bid_size1'] + df['bid_size2']))\n",
    "    df['bid_vol1'] = np.prod([df['bid_price1'], df['bid_size1']])\n",
    "    df['bid_vol2'] = np.prod([df['bid_price2'], df['bid_size2']])\n",
    "    df['ask_vol1'] = np.prod([df['ask_price1'], df['ask_size1']])\n",
    "    df['ask_vol2'] = np.prod([df['ask_price2'], df['ask_size2']])\n",
    "    df['price1_abs'] = df['bid_spread'].abs()\n",
    "    df['price2_abs'] = df['ask_spread'].abs()\n",
    "    df['price_spread1'] = (df['ask_price1'] - df['bid_price1']) / (df['ask_price1'] + df['bid_price1'])\n",
    "    df['depth']=calc_depth(df)\n",
    "    df['slope1'],df['slope2']=calc_slope(df)\n",
    "    # 窗口聚合字典\n",
    "    create_feature_dict = {\n",
    "        'wap1': [np.mean, np.std],\n",
    "        'wap2': [np.mean, np.std],\n",
    "        'log_return1': [realized_volatility, np.std, kurtosis, skew,np.max,np.min],\n",
    "        'log_return2': [realized_volatility, np.std, kurtosis, skew,np.max,np.min],\n",
    "        'log_ask1': [np.mean, np.std,np.max,np.min],\n",
    "        'log_ask2': [np.mean, np.std],\n",
    "        'log_bid1': [np.mean, np.std,np.max,np.min],\n",
    "        'log_bid2': [np.mean, np.std],\n",
    "        'log_ask_total': [np.mean, np.std,np.max,np.min],\n",
    "        'log_bid_total': [np.mean, np.std,np.max,np.min],\n",
    "        'log_askplusbid': [np.mean, np.std,np.max,np.min],\n",
    "        'log_askdbid': [np.mean, np.std,np.max,np.min],\n",
    "        'wap_balance': [np.mean, np.std,np.max,np.min],\n",
    "        'price_spread_1':[np.mean, np.std,np.max,np.min],\n",
    "        'price_spread_2':[np.mean, np.std,np.max,np.min],\n",
    "        'price_spread':[ np.mean, np.std,np.max,np.min],\n",
    "        'price_spread1':[ np.mean, np.std,np.max,np.min],\n",
    "        'price_spread2':[ np.mean, np.std,np.max,np.min],\n",
    "        'bid_spread':[ np.mean, np.std,np.max,np.min],\n",
    "        'ask_spread':[np.mean, np.std,np.max,np.min],\n",
    "        'total_volume':[np.mean, np.std,np.max,np.min],\n",
    "        'volume_imbalance':[np.mean, np.std,np.max,np.min],\n",
    "        'bid_vol1':[np.mean, np.std,np.max,np.min],\n",
    "        'bid_vol2':[np.mean, np.std],\n",
    "        'ask_vol1':[np.mean, np.std,np.max,np.min],\n",
    "        'ask_vol2':[np.mean, np.std],\n",
    "        'depth':[np.mean, np.std,np.max,np.min],\n",
    "        'slope1':[np.mean, np.std,np.max,np.min],\n",
    "        'slope2':[np.mean, np.std,np.max,np.min],\n",
    "        'apply_require':[ np.mean, np.std,np.max,np.min]\n",
    "    }\n",
    "    \n",
    "    #分组窗口聚合\n",
    "    def get_stats_window(seconds_in_bucket, add_suffix = False):\n",
    "\n",
    "        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(create_feature_dict).reset_index()\n",
    "        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n",
    "        if add_suffix:\n",
    "            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n",
    "        return df_feature\n",
    "    \n",
    "    \n",
    "    \n",
    "    df_feature = get_stats_window(seconds_in_bucket = 0, add_suffix = False)\n",
    "    df_feature_300 = get_stats_window(seconds_in_bucket = 300, add_suffix = True)\n",
    "    df_feature_450 = get_stats_window(seconds_in_bucket = 450, add_suffix = True)\n",
    "    df_feature_550 = get_stats_window(seconds_in_bucket = 550, add_suffix = True)\n",
    "    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n",
    "    df_feature = df_feature.merge(df_feature_450, how = 'left', left_on = 'time_id_', right_on = 'time_id__450')\n",
    "    df_feature = df_feature.merge(df_feature_550, how = 'left', left_on = 'time_id_', right_on = 'time_id__550')\n",
    "    df_feature.drop(['time_id__300', 'time_id__450', 'time_id__550'], axis = 1, inplace = True)\n",
    "\n",
    "    \n",
    "    # 创建合并索引\n",
    "    stock_id = file_path.split('=')[1]\n",
    "    df_feature['row_id'] = df_feature['time_id_'].apply(lambda x: f'{stock_id}-{x}')\n",
    "    df_feature.drop(['time_id_'], axis = 1, inplace = True)\n",
    "    return df_feature\n",
    "\n",
    "# 处理交易数据\n",
    "def trade_preprocessor(file_path):\n",
    "    df = pd.read_parquet(file_path)\n",
    "    df['log_return'] = df.groupby('time_id', group_keys=False)['price'].apply(log_return)\n",
    "    df['amount']=df['price']*df['size']\n",
    "    df['vol'] = np.prod([df['price'], df['size']])\n",
    "# 窗口聚合字典\n",
    "    create_feature_dict = {\n",
    "        'log_return':[realized_volatility, np.mean, np.std, kurtosis, skew],\n",
    "        'seconds_in_bucket':[count_unique],\n",
    "        'size':[np.sum, np.mean, np.std, np.max, np.min, kurtosis, skew],\n",
    "        'order_count':[np.sum, np.mean, np.std, np.max, np.min, kurtosis, skew],\n",
    "        'amount':[np.sum, np.mean, np.std, np.max, np.min, kurtosis, skew]\n",
    "    }\n",
    "    \n",
    "     #分组窗口聚合\n",
    "    def get_stats_window(seconds_in_bucket, add_suffix = False):\n",
    "        # 以时间窗口分组统计\n",
    "        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(create_feature_dict).reset_index()\n",
    "        # 重命名\n",
    "        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n",
    "        if add_suffix:\n",
    "            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n",
    "        return df_feature\n",
    "    \n",
    "    \n",
    "    df_feature = get_stats_window(seconds_in_bucket = 0, add_suffix = False)\n",
    "    df_feature_300 = get_stats_window(seconds_in_bucket = 300, add_suffix = True)\n",
    "    df_feature_450 = get_stats_window(seconds_in_bucket = 450, add_suffix = True)\n",
    "    df_feature_550 = get_stats_window(seconds_in_bucket = 550, add_suffix = True)\n",
    "    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n",
    "    df_feature = df_feature.merge(df_feature_450, how = 'left', left_on = 'time_id_', right_on = 'time_id__450')\n",
    "    df_feature = df_feature.merge(df_feature_550, how = 'left', left_on = 'time_id_', right_on = 'time_id__550')\n",
    "    df_feature.drop(['time_id__300', 'time_id__450', 'time_id__550'], axis = 1, inplace = True)\n",
    "\n",
    "    \n",
    "    df_feature = df_feature.add_prefix('trade_')\n",
    "    stock_id = file_path.split('=')[1]\n",
    "    df_feature['row_id'] = df_feature['trade_time_id_'].apply(lambda x:f'{stock_id}-{x}')\n",
    "    df_feature.drop(['trade_time_id_'], axis = 1, inplace = True)\n",
    "    return df_feature\n",
    "\n",
    "\n",
    "def get_time_stock(df):\n",
    "    #需要计算统计特征的列名\n",
    "    vol_cols = ['log_return1_realized_volatility', 'log_return2_realized_volatility', 'log_return1_realized_volatility_300', 'log_return2_realized_volatility_300', \n",
    "                'log_return1_realized_volatility_450', 'log_return2_realized_volatility_450', 'log_return1_realized_volatility_550', 'log_return2_realized_volatility_550',\n",
    "                'trade_log_return_realized_volatility', 'trade_log_return_realized_volatility_300', 'trade_log_return_realized_volatility_450', 'trade_log_return_realized_volatility_550'\n",
    "               ,'trade_seconds_in_bucket_count_unique','trade_seconds_in_bucket_count_unique_450','trade_size_sum','trade_size_sum_300','trade_size_sum_450']\n",
    "\n",
    "    #stock_id聚合\n",
    "    df_stock_id = df.groupby(['stock_id'])[vol_cols].agg(['mean', 'std', 'max', 'min',lambda x: np.percentile(x, q=25), lambda x: np.percentile(x, q=75) ]).reset_index()\n",
    "    df_stock_id.columns = ['_'.join(col) for col in df_stock_id.columns]\n",
    "    df_stock_id = df_stock_id.add_suffix('_' + 'stock')\n",
    "    #time_id聚合 \n",
    "    df_time_id = df.groupby(['time_id'])[vol_cols].agg(['mean', 'std', 'max', 'min',lambda x: np.percentile(x, q=25), lambda x: np.percentile(x, q=75) ]).reset_index()\n",
    "    df_time_id.columns = ['_'.join(col) for col in df_time_id.columns]\n",
    "    df_time_id = df_time_id.add_suffix('_' + 'time')\n",
    "    \n",
    "\n",
    "    df = df.merge(df_stock_id, how = 'left', left_on = ['stock_id'], right_on = ['stock_id__stock'])\n",
    "    df = df.merge(df_time_id, how = 'left', left_on = ['time_id'], right_on = ['time_id__time'])\n",
    "    df.drop(['stock_id__stock', 'time_id__time'], axis = 1, inplace = True)\n",
    "    return df\n",
    "    \n",
    "# 处理函数集成\n",
    "def preprocessor(list_stock_ids, is_train = True):\n",
    "    \n",
    "    def for_joblib(stock_id):\n",
    "        # 训练集\n",
    "        if is_train:\n",
    "            file_path_book = data_dir + \"book_train.parquet/stock_id=\" + str(stock_id)\n",
    "            file_path_trade = data_dir + \"trade_train.parquet/stock_id=\" + str(stock_id)\n",
    "        # 测试集\n",
    "        else:\n",
    "            file_path_book = data_dir + \"book_test.parquet/stock_id=\" + str(stock_id)\n",
    "            file_path_trade = data_dir + \"trade_test.parquet/stock_id=\" + str(stock_id)\n",
    "    \n",
    "        # 合并订单本数据和交易数据\n",
    "        df_tmp = pd.merge(book_preprocessor(file_path_book), trade_preprocessor(file_path_trade), on = 'row_id', how = 'left')\n",
    "        \n",
    "        return df_tmp\n",
    "    \n",
    "      # 并行循环处理\n",
    "    df = Parallel(n_jobs = -1, verbose = 1)(delayed(for_joblib)(stock_id) for stock_id in tqdm(list_stock_ids))\n",
    "    df = pd.concat(df, ignore_index = True)\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def rmspe(y_true, y_pred):\n",
    "    return np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n",
    "\n",
    "def feval_rmspe(y_pred, lgb_train):\n",
    "    y_true = lgb_train.get_label()\n",
    "    return 'RMSPE', rmspe(y_true, y_pred), False\n",
    "\n",
    "def train_and_evaluate_LGB(train, test):\n",
    "    # 超参数\n",
    "    params = {\n",
    "      'objective': 'rmse',  \n",
    "      'boosting_type': 'gbdt',\n",
    "      'num_leaves': 100,\n",
    "      'n_jobs': -1,\n",
    "      'learning_rate': 0.1,\n",
    "      'feature_fraction': 0.8,\n",
    "      'bagging_fraction': 0.8,\n",
    "      'verbose': -1\n",
    "    }\n",
    "    \n",
    "    # 数据集分割\n",
    "    x = train.drop(['row_id', 'target', 'time_id'], axis = 1)\n",
    "    y = train['target']\n",
    "    x_test = test.drop(['row_id', 'time_id'], axis = 1)\n",
    "    x['stock_id'] = x['stock_id'].astype(int)\n",
    "    x_test['stock_id'] = x_test['stock_id'].astype(int)\n",
    "    \n",
    "    # 每一折训练后得到的验证集预测结果的集合\n",
    "    oof_predictions = np.zeros(x.shape[0])\n",
    "    # 每一折训练后对测试集的结果进行的预测的集合\n",
    "    test_predictions = np.zeros(x_test.shape[0])\n",
    "    # 5折交叉验证\n",
    "    kfold = KFold(n_splits = 5, random_state = 66, shuffle = True)\n",
    "    for fold, (trn_ind, val_ind) in enumerate(kfold.split(x)):\n",
    "#         print(f'Training fold {fold + 1}')\n",
    "#         x_train, x_val = x.iloc[trn_ind], x.iloc[val_ind]\n",
    "#         y_train, y_val = y.iloc[trn_ind], y.iloc[val_ind]\n",
    "\n",
    "#         train_weights = 1 / np.square(y_train)\n",
    "#         val_weights = 1 / np.square(y_val)\n",
    "#         train_dataset = lgb.Dataset(x_train, y_train, weight = train_weights, categorical_feature = ['stock_id'])\n",
    "#         val_dataset = lgb.Dataset(x_val, y_val, weight = val_weights, categorical_feature = ['stock_id'])\n",
    "        \n",
    "#         model = lgb.train(params = params, \n",
    "#                         train_set = train_dataset, \n",
    "#                         valid_sets = [train_dataset, val_dataset], \n",
    "#                         num_boost_round = 10000, \n",
    "#                         early_stopping_rounds = 50, \n",
    "#                         verbose_eval = 50,\n",
    "#                         feval = feval_rmspe)\n",
    "#         joblib.dump(model, f\"LGB_{fold}.pkl\")\n",
    "        \n",
    "     model = joblib.load(f'../input/optiver-lgb-cat/LGB_{fold}(1).pkl')\n",
    "#oof_predictions[val_ind] = model.predict(x_val)\n",
    "  \n",
    "     test_predictions += np.abs(model.predict(x_test)) / 5\n",
    "        \n",
    "#     rmspe_score = rmspe(y, oof_predictions)\n",
    "#     print(f'每一折的RMSPE为 {rmspe_score}')\n",
    "\n",
    "    return test_predictions\n",
    "\n",
    "def train_and_evaluate_CAT(train, test):\n",
    "    # 超参数\n",
    "    params = {\n",
    "      'objective': 'RMSE',  \n",
    "      'verbose': True,\n",
    "      'iterations': 3000, \n",
    "      'early_stopping_rounds': 30,\n",
    "      'eval_metric': 'RMSE',\n",
    "      'subsample': 0.8,\n",
    "      'random_seed': 2023,\n",
    "    }\n",
    "\n",
    "    # 数据集分割\n",
    "    x = train.drop(['row_id', 'target', 'time_id'], axis=1)\n",
    "    y = train['target']\n",
    "    x_test = test.drop(['row_id', 'time_id'], axis=1)\n",
    "    x['stock_id'] = x['stock_id'].astype(int)\n",
    "    x_test['stock_id'] = x_test['stock_id'].astype(int)\n",
    "\n",
    "    # 每一折训练后得到的验证集预测结果的集合\n",
    "    oof_predictions = np.zeros(x.shape[0])\n",
    "    # 每一折训练后对测试集的结果进行的预测的集合\n",
    "    test_predictions = np.zeros(x_test.shape[0])\n",
    "    # 5折交叉验证\n",
    "    kfold = KFold(n_splits=5, random_state=11, shuffle=True)\n",
    "\n",
    "    for fold, (trn_ind, val_ind) in enumerate(kfold.split(x)):\n",
    "#         print(f'Training fold {fold + 1}')\n",
    "#         x_train, x_val = x.iloc[trn_ind], x.iloc[val_ind]\n",
    "#         y_train, y_val = y.iloc[trn_ind], y.iloc[val_ind]\n",
    "\n",
    "#         train_weights = 1 / np.square(y_train)\n",
    "#         val_weights = 1 / np.square(y_val)\n",
    "\n",
    "#         train_pool = catboost.Pool(x_train, y_train, weight=train_weights, cat_features=['stock_id'])\n",
    "#         val_pool = catboost.Pool(x_val, y_val, weight=val_weights, cat_features=['stock_id'])\n",
    "\n",
    "#         model = CatBoostRegressor(**params)\n",
    "#         model.fit(train_pool, eval_set=val_pool, verbose_eval=50)\n",
    "#         joblib.dump(model, f\"CAT_{fold}.pkl\")\n",
    "        model = joblib.load(f'../input/optiver-lgb-cat/CAT_{fold}(1).pkl')\n",
    "\n",
    "#         oof_predictions[val_ind] = model.predict(x_val)\n",
    "\n",
    "        test_predictions += np.abs(model.predict(x_test)) / 5\n",
    "\n",
    "#     rmspe_score = rmspe(y, oof_predictions)\n",
    "#     print(f'每一折的RMSPE为 {rmspe_score}')\n",
    "\n",
    "    return test_predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45801a67",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-25T04:14:43.842886Z",
     "iopub.status.busy": "2023-05-25T04:14:43.842407Z",
     "iopub.status.idle": "2023-05-25T04:14:45.019015Z",
     "shell.execute_reply": "2023-05-25T04:14:45.017321Z"
    },
    "papermill": {
     "duration": 1.194374,
     "end_time": "2023-05-25T04:14:45.022750",
     "exception": false,
     "start_time": "2023-05-25T04:14:43.828376",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集行数428932\n",
      "测试集行数3\n"
     ]
    }
   ],
   "source": [
    "train, test = read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a21de8e1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-25T04:14:45.048113Z",
     "iopub.status.busy": "2023-05-25T04:14:45.046757Z",
     "iopub.status.idle": "2023-05-25T04:14:45.053018Z",
     "shell.execute_reply": "2023-05-25T04:14:45.051551Z"
    },
    "papermill": {
     "duration": 0.022404,
     "end_time": "2023-05-25T04:14:45.056352",
     "exception": false,
     "start_time": "2023-05-25T04:14:45.033948",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train_stock_ids = train['stock_id'].unique()\n",
    "# train_ = preprocessor(train_stock_ids, is_train = True)\n",
    "# train = train.merge(train_, on = ['row_id'], how = 'left')\n",
    "# train = get_time_stock(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ee696b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-25T04:14:45.081020Z",
     "iopub.status.busy": "2023-05-25T04:14:45.080340Z",
     "iopub.status.idle": "2023-05-25T04:15:05.444966Z",
     "shell.execute_reply": "2023-05-25T04:15:05.443912Z"
    },
    "papermill": {
     "duration": 20.380603,
     "end_time": "2023-05-25T04:15:05.448110",
     "exception": false,
     "start_time": "2023-05-25T04:14:45.067507",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train.to_parquet('train-lgb-cat.parquet')\n",
    "train = pd.read_parquet('../input/optiver1/train-lgb-cat.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7ae489a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-25T04:15:05.473378Z",
     "iopub.status.busy": "2023-05-25T04:15:05.472838Z",
     "iopub.status.idle": "2023-05-25T04:15:08.357255Z",
     "shell.execute_reply": "2023-05-25T04:15:08.355240Z"
    },
    "papermill": {
     "duration": 2.901887,
     "end_time": "2023-05-25T04:15:08.361388",
     "exception": false,
     "start_time": "2023-05-25T04:15:05.459501",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s][Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "100%|██████████| 1/1 [00:00<00:00, 10.87it/s]\n",
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    2.3s finished\n"
     ]
    }
   ],
   "source": [
    "test_stock_ids = test['stock_id'].unique()\n",
    "test_ = preprocessor(test_stock_ids, is_train = False)\n",
    "test = test.merge(test_, on = ['row_id'], how = 'left')\n",
    "test = get_time_stock(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7444cac0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-25T04:15:08.389140Z",
     "iopub.status.busy": "2023-05-25T04:15:08.388639Z",
     "iopub.status.idle": "2023-05-25T04:15:12.738581Z",
     "shell.execute_reply": "2023-05-25T04:15:12.737436Z"
    },
    "papermill": {
     "duration": 4.366533,
     "end_time": "2023-05-25T04:15:12.741313",
     "exception": false,
     "start_time": "2023-05-25T04:15:08.374780",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_predictions_l = train_and_evaluate_LGB(train, test)\n",
    "test_predictions_c = train_and_evaluate_CAT(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6659fdab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-25T04:15:12.766403Z",
     "iopub.status.busy": "2023-05-25T04:15:12.765145Z",
     "iopub.status.idle": "2023-05-25T04:15:13.012617Z",
     "shell.execute_reply": "2023-05-25T04:15:13.011644Z"
    },
    "papermill": {
     "duration": 0.262437,
     "end_time": "2023-05-25T04:15:13.015103",
     "exception": false,
     "start_time": "2023-05-25T04:15:12.752666",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce9cd35",
   "metadata": {
    "papermill": {
     "duration": 0.01166,
     "end_time": "2023-05-25T04:15:13.038476",
     "exception": false,
     "start_time": "2023-05-25T04:15:13.026816",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70214ad4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-25T04:15:13.063175Z",
     "iopub.status.busy": "2023-05-25T04:15:13.062677Z",
     "iopub.status.idle": "2023-05-25T04:15:23.504546Z",
     "shell.execute_reply": "2023-05-25T04:15:23.503295Z"
    },
    "papermill": {
     "duration": 10.457621,
     "end_time": "2023-05-25T04:15:23.507328",
     "exception": false,
     "start_time": "2023-05-25T04:15:13.049707",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from numpy.random import seed\n",
    "import tensorflow as tf\n",
    "from sklearn.pipeline import Pipeline\n",
    "from collections import Counter, defaultdict\n",
    "from keras.layers import Activation\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "from keras import backend as K\n",
    "from keras.backend import sigmoid\n",
    "import random\n",
    "from sklearn import preprocessing, model_selection\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler,LabelEncoder\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "def swish(x, beta = 1):\n",
    "    return (x * sigmoid(beta * x))\n",
    "custom_objects = keras.utils.get_custom_objects()\n",
    "keras.utils.get_custom_objects().update({'swish': keras.layers.Activation(swish)})\n",
    "\n",
    "def root_mean_squared_per_error(y_true, y_pred):\n",
    "         return K.sqrt(K.mean(K.square( (y_true - y_pred)/ y_true )))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6481b3e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-25T04:15:23.534352Z",
     "iopub.status.busy": "2023-05-25T04:15:23.532481Z",
     "iopub.status.idle": "2023-05-25T04:15:23.543461Z",
     "shell.execute_reply": "2023-05-25T04:15:23.542124Z"
    },
    "papermill": {
     "duration": 0.027061,
     "end_time": "2023-05-25T04:15:23.546322",
     "exception": false,
     "start_time": "2023-05-25T04:15:23.519261",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_time_agg(df):\n",
    "    #需要计算统计特征的列名\n",
    "    gcols=['book_log_return1_realized_volatility']\n",
    "    gcols+=['book_log_return1_realized_volatility_150win_150']\n",
    "    gcols+=['book_log_return2_realized_volatility_150win_300']\n",
    "    gcols+=['book_log_return1_realized_volatility_150win_450']+['trade_log_return_realized_volatility_150win_450']\n",
    "    gcols+=['book_price_spread_sum_150win_150']\n",
    "    gcols+=['trade_size_tau_150win_150']\n",
    "    gcols+=['book_depth_sum_150win_150']\n",
    "    gcols+=['book_dispersion_sum_150win_150']\n",
    "\n",
    "     #time_id聚合\n",
    "    df_time_id = df.groupby('time_id')[gcols].agg(['mean', 'std', 'max', 'min']).reset_index()\n",
    "\n",
    "    df_time_id.columns = ['_'.join(col) for col in df_time_id.columns]\n",
    "    df_time_id = df_time_id.add_suffix('_' + 'time')\n",
    "    \n",
    "\n",
    "    df_time_id = df_time_id.rename(columns={'time_id__time':'time_id'})\n",
    "    return df_time_id, [col for col in df_time_id if col not in ['time_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a9cd431",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-25T04:15:23.572336Z",
     "iopub.status.busy": "2023-05-25T04:15:23.570908Z",
     "iopub.status.idle": "2023-05-25T04:15:23.587110Z",
     "shell.execute_reply": "2023-05-25T04:15:23.586010Z"
    },
    "papermill": {
     "duration": 0.032077,
     "end_time": "2023-05-25T04:15:23.589756",
     "exception": false,
     "start_time": "2023-05-25T04:15:23.557679",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#分层K折交叉验证，分层指的是训练集和测试集中的样本尽可能地包含来自每个组的不同类别的样本\n",
    "def stratified_group_k_fold(X, y, groups, k, seed=None):\n",
    "    \"\"\" https://www.kaggle.com/jakubwasikowski/stratified-group-k-fold-cross-validation \"\"\"\n",
    "    #统计 y 中类别的数量和每个组中每个类别出现的次数\n",
    "    labels_num = np.max(y) + 1\n",
    "    y_counts_per_group = defaultdict(lambda: np.zeros(labels_num))\n",
    "    y_distr = Counter()\n",
    "    for label, g in zip(y, groups):\n",
    "        y_counts_per_group[g][label] += 1\n",
    "        y_distr[label] += 1\n",
    "    \n",
    "    # 初始化 y_counts_per_fold 和 groups_per_fold 变量，用于存储每个折的每个类别在每个组中的出现次数和每个折包含的组的集合\n",
    "    y_counts_per_fold = defaultdict(lambda: np.zeros(labels_num))\n",
    "    groups_per_fold = defaultdict(set)\n",
    "    \n",
    "    #定义辅助函数 eval_y_counts_per_fold，用于计算每一折中每个类别出现次数的标准差\n",
    "    def eval_y_counts_per_fold(y_counts, fold):\n",
    "        y_counts_per_fold[fold] += y_counts\n",
    "        std_per_label = []\n",
    "        for label in range(labels_num):\n",
    "            label_std = np.std([y_counts_per_fold[i][label] / y_distr[label] for i in range(k)])\n",
    "            std_per_label.append(label_std)\n",
    "        y_counts_per_fold[fold] -= y_counts\n",
    "        return np.mean(std_per_label)\n",
    "    \n",
    "    #对 groups 根据 y 的出现次数排序，并随机打乱顺序，得到 groups_and_y_counts\n",
    "    groups_and_y_counts = list(y_counts_per_group.items())\n",
    "    random.Random(seed).shuffle(groups_and_y_counts)\n",
    "\n",
    "    for g, y_counts in tqdm(sorted(groups_and_y_counts, key=lambda x: -np.std(x[1])), total=len(groups_and_y_counts)):\n",
    "        best_fold = None\n",
    "        min_eval = None\n",
    "        for i in range(k):\n",
    "            fold_eval = eval_y_counts_per_fold(y_counts, i)\n",
    "            if min_eval is None or fold_eval < min_eval:\n",
    "                min_eval = fold_eval\n",
    "                best_fold = i\n",
    "        y_counts_per_fold[best_fold] += y_counts\n",
    "        groups_per_fold[best_fold].add(g)\n",
    "\n",
    "    all_groups = set(groups)\n",
    "    for i in range(k):\n",
    "        train_groups = all_groups - groups_per_fold[i]\n",
    "        test_groups = groups_per_fold[i]\n",
    "\n",
    "        train_indices = [i for i, g in enumerate(groups) if g in train_groups]\n",
    "        test_indices = [i for i, g in enumerate(groups) if g in test_groups]\n",
    "\n",
    "        yield train_indices, test_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d13185d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-25T04:15:23.615260Z",
     "iopub.status.busy": "2023-05-25T04:15:23.613969Z",
     "iopub.status.idle": "2023-05-25T04:15:23.626127Z",
     "shell.execute_reply": "2023-05-25T04:15:23.624705Z"
    },
    "papermill": {
     "duration": 0.028085,
     "end_time": "2023-05-25T04:15:23.629097",
     "exception": false,
     "start_time": "2023-05-25T04:15:23.601012",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def base_model(numfeats, cat_data=train['stock_id']):\n",
    "    #定义了神经网络的隐藏层，其中包含了多个全连接层，每层都有不同数量的神经元\n",
    "    hidden_units = (64, 32, 32, 16, 16, 16, 16, 8, 8)\n",
    "    #指定股票 ID 的嵌入维度\n",
    "    stock_embedding_size = 24\n",
    "    \n",
    "    # 分别表示输入的股票 ID（通过 Embedding 层嵌入）和数值特征\n",
    "    stock_id_input = keras.Input(shape=(1,), name='stock_id')\n",
    "    num_input = keras.Input(shape=(numfeats,), name='num_data')\n",
    "\n",
    "\n",
    "    #将每个股票 ID 嵌入到一个矢量空间中，转化为固定长度的特征向量\n",
    "    stock_embedded = keras.layers.Embedding(max(cat_data)+1, stock_embedding_size, \n",
    "                                           input_length=1, name='stock_embedding')(stock_id_input)\n",
    "    \n",
    "    #将 Embedding 层的输出展平，与数值特征拼接在一起\n",
    "    stock_flattened = keras.layers.Flatten()(stock_embedded)\n",
    "    out = keras.layers.Concatenate()([stock_flattened, num_input])\n",
    "    \n",
    "    # 通过循环添加多个全连接层，并使用 Swish 作为激活函数。最后为输出层设计单个神经元，其激活函数为线性函数\n",
    "    for n_hidden in hidden_units:\n",
    "        out = keras.layers.Dense(n_hidden, activation='swish')(out)\n",
    "\n",
    "    #out = keras.layers.Concatenate()([out, num_input])\n",
    "\n",
    "    out = keras.layers.Dense(1, activation='linear', name='prediction')(out)\n",
    "    \n",
    "    model = keras.Model(\n",
    "    inputs = [stock_id_input, num_input],\n",
    "    outputs = out,\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "35dd5ef5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-25T04:15:23.653703Z",
     "iopub.status.busy": "2023-05-25T04:15:23.653183Z",
     "iopub.status.idle": "2023-05-25T04:15:24.808563Z",
     "shell.execute_reply": "2023-05-25T04:15:24.806989Z"
    },
    "papermill": {
     "duration": 1.171887,
     "end_time": "2023-05-25T04:15:24.812238",
     "exception": false,
     "start_time": "2023-05-25T04:15:23.640351",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our training set has 428932 rows\n",
      "Our test set has 3 rows\n",
      "Our training set has 0 missing values\n",
      "Our test set has 0 missing values\n"
     ]
    }
   ],
   "source": [
    "def read_train_test():\n",
    "    \n",
    "    train = pd.read_csv('../input/optiver-realized-volatility-prediction/train.csv')\n",
    "    test = pd.read_csv('../input/optiver-realized-volatility-prediction/test.csv')\n",
    "\n",
    "\n",
    "    train['row_id'] = train['stock_id'].astype(str) + '-' + train['time_id'].astype(str)\n",
    "    test['row_id'] = test['stock_id'].astype(str) + '-' + test['time_id'].astype(str)\n",
    "    print(f'Our training set has {train.shape[0]} rows')\n",
    "    print(f'Our test set has {test.shape[0]} rows')\n",
    "    print(f'Our training set has {train.isna().sum().sum()} missing values')\n",
    "    print(f'Our test set has {test.isna().sum().sum()} missing values')\n",
    "    \n",
    "    return train, test\n",
    "\n",
    "train, test = read_train_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d074458d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-25T04:15:24.840686Z",
     "iopub.status.busy": "2023-05-25T04:15:24.840176Z",
     "iopub.status.idle": "2023-05-25T04:15:24.905162Z",
     "shell.execute_reply": "2023-05-25T04:15:24.903734Z"
    },
    "papermill": {
     "duration": 0.082456,
     "end_time": "2023-05-25T04:15:24.907979",
     "exception": false,
     "start_time": "2023-05-25T04:15:24.825523",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_dir = '../input/optiver-realized-volatility-prediction/'\n",
    "\n",
    "\n",
    "def calc_wap1(df):\n",
    "    wap = (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']) / (df['bid_size1'] + df['ask_size1'])\n",
    "    return wap\n",
    "\n",
    "def calc_wap2(df):\n",
    "    wap = (df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2']) / (df['bid_size2'] + df['ask_size2'])\n",
    "    return wap\n",
    "\n",
    "def calc_wap12(df):\n",
    "    var1 = df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']\n",
    "    var2 = df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2']\n",
    "    den = df['bid_size1'] + df['ask_size1'] + df['bid_size2'] + df['ask_size2']\n",
    "    return (var1+var2) / den\n",
    "\n",
    "def calc_wap3(df):\n",
    "    wap = (df['bid_price1'] * df['bid_size1'] + df['ask_price1'] * df['ask_size1']) / (df['bid_size1'] + df['ask_size1'])\n",
    "    return wap\n",
    "\n",
    "def calc_wap34(df):\n",
    "    var1 = df['bid_price1'] * df['bid_size1'] + df['ask_price1'] * df['ask_size1']\n",
    "    var2 = df['bid_price2'] * df['bid_size2'] + df['ask_price2'] * df['ask_size2']\n",
    "    den = df['bid_size1'] + df['ask_size1'] + df['bid_size2'] + df['ask_size2']\n",
    "    return (var1+var2) / den\n",
    "\n",
    "def calc_swap12(df):\n",
    "    return df['wap12'] - df['wap34']\n",
    "\n",
    "def calc_tswap1(df):\n",
    "    return -df['swap1'].diff()\n",
    "\n",
    "def calc_tswap12(df):\n",
    "    return -df['swap12'].diff()\n",
    "\n",
    "def calc_wss12(df):\n",
    "    ask = (df['ask_price1'] * df['ask_size1'] + df['ask_price2'] * df['ask_size2'])/(df['ask_size1']+df['ask_size2'])\n",
    "    bid = (df['bid_price1'] * df['bid_size1'] + df['bid_price2'] * df['bid_size2'])/(df['bid_size1']+df['bid_size2'])\n",
    "    return (ask - bid) / df['midprice']\n",
    "\n",
    "\n",
    "def calc_depth(df):\n",
    "    depth = df['bid_price1'] * df['bid_size1'] + df['ask_price1'] * df['ask_size1'] + df['bid_price2'] * df[\n",
    "               'bid_size2'] + df['ask_price2'] * df['ask_size2']\n",
    "    return depth\n",
    "\n",
    "\n",
    "def calc_slope(df):\n",
    "    v0 = (df['bid_size1']+df['ask_size1'])/2\n",
    "    p0 = (df['bid_price1']+df['ask_price1'])/2\n",
    "    slope_bid = ((df['bid_size1']/v0)-1)/abs((df['bid_price1']/p0)-1)+(\n",
    "                (df['bid_size2']/df['bid_size1'])-1)/abs((df['bid_price2']/df['bid_price1'])-1)\n",
    "    slope_ask = ((df['ask_size1']/v0)-1)/abs((df['ask_price1']/p0)-1)+(\n",
    "                (df['ask_size2']/df['ask_size1'])-1)/abs((df['ask_price2']/df['ask_price1'])-1)\n",
    "    return (slope_bid+slope_ask)/2, abs(slope_bid-slope_ask)\n",
    "\n",
    "\n",
    "def calc_dispersion(df):\n",
    "    bspread = df['bid_price1'] - df['bid_price2']\n",
    "    aspread = df['ask_price2'] - df['ask_price1']\n",
    "    bmid = (df['bid_price1'] + df['ask_price1'])/2  - df['bid_price1']\n",
    "    bmid2 = (df['bid_price1'] + df['ask_price1'])/2  - df['bid_price2']\n",
    "    amid = df['ask_price1'] - (df['bid_price1'] + df['ask_price1'])/2\n",
    "    amid2 = df['ask_price2'] - (df['bid_price1'] + df['ask_price1'])/2\n",
    "    bdisp = (df['bid_size1']*bmid + df['bid_size2']*bspread)/(df['bid_size1']+df['bid_size2'])\n",
    "    bdisp2 = (df['bid_size1']*bmid + df['bid_size2']*bmid2)/(df['bid_size1']+df['bid_size2'])\n",
    "    adisp = (df['ask_size1']*amid + df['ask_size2']*aspread)/(df['ask_size1']+df['ask_size2'])      \n",
    "    adisp2 = (df['ask_size1']*amid + df['ask_size2']*amid2)/(df['ask_size1']+df['ask_size2'])\n",
    "    return (bdisp + adisp)/2, (bdisp2 + adisp2)/2\n",
    "\n",
    "def calc_price_impact(df):\n",
    "    ask = (df['ask_price1'] * df['ask_size1'] + df['ask_price2'] * df['ask_size2'])/(df['ask_size1']+df['ask_size2'])\n",
    "    bid = (df['bid_price1'] * df['bid_size1'] + df['bid_price2'] * df['bid_size2'])/(df['bid_size1']+df['bid_size2'])\n",
    "    return (df['ask_price1'] - ask)/df['ask_price1'], (df['bid_price1'] - bid)/df['bid_price1']\n",
    "\n",
    "\n",
    "def calc_ofi(df):\n",
    "    a = df['bid_size1']*np.where(df['bid_price1'].diff()>=0,1,0)\n",
    "    b = df['bid_size1'].shift()*np.where(df['bid_price1'].diff()<=0,1,0)\n",
    "    c = df['ask_size1']*np.where(df['ask_price1'].diff()<=0,1,0)\n",
    "    d = df['ask_size1'].shift()*np.where(df['ask_price1'].diff()>=0,1,0)\n",
    "    return a - b - c + d\n",
    "\n",
    "\n",
    "def calc_tt1(df):\n",
    "    p1 = df['ask_price1'] * df['ask_size1'] + df['bid_price1'] * df['bid_size1']\n",
    "    p2 = df['ask_price2'] * df['ask_size2'] + df['bid_price2'] * df['bid_size2']      \n",
    "    return p2 - p1 \n",
    "\n",
    "\n",
    "def log_return(series):\n",
    "    return np.log(series).diff()\n",
    "\n",
    "def log_return_out(series):\n",
    "    ret = np.log(series).diff()\n",
    "    return remove_outliers(ret)\n",
    "\n",
    "def remove_outliers(series):\n",
    "    cu = 6\n",
    "    ser_mean, ser_std = np.mean(series), np.std(series)\n",
    "    series = series.where(series<=(ser_mean + cu*ser_std), ser_mean)\n",
    "    series = series.where(series>=(ser_mean - cu*ser_std), ser_mean)\n",
    "    return series\n",
    "\n",
    "def realized_volatility(series):\n",
    "    return np.sqrt(np.sum(series**2))\n",
    "    \n",
    "def realized_volatility_downside(series):\n",
    "    return np.sqrt(np.sum(series[series<0]**2))\n",
    "\n",
    "def realized_volatility_upside(series):\n",
    "    return np.sqrt(np.sum(series[series>0]**2))\n",
    "\n",
    "\n",
    "def realized_bipowvar(series):\n",
    "    cnt = series.count()\n",
    "    if cnt<3:\n",
    "        return np.nan\n",
    "    else:\n",
    "        cons = (np.pi/2)*(cnt/(cnt-2))\n",
    "        return cons*np.nansum(np.abs(series)*np.abs(series.shift()))\n",
    "    \n",
    "\n",
    "def realized_quarticity(series):\n",
    "    return (series.count()/3)*np.sum(series**4)\n",
    "\n",
    "\n",
    "def realized_medianvar(series):\n",
    "    cnt = series.count()\n",
    "    if cnt<3:\n",
    "        return np.nan\n",
    "    else:\n",
    "        cons = (np.pi/(6-4*np.sqrt(3)+np.pi))*(cnt/(cnt-2))\n",
    "        return cons*np.nansum(np.median([np.abs(series),np.abs(series.shift()),np.abs(series.shift(2))], axis=0)**2)\n",
    "    \n",
    "\n",
    "def realized_absvar(series):\n",
    "    return np.sqrt(np.pi/(2*series.count()))*np.sum(np.abs(series))\n",
    "\n",
    "\n",
    "def realized_vol_weighted(series):\n",
    "    return np.sqrt(np.sum(series**2)/series.count())\n",
    "\n",
    "\n",
    "def realized_skew(series):\n",
    "    return np.sqrt(series.count())*np.sum(series**3)/(realized_volatility(series)**3)\n",
    "\n",
    "\n",
    "def realized_kurtosis(series):\n",
    "    return series.count()*np.sum(series**4)/(realized_volatility(series)**4)\n",
    "\n",
    "\n",
    "def get_stats_bins(df, feat_dict, bins, quantile=False):\n",
    "    # Group by the window\n",
    "    if bins==0:\n",
    "        df_feature = df.groupby('time_id').agg(feat_dict).reset_index()\n",
    "        # Rename columns joining suffix\n",
    "        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n",
    "        df_feature = df_feature.rename(columns={'time_id_': 'time_id'})\n",
    "    else:\n",
    "        if quantile:\n",
    "            df['sbins'] = pd.qcut(df.seconds_in_bucket, bins, labels=False)\n",
    "            q = 'q'\n",
    "        else:\n",
    "            df['sbins'] = pd.cut(df.seconds_in_bucket, bins, labels=False)\n",
    "            q = ''\n",
    "        df_feature = None\n",
    "        for i in range(bins):\n",
    "            df_feat = df.loc[df.sbins==i].groupby('time_id').agg(feat_dict).reset_index()\n",
    "            # Rename columns joining suffix\n",
    "            df_feat.columns = ['_'.join(col) for col in df_feat.columns]\n",
    "            # Add a suffix to differentiate bins\n",
    "            df_feat = df_feat.rename(columns={col: col+'_'+q+str(bins)+'bins_'+str(i) for col in df_feat.columns})\n",
    "            df_feat = df_feat.rename(columns={'time_id_'+'_'+q+str(bins)+'bins_'+str(i): 'time_id'})\n",
    "            if isinstance(df_feature, pd.DataFrame):\n",
    "                df_feature = pd.merge(df_feature, df_feat, how='left', on='time_id')\n",
    "            else:\n",
    "                df_feature = df_feat.copy()\n",
    "        df = df.drop('sbins', axis=1)\n",
    "    return df_feature\n",
    "\n",
    "# Function to get group stats for different windows (seconds in bucket)\n",
    "def get_stats_window(df, feat_dict, window):\n",
    "    # Group by the window\n",
    "    if window==0:\n",
    "        df_feature = df.groupby('time_id').agg(feat_dict).reset_index()\n",
    "        # Rename columns joining suffix\n",
    "        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n",
    "        df_feature = df_feature.rename(columns={'time_id_': 'time_id'})\n",
    "    else:\n",
    "        df_feature = None\n",
    "        for w in range(window, 600, window):\n",
    "            df_feat = df.loc[df.seconds_in_bucket>=w].groupby('time_id').agg(feat_dict).reset_index()\n",
    "            # Rename columns joining suffix\n",
    "            df_feat.columns = ['_'.join(col) for col in df_feat.columns]\n",
    "            # Add a suffix to differentiate bins\n",
    "            df_feat = df_feat.rename(columns={col: col+'_'+str(window)+'win_'+str(w) for col in df_feat.columns})\n",
    "            df_feat = df_feat.rename(columns={'time_id_'+'_'+str(window)+'win_'+str(w): 'time_id'})\n",
    "            if isinstance(df_feature, pd.DataFrame):\n",
    "                df_feature = pd.merge(df_feature, df_feat, how='left', on='time_id')\n",
    "            else:\n",
    "                df_feature = df_feat.copy()\n",
    "    return df_feature\n",
    "\n",
    "# Function to sample from the window and average\n",
    "def get_stats_sampled(df, feat_dict, numsamp, fraction, boot):\n",
    "    df_feature = None\n",
    "    for i in range(numsamp):\n",
    "        df_feat = df.groupby('time_id').sample(frac=fraction, random_state=i, replace=boot).reset_index(drop=True)\n",
    "        df_feat = df_feat.groupby('time_id').agg(feat_dict).reset_index()\n",
    "        df_feat.columns = ['_'.join(col) for col in df_feat.columns]\n",
    "        if isinstance(df_feature, pd.DataFrame):\n",
    "            df_feature += df_feat.values/numsamp   \n",
    "        else:\n",
    "            df_feature = df_feat.copy()/numsamp\n",
    "    df_feature = df_feature.rename(columns={col: col+'_sample_'+str(fraction)+'_'+str(numsamp) for col in df_feature.columns})\n",
    "    df_feature = df_feature.rename(columns={'time_id_'+'_sample_'+str(fraction)+'_'+str(numsamp): 'time_id'})\n",
    "    return df_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ba1f29d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-25T04:15:24.933303Z",
     "iopub.status.busy": "2023-05-25T04:15:24.932797Z",
     "iopub.status.idle": "2023-05-25T05:10:36.285498Z",
     "shell.execute_reply": "2023-05-25T05:10:36.284022Z"
    },
    "papermill": {
     "duration": 3311.369034,
     "end_time": "2023-05-25T05:10:36.288452",
     "exception": false,
     "start_time": "2023-05-25T04:15:24.919418",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done 112 out of 112 | elapsed: 55.2min finished\n",
      "[Parallel(n_jobs=-1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def book_preprocessor(file_path):\n",
    "    df = pd.read_parquet(file_path)\n",
    "    \n",
    "    #### Data Cleansing (if any needed)\n",
    "    \n",
    "    df = df.drop(df.loc[df.ask_price1 <= 0].index)\n",
    "    df = df.drop(df.loc[df.bid_price1 <= 0].index)\n",
    "    df = df.drop(df.loc[(df['ask_price1'] - df['bid_price1']) < 0].index)\n",
    "    df = df.groupby(['time_id','seconds_in_bucket']).mean().reset_index()\n",
    "    \n",
    "    ####\n",
    "    \n",
    "    # Calculate prices\n",
    "    df['wap1'] = calc_wap1(df)\n",
    "    df['wap2'] = calc_wap2(df)\n",
    "    \n",
    "    df['depth'] = calc_depth(df)\n",
    "    df['slope'], _ = calc_slope(df)\n",
    "\n",
    "    # Calculate log returns\n",
    "    df['log_return1'] = df.groupby('time_id')['wap1'].apply(log_return)\n",
    "    df['log_return2'] = df.groupby('time_id')['wap2'].apply(log_return)\n",
    "    \n",
    "    # Calculate spreads\n",
    "    df['price_spread'] = (df['ask_price1'] - df['bid_price1']) / ((df['ask_price1'] + df['bid_price1']) / 2)\n",
    "    df['dispersion'], _ = calc_dispersion(df)\n",
    "\n",
    "    # Dict for aggregations\n",
    "    create_feature_dict = {\n",
    "        'depth': [np.sum],\n",
    "        'slope': [np.sum],\n",
    "        'log_return1': [realized_volatility, realized_absvar],\n",
    "        'log_return2': [realized_volatility, realized_absvar],\n",
    "        'price_spread': [np.sum, np.nanmedian],\n",
    "    }\n",
    "\n",
    "    create_feature_dict_bins = {\n",
    "        'depth': [np.sum],\n",
    "        'slope': [np.sum],\n",
    "        'dispersion': [np.sum],\n",
    "        'log_return1': [realized_volatility, realized_absvar],\n",
    "        'log_return2': [realized_volatility, realized_absvar],\n",
    "        'price_spread': [np.sum],\n",
    "    }\n",
    "\n",
    "\n",
    "    # Get the stats for different windows\n",
    "    df_feature_0 = get_stats_bins(df, create_feature_dict, 0)\n",
    "    df_feature_w4 = get_stats_window(df, create_feature_dict_bins, 150)\n",
    "\n",
    "    # Merge all\n",
    "    df_feature_0 = df_feature_0.merge(df_feature_w4, how = 'left', on = 'time_id')   \n",
    "\n",
    "    df_feature_0 = df_feature_0.add_prefix('book_')\n",
    "    stock_id = file_path.split('=')[1]\n",
    "    df_feature_0['row_id'] = df_feature_0['book_time_id'].apply(lambda x:f'{stock_id}-{x}')\n",
    "    df_feature_0.drop(['book_time_id'], axis = 1, inplace = True)\n",
    "    return df_feature_0\n",
    "\n",
    "# Function to preprocess trade data (for each stock id)\n",
    "def trade_preprocessor(file_path):\n",
    "    df = pd.read_parquet(file_path)\n",
    "\n",
    "    #### Data Cleansing\n",
    "    \n",
    "    df = df.drop(df.loc[df.price <= 0].index)\n",
    "    df = df.groupby(['time_id','seconds_in_bucket']).mean().reset_index()\n",
    "    \n",
    "    ####\n",
    "    \n",
    "    df['log_return'] = df.groupby('time_id')['price'].apply(log_return)\n",
    "    \n",
    "    # Dict for aggregations\n",
    "    create_feature_dict = {\n",
    "        'log_return': [realized_volatility, realized_absvar],\n",
    "        'order_count':[np.sum, np.max],\n",
    "    }\n",
    "    \n",
    "    create_feature_dict_bins = {\n",
    "        'log_return': [realized_volatility, realized_absvar],\n",
    "        'order_count': [np.sum],\n",
    "    }\n",
    "    \n",
    "    # Get the stats for different windows\n",
    "    df_feature_0 = get_stats_bins(df, create_feature_dict, 0)\n",
    "    df_feature_w4 = get_stats_window(df, create_feature_dict_bins, 150)\n",
    "\n",
    "    # Merge all\n",
    "    df_feature_0 = df_feature_0.merge(df_feature_w4, how = 'left', on = 'time_id')   \n",
    "\n",
    "    df_feature_0 = df_feature_0.add_prefix('trade_')\n",
    "    stock_id = file_path.split('=')[1]\n",
    "    df_feature_0['row_id'] = df_feature_0['trade_time_id'].apply(lambda x:f'{stock_id}-{x}')\n",
    "    df_feature_0.drop(['trade_time_id'], axis = 1, inplace = True)\n",
    "    return df_feature_0\n",
    "    \n",
    "# Funtion to make preprocessing function in parallel (for each stock id)\n",
    "def preprocessor(list_stock_ids, is_train = True):\n",
    "    \n",
    "    # Parrallel for loop\n",
    "    def for_joblib(stock_id):\n",
    "        # Train\n",
    "        if is_train:\n",
    "            file_path_book = data_dir + \"book_train.parquet/stock_id=\" + str(stock_id)\n",
    "            file_path_trade = data_dir + \"trade_train.parquet/stock_id=\" + str(stock_id)\n",
    "        # Test\n",
    "        else:\n",
    "            file_path_book = data_dir + \"book_test.parquet/stock_id=\" + str(stock_id)\n",
    "            file_path_trade = data_dir + \"trade_test.parquet/stock_id=\" + str(stock_id)\n",
    "    \n",
    "        # Preprocess book and trade data and merge them\n",
    "        df_tmp = pd.merge(book_preprocessor(file_path_book), trade_preprocessor(file_path_trade), on = 'row_id', how = 'left')      \n",
    "        # Return the merge dataframe\n",
    "        return df_tmp\n",
    "    \n",
    "    # Use parallel api to call paralle for loop\n",
    "    df = Parallel(n_jobs = -1, verbose = 1)(delayed(for_joblib)(stock_id) for stock_id in list_stock_ids)\n",
    "    # Concatenate all the dataframes that return from Parallel\n",
    "    df = pd.concat(df, ignore_index = True)\n",
    "    return df\n",
    "\n",
    "# Get unique stock ids \n",
    "train_stock_ids = train['stock_id'].unique()\n",
    "# Preprocess them using Parallel and our single stock id functions\n",
    "train_ = preprocessor(train_stock_ids, is_train = True)\n",
    "train = train.merge(train_, on = ['row_id'], how = 'left')\n",
    "\n",
    "# Get unique stock ids \n",
    "test_stock_ids = test['stock_id'].unique()\n",
    "# Preprocess them using Parallel and our single stock id functions\n",
    "test_ = preprocessor(test_stock_ids, is_train = False)\n",
    "test = test.merge(test_, on = ['row_id'], how = 'left')\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d285f93b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-25T05:10:36.316650Z",
     "iopub.status.busy": "2023-05-25T05:10:36.316140Z",
     "iopub.status.idle": "2023-05-25T05:10:36.339451Z",
     "shell.execute_reply": "2023-05-25T05:10:36.337973Z"
    },
    "papermill": {
     "duration": 0.040801,
     "end_time": "2023-05-25T05:10:36.342400",
     "exception": false,
     "start_time": "2023-05-25T05:10:36.301599",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train['trade_size_tau'] = np.sqrt(1/train['trade_order_count_sum'])\n",
    "    \n",
    "for w in range(150, 600, 150):\n",
    "    train['trade_size_tau_150win_'+str(w)] = np.sqrt(1/train['trade_order_count_sum_150win_'+str(w)])\n",
    "    \n",
    "test['trade_size_tau'] = np.sqrt(1/test['trade_order_count_sum'])\n",
    "    \n",
    "for w in range(150, 600, 150):\n",
    "    test['trade_size_tau_150win_'+str(w)] = np.sqrt(1/test['trade_order_count_sum_150win_'+str(w)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0da15100",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-25T05:10:36.368904Z",
     "iopub.status.busy": "2023-05-25T05:10:36.368167Z",
     "iopub.status.idle": "2023-05-25T05:10:36.535198Z",
     "shell.execute_reply": "2023-05-25T05:10:36.534007Z"
    },
    "papermill": {
     "duration": 0.183712,
     "end_time": "2023-05-25T05:10:36.538117",
     "exception": false,
     "start_time": "2023-05-25T05:10:36.354405",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cols = ['row_id','time_id','stock_id','target']\n",
    "cols+=['book_log_return1_realized_volatility']\n",
    "cols+=['book_log_return1_realized_volatility_150win_150']+['book_log_return2_realized_volatility_150win_150']+[\n",
    "            'trade_log_return_realized_volatility_150win_150']\n",
    "cols+=['book_log_return1_realized_absvar_150win_150']+['book_log_return2_realized_absvar_150win_150']+[\n",
    "            'trade_log_return_realized_absvar_150win_150']\n",
    "cols+=['book_log_return2_realized_volatility_150win_300']\n",
    "cols+=['book_log_return1_realized_volatility_150win_450']+['trade_log_return_realized_volatility_150win_450']\n",
    "cols+=['book_price_spread_sum_150win_150']\n",
    "cols+=['trade_size_tau_150win_150']\n",
    "cols+=['book_depth_sum_150win_150']\n",
    "cols+=['book_dispersion_sum_150win_150']\n",
    "\n",
    "train = train[[col for col in train.columns if col in cols]]\n",
    "test = test[[col for col in test.columns if col in cols]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "56e2edfa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-25T05:10:36.566515Z",
     "iopub.status.busy": "2023-05-25T05:10:36.566022Z",
     "iopub.status.idle": "2023-05-25T05:10:36.611391Z",
     "shell.execute_reply": "2023-05-25T05:10:36.610237Z"
    },
    "papermill": {
     "duration": 0.063711,
     "end_time": "2023-05-25T05:10:36.614410",
     "exception": false,
     "start_time": "2023-05-25T05:10:36.550699",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "seed(42)\n",
    "\n",
    "es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=50, verbose=0,\n",
    "                                      mode='min', restore_best_weights=True)\n",
    "\n",
    "plateau = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=15, \n",
    "                                               verbose=0, mode='min')\n",
    "\n",
    "scores_folds = {}\n",
    "model_name = 'NN'\n",
    "\n",
    "scores_folds[model_name] = []\n",
    "counter = 1\n",
    "\n",
    "predictions_nn_1 = np.zeros(test.shape[0])\n",
    "y = train['target']\n",
    "# Create a KFold object\n",
    "kf = 5\n",
    "# Iterate through each fold\n",
    "features = [col for col in train.columns if col not in {\"time_id\", \"target\", \"row_id\"}]\n",
    "feats_nostock = [col for col in train.columns if col not in {\"time_id\", \"target\", \"row_id\", \"stock_id\"}] \n",
    "skf = stratified_group_k_fold(X=train[feats_nostock], y=train['stock_id'].astype('category').cat.codes.values, \n",
    "                              groups=np.array(train['time_id'].astype('category').cat.codes.values), k=kf, seed=2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b7228206",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-25T05:10:36.641515Z",
     "iopub.status.busy": "2023-05-25T05:10:36.641025Z",
     "iopub.status.idle": "2023-05-25T06:00:21.515001Z",
     "shell.execute_reply": "2023-05-25T06:00:21.512986Z"
    },
    "papermill": {
     "duration": 2989.22361,
     "end_time": "2023-05-25T06:00:25.850407",
     "exception": false,
     "start_time": "2023-05-25T05:10:36.626797",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3830/3830 [01:59<00:00, 32.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fold 1\n",
      "(343146, 18)\n",
      "(85786, 18)\n",
      "Epoch 1/1000\n",
      "168/168 [==============================] - 6s 16ms/step - loss: 0.3534 - val_loss: 0.2704 - lr: 0.0010\n",
      "Epoch 2/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2485 - val_loss: 0.2816 - lr: 0.0010\n",
      "Epoch 3/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2405 - val_loss: 0.2424 - lr: 0.0010\n",
      "Epoch 4/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2568 - val_loss: 0.2369 - lr: 0.0010\n",
      "Epoch 5/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2364 - val_loss: 0.2307 - lr: 0.0010\n",
      "Epoch 6/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2497 - val_loss: 0.2320 - lr: 0.0010\n",
      "Epoch 7/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2372 - val_loss: 0.2375 - lr: 0.0010\n",
      "Epoch 8/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2351 - val_loss: 0.2309 - lr: 0.0010\n",
      "Epoch 9/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2357 - val_loss: 0.2293 - lr: 0.0010\n",
      "Epoch 10/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2333 - val_loss: 0.2323 - lr: 0.0010\n",
      "Epoch 11/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2378 - val_loss: 0.2265 - lr: 0.0010\n",
      "Epoch 12/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2347 - val_loss: 0.2266 - lr: 0.0010\n",
      "Epoch 13/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2333 - val_loss: 0.2295 - lr: 0.0010\n",
      "Epoch 14/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2318 - val_loss: 0.2278 - lr: 0.0010\n",
      "Epoch 15/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2271 - val_loss: 0.2295 - lr: 0.0010\n",
      "Epoch 16/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2367 - val_loss: 0.2250 - lr: 0.0010\n",
      "Epoch 17/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2281 - val_loss: 0.2316 - lr: 0.0010\n",
      "Epoch 18/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2284 - val_loss: 0.2266 - lr: 0.0010\n",
      "Epoch 19/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2251 - val_loss: 0.2243 - lr: 0.0010\n",
      "Epoch 20/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2292 - val_loss: 0.2248 - lr: 0.0010\n",
      "Epoch 21/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2313 - val_loss: 0.2239 - lr: 0.0010\n",
      "Epoch 22/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2325 - val_loss: 0.2281 - lr: 0.0010\n",
      "Epoch 23/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2270 - val_loss: 0.2307 - lr: 0.0010\n",
      "Epoch 24/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2249 - val_loss: 0.2378 - lr: 0.0010\n",
      "Epoch 25/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2266 - val_loss: 0.2417 - lr: 0.0010\n",
      "Epoch 26/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2279 - val_loss: 0.2554 - lr: 0.0010\n",
      "Epoch 27/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2264 - val_loss: 0.2289 - lr: 0.0010\n",
      "Epoch 28/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2229 - val_loss: 0.2235 - lr: 0.0010\n",
      "Epoch 29/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2211 - val_loss: 0.2445 - lr: 0.0010\n",
      "Epoch 30/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2266 - val_loss: 0.2465 - lr: 0.0010\n",
      "Epoch 31/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2219 - val_loss: 0.2326 - lr: 0.0010\n",
      "Epoch 32/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2219 - val_loss: 0.2220 - lr: 0.0010\n",
      "Epoch 33/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2214 - val_loss: 0.2246 - lr: 0.0010\n",
      "Epoch 34/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2219 - val_loss: 0.2224 - lr: 0.0010\n",
      "Epoch 35/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2219 - val_loss: 0.2326 - lr: 0.0010\n",
      "Epoch 36/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2258 - val_loss: 0.2256 - lr: 0.0010\n",
      "Epoch 37/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2217 - val_loss: 0.2276 - lr: 0.0010\n",
      "Epoch 38/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2211 - val_loss: 0.2208 - lr: 0.0010\n",
      "Epoch 39/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2228 - val_loss: 0.2215 - lr: 0.0010\n",
      "Epoch 40/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2217 - val_loss: 0.2226 - lr: 0.0010\n",
      "Epoch 41/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2205 - val_loss: 0.2207 - lr: 0.0010\n",
      "Epoch 42/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2190 - val_loss: 0.2239 - lr: 0.0010\n",
      "Epoch 43/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2214 - val_loss: 0.2202 - lr: 0.0010\n",
      "Epoch 44/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2197 - val_loss: 0.2279 - lr: 0.0010\n",
      "Epoch 45/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2190 - val_loss: 0.2228 - lr: 0.0010\n",
      "Epoch 46/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2188 - val_loss: 0.2192 - lr: 0.0010\n",
      "Epoch 47/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2191 - val_loss: 0.2211 - lr: 0.0010\n",
      "Epoch 48/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2215 - val_loss: 0.2344 - lr: 0.0010\n",
      "Epoch 49/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2192 - val_loss: 0.2419 - lr: 0.0010\n",
      "Epoch 50/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2190 - val_loss: 0.2203 - lr: 0.0010\n",
      "Epoch 51/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2219 - val_loss: 0.2201 - lr: 0.0010\n",
      "Epoch 52/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2165 - val_loss: 0.2221 - lr: 0.0010\n",
      "Epoch 53/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2182 - val_loss: 0.2227 - lr: 0.0010\n",
      "Epoch 54/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2169 - val_loss: 0.2223 - lr: 0.0010\n",
      "Epoch 55/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2162 - val_loss: 0.2207 - lr: 0.0010\n",
      "Epoch 56/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2155 - val_loss: 0.2660 - lr: 0.0010\n",
      "Epoch 57/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2200 - val_loss: 0.2178 - lr: 0.0010\n",
      "Epoch 58/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2175 - val_loss: 0.2204 - lr: 0.0010\n",
      "Epoch 59/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2161 - val_loss: 0.2186 - lr: 0.0010\n",
      "Epoch 60/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2158 - val_loss: 0.2256 - lr: 0.0010\n",
      "Epoch 61/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2162 - val_loss: 0.2177 - lr: 0.0010\n",
      "Epoch 62/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2174 - val_loss: 0.2218 - lr: 0.0010\n",
      "Epoch 63/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2190 - val_loss: 0.2172 - lr: 0.0010\n",
      "Epoch 64/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2150 - val_loss: 0.2286 - lr: 0.0010\n",
      "Epoch 65/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2172 - val_loss: 0.2216 - lr: 0.0010\n",
      "Epoch 66/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2175 - val_loss: 0.2177 - lr: 0.0010\n",
      "Epoch 67/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2169 - val_loss: 0.2243 - lr: 0.0010\n",
      "Epoch 68/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2177 - val_loss: 0.2239 - lr: 0.0010\n",
      "Epoch 69/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2138 - val_loss: 0.2178 - lr: 0.0010\n",
      "Epoch 70/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2158 - val_loss: 0.2172 - lr: 0.0010\n",
      "Epoch 71/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2146 - val_loss: 0.2171 - lr: 0.0010\n",
      "Epoch 72/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2173 - val_loss: 0.2258 - lr: 0.0010\n",
      "Epoch 73/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2149 - val_loss: 0.2165 - lr: 0.0010\n",
      "Epoch 74/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2143 - val_loss: 0.2161 - lr: 0.0010\n",
      "Epoch 75/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2170 - val_loss: 0.2173 - lr: 0.0010\n",
      "Epoch 76/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2147 - val_loss: 0.2249 - lr: 0.0010\n",
      "Epoch 77/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2155 - val_loss: 0.2189 - lr: 0.0010\n",
      "Epoch 78/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2144 - val_loss: 0.2163 - lr: 0.0010\n",
      "Epoch 79/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2150 - val_loss: 0.2180 - lr: 0.0010\n",
      "Epoch 80/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2137 - val_loss: 0.2160 - lr: 0.0010\n",
      "Epoch 81/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2145 - val_loss: 0.2158 - lr: 0.0010\n",
      "Epoch 82/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2126 - val_loss: 0.2164 - lr: 0.0010\n",
      "Epoch 83/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2148 - val_loss: 0.2165 - lr: 0.0010\n",
      "Epoch 84/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2149 - val_loss: 0.2217 - lr: 0.0010\n",
      "Epoch 85/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2168 - val_loss: 0.2178 - lr: 0.0010\n",
      "Epoch 86/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2154 - val_loss: 0.2171 - lr: 0.0010\n",
      "Epoch 87/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2147 - val_loss: 0.2154 - lr: 0.0010\n",
      "Epoch 88/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2130 - val_loss: 0.2152 - lr: 0.0010\n",
      "Epoch 89/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2131 - val_loss: 0.2157 - lr: 0.0010\n",
      "Epoch 90/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2136 - val_loss: 0.2195 - lr: 0.0010\n",
      "Epoch 91/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2130 - val_loss: 0.2161 - lr: 0.0010\n",
      "Epoch 92/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2128 - val_loss: 0.2156 - lr: 0.0010\n",
      "Epoch 93/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2143 - val_loss: 0.2163 - lr: 0.0010\n",
      "Epoch 94/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2148 - val_loss: 0.2152 - lr: 0.0010\n",
      "Epoch 95/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2156 - val_loss: 0.2166 - lr: 0.0010\n",
      "Epoch 96/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2138 - val_loss: 0.2216 - lr: 0.0010\n",
      "Epoch 97/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2124 - val_loss: 0.2154 - lr: 0.0010\n",
      "Epoch 98/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2144 - val_loss: 0.2221 - lr: 0.0010\n",
      "Epoch 99/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2139 - val_loss: 0.2187 - lr: 0.0010\n",
      "Epoch 100/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2127 - val_loss: 0.2175 - lr: 0.0010\n",
      "Epoch 101/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2133 - val_loss: 0.2245 - lr: 0.0010\n",
      "Epoch 102/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2127 - val_loss: 0.2173 - lr: 0.0010\n",
      "Epoch 103/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2124 - val_loss: 0.2149 - lr: 0.0010\n",
      "Epoch 104/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2134 - val_loss: 0.2191 - lr: 0.0010\n",
      "Epoch 105/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2127 - val_loss: 0.2149 - lr: 0.0010\n",
      "Epoch 106/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2128 - val_loss: 0.2160 - lr: 0.0010\n",
      "Epoch 107/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2122 - val_loss: 0.2155 - lr: 0.0010\n",
      "Epoch 108/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2134 - val_loss: 0.2336 - lr: 0.0010\n",
      "Epoch 109/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2130 - val_loss: 0.2152 - lr: 0.0010\n",
      "Epoch 110/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2121 - val_loss: 0.2150 - lr: 0.0010\n",
      "Epoch 111/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2114 - val_loss: 0.2152 - lr: 0.0010\n",
      "Epoch 112/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2122 - val_loss: 0.2160 - lr: 0.0010\n",
      "Epoch 113/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2120 - val_loss: 0.2152 - lr: 0.0010\n",
      "Epoch 114/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2110 - val_loss: 0.2146 - lr: 0.0010\n",
      "Epoch 115/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2116 - val_loss: 0.2193 - lr: 0.0010\n",
      "Epoch 116/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2114 - val_loss: 0.2153 - lr: 0.0010\n",
      "Epoch 117/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2123 - val_loss: 0.2156 - lr: 0.0010\n",
      "Epoch 118/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2120 - val_loss: 0.2172 - lr: 0.0010\n",
      "Epoch 119/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2122 - val_loss: 0.2163 - lr: 0.0010\n",
      "Epoch 120/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2128 - val_loss: 0.2144 - lr: 0.0010\n",
      "Epoch 121/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2124 - val_loss: 0.2173 - lr: 0.0010\n",
      "Epoch 122/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2112 - val_loss: 0.2150 - lr: 0.0010\n",
      "Epoch 123/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2109 - val_loss: 0.2146 - lr: 0.0010\n",
      "Epoch 124/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2123 - val_loss: 0.2137 - lr: 0.0010\n",
      "Epoch 125/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2116 - val_loss: 0.2155 - lr: 0.0010\n",
      "Epoch 126/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2105 - val_loss: 0.2152 - lr: 0.0010\n",
      "Epoch 127/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2116 - val_loss: 0.2251 - lr: 0.0010\n",
      "Epoch 128/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2109 - val_loss: 0.2246 - lr: 0.0010\n",
      "Epoch 129/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2107 - val_loss: 0.2139 - lr: 0.0010\n",
      "Epoch 130/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2103 - val_loss: 0.2229 - lr: 0.0010\n",
      "Epoch 131/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2145 - val_loss: 0.2196 - lr: 0.0010\n",
      "Epoch 132/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2104 - val_loss: 0.2148 - lr: 0.0010\n",
      "Epoch 133/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2128 - val_loss: 0.2160 - lr: 0.0010\n",
      "Epoch 134/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2106 - val_loss: 0.2137 - lr: 0.0010\n",
      "Epoch 135/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2101 - val_loss: 0.2169 - lr: 0.0010\n",
      "Epoch 136/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2107 - val_loss: 0.2142 - lr: 0.0010\n",
      "Epoch 137/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2115 - val_loss: 0.2252 - lr: 0.0010\n",
      "Epoch 138/1000\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2112 - val_loss: 0.2192 - lr: 0.0010\n",
      "Epoch 139/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2095 - val_loss: 0.2134 - lr: 0.0010\n",
      "Epoch 140/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2110 - val_loss: 0.2168 - lr: 0.0010\n",
      "Epoch 141/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2104 - val_loss: 0.2266 - lr: 0.0010\n",
      "Epoch 142/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2109 - val_loss: 0.2134 - lr: 0.0010\n",
      "Epoch 143/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2097 - val_loss: 0.2155 - lr: 0.0010\n",
      "Epoch 144/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2112 - val_loss: 0.2139 - lr: 0.0010\n",
      "Epoch 145/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2103 - val_loss: 0.2161 - lr: 0.0010\n",
      "Epoch 146/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2102 - val_loss: 0.2149 - lr: 0.0010\n",
      "Epoch 147/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2100 - val_loss: 0.2199 - lr: 0.0010\n",
      "Epoch 148/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2123 - val_loss: 0.2233 - lr: 0.0010\n",
      "Epoch 149/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2102 - val_loss: 0.2133 - lr: 0.0010\n",
      "Epoch 150/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2108 - val_loss: 0.2150 - lr: 0.0010\n",
      "Epoch 151/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2105 - val_loss: 0.2147 - lr: 0.0010\n",
      "Epoch 152/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2090 - val_loss: 0.2161 - lr: 0.0010\n",
      "Epoch 153/1000\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2099 - val_loss: 0.2187 - lr: 0.0010\n",
      "Epoch 154/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2086 - val_loss: 0.2158 - lr: 0.0010\n",
      "Epoch 155/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2096 - val_loss: 0.2221 - lr: 0.0010\n",
      "Epoch 156/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2099 - val_loss: 0.2159 - lr: 0.0010\n",
      "Epoch 157/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2107 - val_loss: 0.2138 - lr: 0.0010\n",
      "Epoch 158/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2085 - val_loss: 0.2152 - lr: 0.0010\n",
      "Epoch 159/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2117 - val_loss: 0.2190 - lr: 0.0010\n",
      "Epoch 160/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2104 - val_loss: 0.2134 - lr: 0.0010\n",
      "Epoch 161/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2089 - val_loss: 0.2135 - lr: 0.0010\n",
      "Epoch 162/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2086 - val_loss: 0.2130 - lr: 0.0010\n",
      "Epoch 163/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2087 - val_loss: 0.2143 - lr: 0.0010\n",
      "Epoch 164/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2088 - val_loss: 0.2130 - lr: 0.0010\n",
      "Epoch 165/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2087 - val_loss: 0.2129 - lr: 0.0010\n",
      "Epoch 166/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2087 - val_loss: 0.2198 - lr: 0.0010\n",
      "Epoch 167/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2091 - val_loss: 0.2127 - lr: 0.0010\n",
      "Epoch 168/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2084 - val_loss: 0.2173 - lr: 0.0010\n",
      "Epoch 169/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2095 - val_loss: 0.2144 - lr: 0.0010\n",
      "Epoch 170/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2090 - val_loss: 0.2128 - lr: 0.0010\n",
      "Epoch 171/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2087 - val_loss: 0.2125 - lr: 0.0010\n",
      "Epoch 172/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2086 - val_loss: 0.2171 - lr: 0.0010\n",
      "Epoch 173/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2099 - val_loss: 0.2143 - lr: 0.0010\n",
      "Epoch 174/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2079 - val_loss: 0.2127 - lr: 0.0010\n",
      "Epoch 175/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2100 - val_loss: 0.2175 - lr: 0.0010\n",
      "Epoch 176/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2092 - val_loss: 0.2126 - lr: 0.0010\n",
      "Epoch 177/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2085 - val_loss: 0.2124 - lr: 0.0010\n",
      "Epoch 178/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2080 - val_loss: 0.2184 - lr: 0.0010\n",
      "Epoch 179/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2086 - val_loss: 0.2166 - lr: 0.0010\n",
      "Epoch 180/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2078 - val_loss: 0.2164 - lr: 0.0010\n",
      "Epoch 181/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2081 - val_loss: 0.2130 - lr: 0.0010\n",
      "Epoch 182/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2091 - val_loss: 0.2165 - lr: 0.0010\n",
      "Epoch 183/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2091 - val_loss: 0.2142 - lr: 0.0010\n",
      "Epoch 184/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2078 - val_loss: 0.2140 - lr: 0.0010\n",
      "Epoch 185/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2105 - val_loss: 0.2131 - lr: 0.0010\n",
      "Epoch 186/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2075 - val_loss: 0.2139 - lr: 0.0010\n",
      "Epoch 187/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2063 - val_loss: 0.2130 - lr: 5.0000e-04\n",
      "Epoch 188/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2061 - val_loss: 0.2119 - lr: 5.0000e-04\n",
      "Epoch 189/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2062 - val_loss: 0.2147 - lr: 5.0000e-04\n",
      "Epoch 190/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2064 - val_loss: 0.2131 - lr: 5.0000e-04\n",
      "Epoch 191/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2058 - val_loss: 0.2124 - lr: 5.0000e-04\n",
      "Epoch 192/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2066 - val_loss: 0.2127 - lr: 5.0000e-04\n",
      "Epoch 193/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2062 - val_loss: 0.2130 - lr: 5.0000e-04\n",
      "Epoch 194/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2060 - val_loss: 0.2127 - lr: 5.0000e-04\n",
      "Epoch 195/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2066 - val_loss: 0.2117 - lr: 5.0000e-04\n",
      "Epoch 196/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2060 - val_loss: 0.2131 - lr: 5.0000e-04\n",
      "Epoch 197/1000\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.2062 - val_loss: 0.2126 - lr: 5.0000e-04\n",
      "Epoch 198/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2064 - val_loss: 0.2120 - lr: 5.0000e-04\n",
      "Epoch 199/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2060 - val_loss: 0.2124 - lr: 5.0000e-04\n",
      "Epoch 200/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2061 - val_loss: 0.2124 - lr: 5.0000e-04\n",
      "Epoch 201/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2064 - val_loss: 0.2133 - lr: 5.0000e-04\n",
      "Epoch 202/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2060 - val_loss: 0.2127 - lr: 5.0000e-04\n",
      "Epoch 203/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2066 - val_loss: 0.2121 - lr: 5.0000e-04\n",
      "Epoch 204/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2056 - val_loss: 0.2120 - lr: 5.0000e-04\n",
      "Epoch 205/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2064 - val_loss: 0.2143 - lr: 5.0000e-04\n",
      "Epoch 206/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2062 - val_loss: 0.2164 - lr: 5.0000e-04\n",
      "Epoch 207/1000\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2057 - val_loss: 0.2118 - lr: 5.0000e-04\n",
      "Epoch 208/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2054 - val_loss: 0.2126 - lr: 5.0000e-04\n",
      "Epoch 209/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2061 - val_loss: 0.2132 - lr: 5.0000e-04\n",
      "Epoch 210/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2056 - val_loss: 0.2131 - lr: 5.0000e-04\n",
      "Epoch 211/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2048 - val_loss: 0.2140 - lr: 2.5000e-04\n",
      "Epoch 212/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2049 - val_loss: 0.2121 - lr: 2.5000e-04\n",
      "Epoch 213/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2049 - val_loss: 0.2124 - lr: 2.5000e-04\n",
      "Epoch 214/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2047 - val_loss: 0.2123 - lr: 2.5000e-04\n",
      "Epoch 215/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2046 - val_loss: 0.2132 - lr: 2.5000e-04\n",
      "Epoch 216/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2046 - val_loss: 0.2123 - lr: 2.5000e-04\n",
      "Epoch 217/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2047 - val_loss: 0.2128 - lr: 2.5000e-04\n",
      "Epoch 218/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2046 - val_loss: 0.2123 - lr: 2.5000e-04\n",
      "Epoch 219/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2047 - val_loss: 0.2122 - lr: 2.5000e-04\n",
      "Epoch 220/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2051 - val_loss: 0.2121 - lr: 2.5000e-04\n",
      "Epoch 221/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2046 - val_loss: 0.2148 - lr: 2.5000e-04\n",
      "Epoch 222/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2047 - val_loss: 0.2170 - lr: 2.5000e-04\n",
      "Epoch 223/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2049 - val_loss: 0.2119 - lr: 2.5000e-04\n",
      "Epoch 224/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2047 - val_loss: 0.2126 - lr: 2.5000e-04\n",
      "Epoch 225/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2048 - val_loss: 0.2126 - lr: 2.5000e-04\n",
      "Epoch 226/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2042 - val_loss: 0.2119 - lr: 1.2500e-04\n",
      "Epoch 227/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2043 - val_loss: 0.2125 - lr: 1.2500e-04\n",
      "Epoch 228/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2041 - val_loss: 0.2126 - lr: 1.2500e-04\n",
      "Epoch 229/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2042 - val_loss: 0.2125 - lr: 1.2500e-04\n",
      "Epoch 230/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2041 - val_loss: 0.2118 - lr: 1.2500e-04\n",
      "Epoch 231/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2041 - val_loss: 0.2121 - lr: 1.2500e-04\n",
      "Epoch 232/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2041 - val_loss: 0.2122 - lr: 1.2500e-04\n",
      "Epoch 233/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2045 - val_loss: 0.2131 - lr: 1.2500e-04\n",
      "Epoch 234/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2043 - val_loss: 0.2126 - lr: 1.2500e-04\n",
      "Epoch 235/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2044 - val_loss: 0.2123 - lr: 1.2500e-04\n",
      "Epoch 236/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2040 - val_loss: 0.2122 - lr: 1.2500e-04\n",
      "Epoch 237/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2041 - val_loss: 0.2119 - lr: 1.2500e-04\n",
      "Epoch 238/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2042 - val_loss: 0.2122 - lr: 1.2500e-04\n",
      "Epoch 239/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2040 - val_loss: 0.2128 - lr: 1.2500e-04\n",
      "Epoch 240/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2040 - val_loss: 0.2131 - lr: 1.2500e-04\n",
      "Epoch 241/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2038 - val_loss: 0.2125 - lr: 6.2500e-05\n",
      "Epoch 242/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2039 - val_loss: 0.2119 - lr: 6.2500e-05\n",
      "Epoch 243/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2039 - val_loss: 0.2129 - lr: 6.2500e-05\n",
      "Epoch 244/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2040 - val_loss: 0.2121 - lr: 6.2500e-05\n",
      "Epoch 245/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2038 - val_loss: 0.2123 - lr: 6.2500e-05\n",
      "2681/2681 [==============================] - 5s 2ms/step\n",
      "Fold 1 NN: 0.21173\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "Training fold 2\n",
      "(343146, 18)\n",
      "(85786, 18)\n",
      "Epoch 1/1000\n",
      "168/168 [==============================] - 5s 15ms/step - loss: 0.3764 - val_loss: 0.2472 - lr: 0.0010\n",
      "Epoch 2/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2517 - val_loss: 0.2377 - lr: 0.0010\n",
      "Epoch 3/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2473 - val_loss: 0.2353 - lr: 0.0010\n",
      "Epoch 4/1000\n",
      "168/168 [==============================] - 3s 16ms/step - loss: 0.2387 - val_loss: 0.2656 - lr: 0.0010\n",
      "Epoch 5/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2393 - val_loss: 0.2384 - lr: 0.0010\n",
      "Epoch 6/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2407 - val_loss: 0.2543 - lr: 0.0010\n",
      "Epoch 7/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2503 - val_loss: 0.2340 - lr: 0.0010\n",
      "Epoch 8/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2355 - val_loss: 0.2312 - lr: 0.0010\n",
      "Epoch 9/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2358 - val_loss: 0.2525 - lr: 0.0010\n",
      "Epoch 10/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2565 - val_loss: 0.2343 - lr: 0.0010\n",
      "Epoch 11/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2413 - val_loss: 0.2303 - lr: 0.0010\n",
      "Epoch 12/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2367 - val_loss: 0.2302 - lr: 0.0010\n",
      "Epoch 13/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2304 - val_loss: 0.2318 - lr: 0.0010\n",
      "Epoch 14/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2286 - val_loss: 0.2310 - lr: 0.0010\n",
      "Epoch 15/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2279 - val_loss: 0.2292 - lr: 0.0010\n",
      "Epoch 16/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2274 - val_loss: 0.2295 - lr: 0.0010\n",
      "Epoch 17/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2307 - val_loss: 0.2309 - lr: 0.0010\n",
      "Epoch 18/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2259 - val_loss: 0.2939 - lr: 0.0010\n",
      "Epoch 19/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2451 - val_loss: 0.2304 - lr: 0.0010\n",
      "Epoch 20/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2267 - val_loss: 0.2263 - lr: 0.0010\n",
      "Epoch 21/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2298 - val_loss: 0.2280 - lr: 0.0010\n",
      "Epoch 22/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2316 - val_loss: 0.2352 - lr: 0.0010\n",
      "Epoch 23/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2306 - val_loss: 0.2281 - lr: 0.0010\n",
      "Epoch 24/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2360 - val_loss: 0.2598 - lr: 0.0010\n",
      "Epoch 25/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2274 - val_loss: 0.2276 - lr: 0.0010\n",
      "Epoch 26/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2291 - val_loss: 0.2276 - lr: 0.0010\n",
      "Epoch 27/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2256 - val_loss: 0.2269 - lr: 0.0010\n",
      "Epoch 28/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2291 - val_loss: 0.2924 - lr: 0.0010\n",
      "Epoch 29/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2299 - val_loss: 0.2270 - lr: 0.0010\n",
      "Epoch 30/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2305 - val_loss: 0.2250 - lr: 0.0010\n",
      "Epoch 31/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2246 - val_loss: 0.2258 - lr: 0.0010\n",
      "Epoch 32/1000\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2235 - val_loss: 0.2243 - lr: 0.0010\n",
      "Epoch 33/1000\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2231 - val_loss: 0.2246 - lr: 0.0010\n",
      "Epoch 34/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2249 - val_loss: 0.2221 - lr: 0.0010\n",
      "Epoch 35/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2240 - val_loss: 0.2287 - lr: 0.0010\n",
      "Epoch 36/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2226 - val_loss: 0.2232 - lr: 0.0010\n",
      "Epoch 37/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2289 - val_loss: 0.2329 - lr: 0.0010\n",
      "Epoch 38/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2242 - val_loss: 0.2228 - lr: 0.0010\n",
      "Epoch 39/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2230 - val_loss: 0.2246 - lr: 0.0010\n",
      "Epoch 40/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2212 - val_loss: 0.2441 - lr: 0.0010\n",
      "Epoch 41/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2224 - val_loss: 0.2225 - lr: 0.0010\n",
      "Epoch 42/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2214 - val_loss: 0.2231 - lr: 0.0010\n",
      "Epoch 43/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2220 - val_loss: 0.2214 - lr: 0.0010\n",
      "Epoch 44/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2198 - val_loss: 0.2273 - lr: 0.0010\n",
      "Epoch 45/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2215 - val_loss: 0.2319 - lr: 0.0010\n",
      "Epoch 46/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2217 - val_loss: 0.2213 - lr: 0.0010\n",
      "Epoch 47/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2201 - val_loss: 0.2258 - lr: 0.0010\n",
      "Epoch 48/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2251 - val_loss: 0.2551 - lr: 0.0010\n",
      "Epoch 49/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2225 - val_loss: 0.2306 - lr: 0.0010\n",
      "Epoch 50/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2199 - val_loss: 0.2231 - lr: 0.0010\n",
      "Epoch 51/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2192 - val_loss: 0.2294 - lr: 0.0010\n",
      "Epoch 52/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2254 - val_loss: 0.2205 - lr: 0.0010\n",
      "Epoch 53/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2181 - val_loss: 0.2208 - lr: 0.0010\n",
      "Epoch 54/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2191 - val_loss: 0.2265 - lr: 0.0010\n",
      "Epoch 55/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2181 - val_loss: 0.2213 - lr: 0.0010\n",
      "Epoch 56/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2203 - val_loss: 0.2195 - lr: 0.0010\n",
      "Epoch 57/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2236 - val_loss: 0.2202 - lr: 0.0010\n",
      "Epoch 58/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2177 - val_loss: 0.2194 - lr: 0.0010\n",
      "Epoch 59/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2247 - val_loss: 0.2253 - lr: 0.0010\n",
      "Epoch 60/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2159 - val_loss: 0.2210 - lr: 0.0010\n",
      "Epoch 61/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2167 - val_loss: 0.2276 - lr: 0.0010\n",
      "Epoch 62/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2184 - val_loss: 0.2205 - lr: 0.0010\n",
      "Epoch 63/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2182 - val_loss: 0.2229 - lr: 0.0010\n",
      "Epoch 64/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2171 - val_loss: 0.2201 - lr: 0.0010\n",
      "Epoch 65/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2175 - val_loss: 0.2249 - lr: 0.0010\n",
      "Epoch 66/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2176 - val_loss: 0.2201 - lr: 0.0010\n",
      "Epoch 67/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2175 - val_loss: 0.2187 - lr: 0.0010\n",
      "Epoch 68/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2181 - val_loss: 0.2186 - lr: 0.0010\n",
      "Epoch 69/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2204 - val_loss: 0.2211 - lr: 0.0010\n",
      "Epoch 70/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2157 - val_loss: 0.2225 - lr: 0.0010\n",
      "Epoch 71/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2185 - val_loss: 0.2199 - lr: 0.0010\n",
      "Epoch 72/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2176 - val_loss: 0.2235 - lr: 0.0010\n",
      "Epoch 73/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2184 - val_loss: 0.2211 - lr: 0.0010\n",
      "Epoch 74/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2160 - val_loss: 0.2239 - lr: 0.0010\n",
      "Epoch 75/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2163 - val_loss: 0.2194 - lr: 0.0010\n",
      "Epoch 76/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2180 - val_loss: 0.2216 - lr: 0.0010\n",
      "Epoch 77/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2155 - val_loss: 0.2184 - lr: 0.0010\n",
      "Epoch 78/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2179 - val_loss: 0.2212 - lr: 0.0010\n",
      "Epoch 79/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2167 - val_loss: 0.2191 - lr: 0.0010\n",
      "Epoch 80/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2158 - val_loss: 0.2252 - lr: 0.0010\n",
      "Epoch 81/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2158 - val_loss: 0.2268 - lr: 0.0010\n",
      "Epoch 82/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2148 - val_loss: 0.2202 - lr: 0.0010\n",
      "Epoch 83/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2156 - val_loss: 0.2597 - lr: 0.0010\n",
      "Epoch 84/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2165 - val_loss: 0.2191 - lr: 0.0010\n",
      "Epoch 85/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2159 - val_loss: 0.2196 - lr: 0.0010\n",
      "Epoch 86/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2156 - val_loss: 0.2183 - lr: 0.0010\n",
      "Epoch 87/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2138 - val_loss: 0.2196 - lr: 0.0010\n",
      "Epoch 88/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2156 - val_loss: 0.2198 - lr: 0.0010\n",
      "Epoch 89/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2172 - val_loss: 0.2229 - lr: 0.0010\n",
      "Epoch 90/1000\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2148 - val_loss: 0.2220 - lr: 0.0010\n",
      "Epoch 91/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2163 - val_loss: 0.2187 - lr: 0.0010\n",
      "Epoch 92/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2146 - val_loss: 0.2179 - lr: 0.0010\n",
      "Epoch 93/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2142 - val_loss: 0.2327 - lr: 0.0010\n",
      "Epoch 94/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2139 - val_loss: 0.2282 - lr: 0.0010\n",
      "Epoch 95/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2150 - val_loss: 0.2180 - lr: 0.0010\n",
      "Epoch 96/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2157 - val_loss: 0.2181 - lr: 0.0010\n",
      "Epoch 97/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2142 - val_loss: 0.2178 - lr: 0.0010\n",
      "Epoch 98/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2152 - val_loss: 0.2194 - lr: 0.0010\n",
      "Epoch 99/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2146 - val_loss: 0.2189 - lr: 0.0010\n",
      "Epoch 100/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2147 - val_loss: 0.2209 - lr: 0.0010\n",
      "Epoch 101/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2145 - val_loss: 0.2180 - lr: 0.0010\n",
      "Epoch 102/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2142 - val_loss: 0.2190 - lr: 0.0010\n",
      "Epoch 103/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2134 - val_loss: 0.2212 - lr: 0.0010\n",
      "Epoch 104/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2148 - val_loss: 0.2168 - lr: 0.0010\n",
      "Epoch 105/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2146 - val_loss: 0.2169 - lr: 0.0010\n",
      "Epoch 106/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2143 - val_loss: 0.2272 - lr: 0.0010\n",
      "Epoch 107/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2125 - val_loss: 0.2172 - lr: 0.0010\n",
      "Epoch 108/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2155 - val_loss: 0.2275 - lr: 0.0010\n",
      "Epoch 109/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2136 - val_loss: 0.2173 - lr: 0.0010\n",
      "Epoch 110/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2137 - val_loss: 0.2185 - lr: 0.0010\n",
      "Epoch 111/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2126 - val_loss: 0.2175 - lr: 0.0010\n",
      "Epoch 112/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2122 - val_loss: 0.2170 - lr: 0.0010\n",
      "Epoch 113/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2141 - val_loss: 0.2172 - lr: 0.0010\n",
      "Epoch 114/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2145 - val_loss: 0.2179 - lr: 0.0010\n",
      "Epoch 115/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2138 - val_loss: 0.2213 - lr: 0.0010\n",
      "Epoch 116/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2124 - val_loss: 0.2282 - lr: 0.0010\n",
      "Epoch 117/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2135 - val_loss: 0.2211 - lr: 0.0010\n",
      "Epoch 118/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2138 - val_loss: 0.2165 - lr: 0.0010\n",
      "Epoch 119/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2116 - val_loss: 0.2182 - lr: 0.0010\n",
      "Epoch 120/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2134 - val_loss: 0.2198 - lr: 0.0010\n",
      "Epoch 121/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2123 - val_loss: 0.2164 - lr: 0.0010\n",
      "Epoch 122/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2121 - val_loss: 0.2174 - lr: 0.0010\n",
      "Epoch 123/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2136 - val_loss: 0.2198 - lr: 0.0010\n",
      "Epoch 124/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2118 - val_loss: 0.2239 - lr: 0.0010\n",
      "Epoch 125/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2118 - val_loss: 0.2191 - lr: 0.0010\n",
      "Epoch 126/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2141 - val_loss: 0.2265 - lr: 0.0010\n",
      "Epoch 127/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2118 - val_loss: 0.2219 - lr: 0.0010\n",
      "Epoch 128/1000\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2111 - val_loss: 0.2164 - lr: 0.0010\n",
      "Epoch 129/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2118 - val_loss: 0.2226 - lr: 0.0010\n",
      "Epoch 130/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2132 - val_loss: 0.2155 - lr: 0.0010\n",
      "Epoch 131/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2127 - val_loss: 0.2170 - lr: 0.0010\n",
      "Epoch 132/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2129 - val_loss: 0.2226 - lr: 0.0010\n",
      "Epoch 133/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2111 - val_loss: 0.2221 - lr: 0.0010\n",
      "Epoch 134/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2134 - val_loss: 0.2157 - lr: 0.0010\n",
      "Epoch 135/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2119 - val_loss: 0.2165 - lr: 0.0010\n",
      "Epoch 136/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2104 - val_loss: 0.2168 - lr: 0.0010\n",
      "Epoch 137/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2111 - val_loss: 0.2157 - lr: 0.0010\n",
      "Epoch 138/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2117 - val_loss: 0.2168 - lr: 0.0010\n",
      "Epoch 139/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2117 - val_loss: 0.2165 - lr: 0.0010\n",
      "Epoch 140/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2112 - val_loss: 0.2234 - lr: 0.0010\n",
      "Epoch 141/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2133 - val_loss: 0.2219 - lr: 0.0010\n",
      "Epoch 142/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2109 - val_loss: 0.2160 - lr: 0.0010\n",
      "Epoch 143/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2117 - val_loss: 0.2172 - lr: 0.0010\n",
      "Epoch 144/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2120 - val_loss: 0.2176 - lr: 0.0010\n",
      "Epoch 145/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2106 - val_loss: 0.2156 - lr: 0.0010\n",
      "Epoch 146/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2088 - val_loss: 0.2142 - lr: 5.0000e-04\n",
      "Epoch 147/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2090 - val_loss: 0.2158 - lr: 5.0000e-04\n",
      "Epoch 148/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2090 - val_loss: 0.2163 - lr: 5.0000e-04\n",
      "Epoch 149/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2086 - val_loss: 0.2158 - lr: 5.0000e-04\n",
      "Epoch 150/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2092 - val_loss: 0.2218 - lr: 5.0000e-04\n",
      "Epoch 151/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2089 - val_loss: 0.2161 - lr: 5.0000e-04\n",
      "Epoch 152/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2087 - val_loss: 0.2171 - lr: 5.0000e-04\n",
      "Epoch 153/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2091 - val_loss: 0.2147 - lr: 5.0000e-04\n",
      "Epoch 154/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2085 - val_loss: 0.2170 - lr: 5.0000e-04\n",
      "Epoch 155/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2096 - val_loss: 0.2225 - lr: 5.0000e-04\n",
      "Epoch 156/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2094 - val_loss: 0.2160 - lr: 5.0000e-04\n",
      "Epoch 157/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2085 - val_loss: 0.2151 - lr: 5.0000e-04\n",
      "Epoch 158/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2088 - val_loss: 0.2150 - lr: 5.0000e-04\n",
      "Epoch 159/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2082 - val_loss: 0.2140 - lr: 5.0000e-04\n",
      "Epoch 160/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2087 - val_loss: 0.2166 - lr: 5.0000e-04\n",
      "Epoch 161/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2089 - val_loss: 0.2155 - lr: 5.0000e-04\n",
      "Epoch 162/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2088 - val_loss: 0.2156 - lr: 5.0000e-04\n",
      "Epoch 163/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2086 - val_loss: 0.2158 - lr: 5.0000e-04\n",
      "Epoch 164/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2086 - val_loss: 0.2195 - lr: 5.0000e-04\n",
      "Epoch 165/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2089 - val_loss: 0.2168 - lr: 5.0000e-04\n",
      "Epoch 166/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2086 - val_loss: 0.2216 - lr: 5.0000e-04\n",
      "Epoch 167/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2094 - val_loss: 0.2151 - lr: 5.0000e-04\n",
      "Epoch 168/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2086 - val_loss: 0.2148 - lr: 5.0000e-04\n",
      "Epoch 169/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2087 - val_loss: 0.2194 - lr: 5.0000e-04\n",
      "Epoch 170/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2086 - val_loss: 0.2144 - lr: 5.0000e-04\n",
      "Epoch 171/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2089 - val_loss: 0.2145 - lr: 5.0000e-04\n",
      "Epoch 172/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2086 - val_loss: 0.2151 - lr: 5.0000e-04\n",
      "Epoch 173/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2085 - val_loss: 0.2149 - lr: 5.0000e-04\n",
      "Epoch 174/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2087 - val_loss: 0.2229 - lr: 5.0000e-04\n",
      "Epoch 175/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2073 - val_loss: 0.2143 - lr: 2.5000e-04\n",
      "Epoch 176/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2073 - val_loss: 0.2156 - lr: 2.5000e-04\n",
      "Epoch 177/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2073 - val_loss: 0.2158 - lr: 2.5000e-04\n",
      "Epoch 178/1000\n",
      "168/168 [==============================] - 3s 16ms/step - loss: 0.2074 - val_loss: 0.2146 - lr: 2.5000e-04\n",
      "Epoch 179/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2075 - val_loss: 0.2145 - lr: 2.5000e-04\n",
      "Epoch 180/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2073 - val_loss: 0.2161 - lr: 2.5000e-04\n",
      "Epoch 181/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2075 - val_loss: 0.2145 - lr: 2.5000e-04\n",
      "Epoch 182/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2072 - val_loss: 0.2145 - lr: 2.5000e-04\n",
      "Epoch 183/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2075 - val_loss: 0.2141 - lr: 2.5000e-04\n",
      "Epoch 184/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2075 - val_loss: 0.2141 - lr: 2.5000e-04\n",
      "Epoch 185/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2072 - val_loss: 0.2159 - lr: 2.5000e-04\n",
      "Epoch 186/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2074 - val_loss: 0.2150 - lr: 2.5000e-04\n",
      "Epoch 187/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2071 - val_loss: 0.2149 - lr: 2.5000e-04\n",
      "Epoch 188/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2072 - val_loss: 0.2149 - lr: 2.5000e-04\n",
      "Epoch 189/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2074 - val_loss: 0.2201 - lr: 2.5000e-04\n",
      "Epoch 190/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2069 - val_loss: 0.2142 - lr: 1.2500e-04\n",
      "Epoch 191/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2068 - val_loss: 0.2139 - lr: 1.2500e-04\n",
      "Epoch 192/1000\n",
      "168/168 [==============================] - 3s 17ms/step - loss: 0.2066 - val_loss: 0.2170 - lr: 1.2500e-04\n",
      "Epoch 193/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2069 - val_loss: 0.2136 - lr: 1.2500e-04\n",
      "Epoch 194/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2067 - val_loss: 0.2143 - lr: 1.2500e-04\n",
      "Epoch 195/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2067 - val_loss: 0.2141 - lr: 1.2500e-04\n",
      "Epoch 196/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2066 - val_loss: 0.2143 - lr: 1.2500e-04\n",
      "Epoch 197/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2068 - val_loss: 0.2146 - lr: 1.2500e-04\n",
      "Epoch 198/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2067 - val_loss: 0.2143 - lr: 1.2500e-04\n",
      "Epoch 199/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2067 - val_loss: 0.2146 - lr: 1.2500e-04\n",
      "Epoch 200/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2068 - val_loss: 0.2141 - lr: 1.2500e-04\n",
      "Epoch 201/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2066 - val_loss: 0.2141 - lr: 1.2500e-04\n",
      "Epoch 202/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2068 - val_loss: 0.2142 - lr: 1.2500e-04\n",
      "Epoch 203/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2067 - val_loss: 0.2141 - lr: 1.2500e-04\n",
      "Epoch 204/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2071 - val_loss: 0.2148 - lr: 1.2500e-04\n",
      "Epoch 205/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2067 - val_loss: 0.2141 - lr: 1.2500e-04\n",
      "Epoch 206/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2066 - val_loss: 0.2138 - lr: 1.2500e-04\n",
      "Epoch 207/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2068 - val_loss: 0.2141 - lr: 1.2500e-04\n",
      "Epoch 208/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2070 - val_loss: 0.2146 - lr: 1.2500e-04\n",
      "Epoch 209/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2064 - val_loss: 0.2143 - lr: 6.2500e-05\n",
      "Epoch 210/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2064 - val_loss: 0.2143 - lr: 6.2500e-05\n",
      "Epoch 211/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2065 - val_loss: 0.2143 - lr: 6.2500e-05\n",
      "Epoch 212/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2064 - val_loss: 0.2144 - lr: 6.2500e-05\n",
      "Epoch 213/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2064 - val_loss: 0.2140 - lr: 6.2500e-05\n",
      "Epoch 214/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2063 - val_loss: 0.2140 - lr: 6.2500e-05\n",
      "Epoch 215/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2064 - val_loss: 0.2142 - lr: 6.2500e-05\n",
      "Epoch 216/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2064 - val_loss: 0.2141 - lr: 6.2500e-05\n",
      "Epoch 217/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2063 - val_loss: 0.2145 - lr: 6.2500e-05\n",
      "Epoch 218/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2064 - val_loss: 0.2139 - lr: 6.2500e-05\n",
      "Epoch 219/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2064 - val_loss: 0.2142 - lr: 6.2500e-05\n",
      "Epoch 220/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2063 - val_loss: 0.2141 - lr: 6.2500e-05\n",
      "Epoch 221/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2063 - val_loss: 0.2140 - lr: 6.2500e-05\n",
      "Epoch 222/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2063 - val_loss: 0.2141 - lr: 6.2500e-05\n",
      "Epoch 223/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2063 - val_loss: 0.2140 - lr: 6.2500e-05\n",
      "Epoch 224/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2062 - val_loss: 0.2141 - lr: 3.1250e-05\n",
      "Epoch 225/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2062 - val_loss: 0.2142 - lr: 3.1250e-05\n",
      "Epoch 226/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2062 - val_loss: 0.2144 - lr: 3.1250e-05\n",
      "Epoch 227/1000\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2062 - val_loss: 0.2144 - lr: 3.1250e-05\n",
      "Epoch 228/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2061 - val_loss: 0.2142 - lr: 3.1250e-05\n",
      "Epoch 229/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2062 - val_loss: 0.2141 - lr: 3.1250e-05\n",
      "Epoch 230/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2062 - val_loss: 0.2140 - lr: 3.1250e-05\n",
      "Epoch 231/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2062 - val_loss: 0.2140 - lr: 3.1250e-05\n",
      "Epoch 232/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2061 - val_loss: 0.2142 - lr: 3.1250e-05\n",
      "Epoch 233/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2062 - val_loss: 0.2141 - lr: 3.1250e-05\n",
      "Epoch 234/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2061 - val_loss: 0.2144 - lr: 3.1250e-05\n",
      "Epoch 235/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2062 - val_loss: 0.2142 - lr: 3.1250e-05\n",
      "Epoch 236/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2061 - val_loss: 0.2141 - lr: 3.1250e-05\n",
      "Epoch 237/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2062 - val_loss: 0.2141 - lr: 3.1250e-05\n",
      "Epoch 238/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2061 - val_loss: 0.2141 - lr: 3.1250e-05\n",
      "Epoch 239/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2061 - val_loss: 0.2140 - lr: 1.5625e-05\n",
      "Epoch 240/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2061 - val_loss: 0.2140 - lr: 1.5625e-05\n",
      "Epoch 241/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2061 - val_loss: 0.2140 - lr: 1.5625e-05\n",
      "Epoch 242/1000\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2061 - val_loss: 0.2142 - lr: 1.5625e-05\n",
      "Epoch 243/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2060 - val_loss: 0.2141 - lr: 1.5625e-05\n",
      "2681/2681 [==============================] - 5s 2ms/step\n",
      "Fold 2 NN: 0.2136\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "Training fold 3\n",
      "(343146, 18)\n",
      "(85786, 18)\n",
      "Epoch 1/1000\n",
      "168/168 [==============================] - 5s 15ms/step - loss: 0.4063 - val_loss: 0.2698 - lr: 0.0010\n",
      "Epoch 2/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2571 - val_loss: 0.2491 - lr: 0.0010\n",
      "Epoch 3/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2442 - val_loss: 0.2478 - lr: 0.0010\n",
      "Epoch 4/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2504 - val_loss: 0.2444 - lr: 0.0010\n",
      "Epoch 5/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2421 - val_loss: 0.2430 - lr: 0.0010\n",
      "Epoch 6/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2405 - val_loss: 0.2415 - lr: 0.0010\n",
      "Epoch 7/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2412 - val_loss: 0.2430 - lr: 0.0010\n",
      "Epoch 8/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2367 - val_loss: 0.2437 - lr: 0.0010\n",
      "Epoch 9/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2346 - val_loss: 0.2418 - lr: 0.0010\n",
      "Epoch 10/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2358 - val_loss: 0.2606 - lr: 0.0010\n",
      "Epoch 11/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2324 - val_loss: 0.2587 - lr: 0.0010\n",
      "Epoch 12/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2334 - val_loss: 0.2408 - lr: 0.0010\n",
      "Epoch 13/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2339 - val_loss: 0.2437 - lr: 0.0010\n",
      "Epoch 14/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2281 - val_loss: 0.3373 - lr: 0.0010\n",
      "Epoch 15/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2415 - val_loss: 0.2363 - lr: 0.0010\n",
      "Epoch 16/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2289 - val_loss: 0.2473 - lr: 0.0010\n",
      "Epoch 17/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2315 - val_loss: 0.2516 - lr: 0.0010\n",
      "Epoch 18/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2266 - val_loss: 0.2340 - lr: 0.0010\n",
      "Epoch 19/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2279 - val_loss: 0.2396 - lr: 0.0010\n",
      "Epoch 20/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2254 - val_loss: 0.2329 - lr: 0.0010\n",
      "Epoch 21/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2243 - val_loss: 0.2310 - lr: 0.0010\n",
      "Epoch 22/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2268 - val_loss: 0.2359 - lr: 0.0010\n",
      "Epoch 23/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2283 - val_loss: 0.2378 - lr: 0.0010\n",
      "Epoch 24/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2242 - val_loss: 0.2298 - lr: 0.0010\n",
      "Epoch 25/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2255 - val_loss: 0.2304 - lr: 0.0010\n",
      "Epoch 26/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2307 - val_loss: 0.2708 - lr: 0.0010\n",
      "Epoch 27/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2265 - val_loss: 0.2290 - lr: 0.0010\n",
      "Epoch 28/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2238 - val_loss: 0.2390 - lr: 0.0010\n",
      "Epoch 29/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2230 - val_loss: 0.2605 - lr: 0.0010\n",
      "Epoch 30/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2226 - val_loss: 0.2293 - lr: 0.0010\n",
      "Epoch 31/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2211 - val_loss: 0.2278 - lr: 0.0010\n",
      "Epoch 32/1000\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.2218 - val_loss: 0.2620 - lr: 0.0010\n",
      "Epoch 33/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2225 - val_loss: 0.2264 - lr: 0.0010\n",
      "Epoch 34/1000\n",
      "168/168 [==============================] - 3s 17ms/step - loss: 0.2225 - val_loss: 0.2288 - lr: 0.0010\n",
      "Epoch 35/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2207 - val_loss: 0.2275 - lr: 0.0010\n",
      "Epoch 36/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2192 - val_loss: 0.2299 - lr: 0.0010\n",
      "Epoch 37/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2218 - val_loss: 0.2257 - lr: 0.0010\n",
      "Epoch 38/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2280 - val_loss: 0.2273 - lr: 0.0010\n",
      "Epoch 39/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2231 - val_loss: 0.2303 - lr: 0.0010\n",
      "Epoch 40/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2199 - val_loss: 0.2296 - lr: 0.0010\n",
      "Epoch 41/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2186 - val_loss: 0.2274 - lr: 0.0010\n",
      "Epoch 42/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2211 - val_loss: 0.2251 - lr: 0.0010\n",
      "Epoch 43/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2174 - val_loss: 0.2245 - lr: 0.0010\n",
      "Epoch 44/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2233 - val_loss: 0.2251 - lr: 0.0010\n",
      "Epoch 45/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2184 - val_loss: 0.2319 - lr: 0.0010\n",
      "Epoch 46/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2181 - val_loss: 0.2278 - lr: 0.0010\n",
      "Epoch 47/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2198 - val_loss: 0.2258 - lr: 0.0010\n",
      "Epoch 48/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2164 - val_loss: 0.2257 - lr: 0.0010\n",
      "Epoch 49/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2168 - val_loss: 0.2362 - lr: 0.0010\n",
      "Epoch 50/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2190 - val_loss: 0.2270 - lr: 0.0010\n",
      "Epoch 51/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2192 - val_loss: 0.2317 - lr: 0.0010\n",
      "Epoch 52/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2210 - val_loss: 0.2243 - lr: 0.0010\n",
      "Epoch 53/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2160 - val_loss: 0.2236 - lr: 0.0010\n",
      "Epoch 54/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2179 - val_loss: 0.2273 - lr: 0.0010\n",
      "Epoch 55/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2165 - val_loss: 0.2329 - lr: 0.0010\n",
      "Epoch 56/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2182 - val_loss: 0.2240 - lr: 0.0010\n",
      "Epoch 57/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2161 - val_loss: 0.2234 - lr: 0.0010\n",
      "Epoch 58/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2175 - val_loss: 0.2243 - lr: 0.0010\n",
      "Epoch 59/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2161 - val_loss: 0.2235 - lr: 0.0010\n",
      "Epoch 60/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2174 - val_loss: 0.2260 - lr: 0.0010\n",
      "Epoch 61/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2162 - val_loss: 0.2232 - lr: 0.0010\n",
      "Epoch 62/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2177 - val_loss: 0.2229 - lr: 0.0010\n",
      "Epoch 63/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2158 - val_loss: 0.2314 - lr: 0.0010\n",
      "Epoch 64/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2150 - val_loss: 0.2578 - lr: 0.0010\n",
      "Epoch 65/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2165 - val_loss: 0.2231 - lr: 0.0010\n",
      "Epoch 66/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2171 - val_loss: 0.2240 - lr: 0.0010\n",
      "Epoch 67/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2141 - val_loss: 0.2258 - lr: 0.0010\n",
      "Epoch 68/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2138 - val_loss: 0.2265 - lr: 0.0010\n",
      "Epoch 69/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2163 - val_loss: 0.2342 - lr: 0.0010\n",
      "Epoch 70/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2168 - val_loss: 0.2231 - lr: 0.0010\n",
      "Epoch 71/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2155 - val_loss: 0.2269 - lr: 0.0010\n",
      "Epoch 72/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2148 - val_loss: 0.2239 - lr: 0.0010\n",
      "Epoch 73/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2144 - val_loss: 0.2294 - lr: 0.0010\n",
      "Epoch 74/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2142 - val_loss: 0.2231 - lr: 0.0010\n",
      "Epoch 75/1000\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.2145 - val_loss: 0.2263 - lr: 0.0010\n",
      "Epoch 76/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2140 - val_loss: 0.2288 - lr: 0.0010\n",
      "Epoch 77/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2149 - val_loss: 0.2279 - lr: 0.0010\n",
      "Epoch 78/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2117 - val_loss: 0.2212 - lr: 5.0000e-04\n",
      "Epoch 79/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2121 - val_loss: 0.2228 - lr: 5.0000e-04\n",
      "Epoch 80/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2118 - val_loss: 0.2219 - lr: 5.0000e-04\n",
      "Epoch 81/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2109 - val_loss: 0.2219 - lr: 5.0000e-04\n",
      "Epoch 82/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2115 - val_loss: 0.2229 - lr: 5.0000e-04\n",
      "Epoch 83/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2113 - val_loss: 0.2216 - lr: 5.0000e-04\n",
      "Epoch 84/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2115 - val_loss: 0.2229 - lr: 5.0000e-04\n",
      "Epoch 85/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2116 - val_loss: 0.2235 - lr: 5.0000e-04\n",
      "Epoch 86/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2116 - val_loss: 0.2230 - lr: 5.0000e-04\n",
      "Epoch 87/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2118 - val_loss: 0.2244 - lr: 5.0000e-04\n",
      "Epoch 88/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2122 - val_loss: 0.2216 - lr: 5.0000e-04\n",
      "Epoch 89/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2115 - val_loss: 0.2206 - lr: 5.0000e-04\n",
      "Epoch 90/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2114 - val_loss: 0.2251 - lr: 5.0000e-04\n",
      "Epoch 91/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2112 - val_loss: 0.2279 - lr: 5.0000e-04\n",
      "Epoch 92/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2111 - val_loss: 0.2205 - lr: 5.0000e-04\n",
      "Epoch 93/1000\n",
      "168/168 [==============================] - 3s 16ms/step - loss: 0.2113 - val_loss: 0.2237 - lr: 5.0000e-04\n",
      "Epoch 94/1000\n",
      "168/168 [==============================] - 3s 16ms/step - loss: 0.2108 - val_loss: 0.2209 - lr: 5.0000e-04\n",
      "Epoch 95/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2116 - val_loss: 0.2205 - lr: 5.0000e-04\n",
      "Epoch 96/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2103 - val_loss: 0.2226 - lr: 5.0000e-04\n",
      "Epoch 97/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2106 - val_loss: 0.2214 - lr: 5.0000e-04\n",
      "Epoch 98/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2115 - val_loss: 0.2223 - lr: 5.0000e-04\n",
      "Epoch 99/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2107 - val_loss: 0.2216 - lr: 5.0000e-04\n",
      "Epoch 100/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2100 - val_loss: 0.2211 - lr: 5.0000e-04\n",
      "Epoch 101/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2106 - val_loss: 0.2219 - lr: 5.0000e-04\n",
      "Epoch 102/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2102 - val_loss: 0.2227 - lr: 5.0000e-04\n",
      "Epoch 103/1000\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2100 - val_loss: 0.2238 - lr: 5.0000e-04\n",
      "Epoch 104/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2109 - val_loss: 0.2219 - lr: 5.0000e-04\n",
      "Epoch 105/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2094 - val_loss: 0.2202 - lr: 2.5000e-04\n",
      "Epoch 106/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2091 - val_loss: 0.2200 - lr: 2.5000e-04\n",
      "Epoch 107/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2091 - val_loss: 0.2214 - lr: 2.5000e-04\n",
      "Epoch 108/1000\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.2092 - val_loss: 0.2219 - lr: 2.5000e-04\n",
      "Epoch 109/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2097 - val_loss: 0.2207 - lr: 2.5000e-04\n",
      "Epoch 110/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2094 - val_loss: 0.2204 - lr: 2.5000e-04\n",
      "Epoch 111/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2094 - val_loss: 0.2203 - lr: 2.5000e-04\n",
      "Epoch 112/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2092 - val_loss: 0.2205 - lr: 2.5000e-04\n",
      "Epoch 113/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2090 - val_loss: 0.2196 - lr: 2.5000e-04\n",
      "Epoch 114/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2091 - val_loss: 0.2217 - lr: 2.5000e-04\n",
      "Epoch 115/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2091 - val_loss: 0.2200 - lr: 2.5000e-04\n",
      "Epoch 116/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2093 - val_loss: 0.2201 - lr: 2.5000e-04\n",
      "Epoch 117/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2087 - val_loss: 0.2203 - lr: 2.5000e-04\n",
      "Epoch 118/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2087 - val_loss: 0.2210 - lr: 2.5000e-04\n",
      "Epoch 119/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2095 - val_loss: 0.2203 - lr: 2.5000e-04\n",
      "Epoch 120/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2087 - val_loss: 0.2289 - lr: 2.5000e-04\n",
      "Epoch 121/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2094 - val_loss: 0.2200 - lr: 2.5000e-04\n",
      "Epoch 122/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2087 - val_loss: 0.2203 - lr: 2.5000e-04\n",
      "Epoch 123/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2090 - val_loss: 0.2196 - lr: 2.5000e-04\n",
      "Epoch 124/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2090 - val_loss: 0.2204 - lr: 2.5000e-04\n",
      "Epoch 125/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2090 - val_loss: 0.2198 - lr: 2.5000e-04\n",
      "Epoch 126/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2086 - val_loss: 0.2212 - lr: 2.5000e-04\n",
      "Epoch 127/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2087 - val_loss: 0.2205 - lr: 2.5000e-04\n",
      "Epoch 128/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2085 - val_loss: 0.2212 - lr: 2.5000e-04\n",
      "Epoch 129/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2083 - val_loss: 0.2198 - lr: 1.2500e-04\n",
      "Epoch 130/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2080 - val_loss: 0.2202 - lr: 1.2500e-04\n",
      "Epoch 131/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2081 - val_loss: 0.2196 - lr: 1.2500e-04\n",
      "Epoch 132/1000\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2081 - val_loss: 0.2201 - lr: 1.2500e-04\n",
      "Epoch 133/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2083 - val_loss: 0.2210 - lr: 1.2500e-04\n",
      "Epoch 134/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2081 - val_loss: 0.2196 - lr: 1.2500e-04\n",
      "Epoch 135/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2082 - val_loss: 0.2194 - lr: 1.2500e-04\n",
      "Epoch 136/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2080 - val_loss: 0.2194 - lr: 1.2500e-04\n",
      "Epoch 137/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2081 - val_loss: 0.2197 - lr: 1.2500e-04\n",
      "Epoch 138/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2081 - val_loss: 0.2197 - lr: 1.2500e-04\n",
      "Epoch 139/1000\n",
      "168/168 [==============================] - 3s 17ms/step - loss: 0.2083 - val_loss: 0.2198 - lr: 1.2500e-04\n",
      "Epoch 140/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2081 - val_loss: 0.2195 - lr: 1.2500e-04\n",
      "Epoch 141/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2079 - val_loss: 0.2201 - lr: 1.2500e-04\n",
      "Epoch 142/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2081 - val_loss: 0.2196 - lr: 1.2500e-04\n",
      "Epoch 143/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2078 - val_loss: 0.2198 - lr: 1.2500e-04\n",
      "Epoch 144/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2081 - val_loss: 0.2204 - lr: 1.2500e-04\n",
      "Epoch 145/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2082 - val_loss: 0.2202 - lr: 1.2500e-04\n",
      "Epoch 146/1000\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2082 - val_loss: 0.2195 - lr: 1.2500e-04\n",
      "Epoch 147/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2080 - val_loss: 0.2209 - lr: 1.2500e-04\n",
      "Epoch 148/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2080 - val_loss: 0.2200 - lr: 1.2500e-04\n",
      "Epoch 149/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2082 - val_loss: 0.2202 - lr: 1.2500e-04\n",
      "Epoch 150/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2080 - val_loss: 0.2193 - lr: 1.2500e-04\n",
      "Epoch 151/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2076 - val_loss: 0.2194 - lr: 6.2500e-05\n",
      "Epoch 152/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2076 - val_loss: 0.2196 - lr: 6.2500e-05\n",
      "Epoch 153/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2076 - val_loss: 0.2196 - lr: 6.2500e-05\n",
      "Epoch 154/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2075 - val_loss: 0.2194 - lr: 6.2500e-05\n",
      "Epoch 155/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2077 - val_loss: 0.2216 - lr: 6.2500e-05\n",
      "Epoch 156/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2075 - val_loss: 0.2196 - lr: 6.2500e-05\n",
      "Epoch 157/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2077 - val_loss: 0.2199 - lr: 6.2500e-05\n",
      "Epoch 158/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2075 - val_loss: 0.2224 - lr: 6.2500e-05\n",
      "Epoch 159/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2078 - val_loss: 0.2199 - lr: 6.2500e-05\n",
      "Epoch 160/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2076 - val_loss: 0.2195 - lr: 6.2500e-05\n",
      "Epoch 161/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2075 - val_loss: 0.2202 - lr: 6.2500e-05\n",
      "Epoch 162/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2077 - val_loss: 0.2199 - lr: 6.2500e-05\n",
      "Epoch 163/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2075 - val_loss: 0.2201 - lr: 6.2500e-05\n",
      "Epoch 164/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2077 - val_loss: 0.2197 - lr: 6.2500e-05\n",
      "Epoch 165/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2075 - val_loss: 0.2197 - lr: 6.2500e-05\n",
      "Epoch 166/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2073 - val_loss: 0.2198 - lr: 3.1250e-05\n",
      "Epoch 167/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2073 - val_loss: 0.2205 - lr: 3.1250e-05\n",
      "Epoch 168/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2073 - val_loss: 0.2195 - lr: 3.1250e-05\n",
      "Epoch 169/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2073 - val_loss: 0.2193 - lr: 3.1250e-05\n",
      "Epoch 170/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2074 - val_loss: 0.2194 - lr: 3.1250e-05\n",
      "Epoch 171/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2073 - val_loss: 0.2197 - lr: 3.1250e-05\n",
      "Epoch 172/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2073 - val_loss: 0.2201 - lr: 3.1250e-05\n",
      "Epoch 173/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2073 - val_loss: 0.2195 - lr: 3.1250e-05\n",
      "Epoch 174/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2073 - val_loss: 0.2195 - lr: 3.1250e-05\n",
      "Epoch 175/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2074 - val_loss: 0.2193 - lr: 3.1250e-05\n",
      "Epoch 176/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2073 - val_loss: 0.2196 - lr: 3.1250e-05\n",
      "Epoch 177/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2073 - val_loss: 0.2196 - lr: 3.1250e-05\n",
      "Epoch 178/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2074 - val_loss: 0.2198 - lr: 3.1250e-05\n",
      "Epoch 179/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2073 - val_loss: 0.2193 - lr: 3.1250e-05\n",
      "Epoch 180/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2073 - val_loss: 0.2195 - lr: 3.1250e-05\n",
      "Epoch 181/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2073 - val_loss: 0.2198 - lr: 3.1250e-05\n",
      "Epoch 182/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2072 - val_loss: 0.2194 - lr: 3.1250e-05\n",
      "Epoch 183/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2073 - val_loss: 0.2195 - lr: 3.1250e-05\n",
      "Epoch 184/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2073 - val_loss: 0.2193 - lr: 3.1250e-05\n",
      "Epoch 185/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2072 - val_loss: 0.2196 - lr: 1.5625e-05\n",
      "Epoch 186/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2072 - val_loss: 0.2195 - lr: 1.5625e-05\n",
      "Epoch 187/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2072 - val_loss: 0.2196 - lr: 1.5625e-05\n",
      "Epoch 188/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2072 - val_loss: 0.2195 - lr: 1.5625e-05\n",
      "Epoch 189/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2072 - val_loss: 0.2192 - lr: 1.5625e-05\n",
      "Epoch 190/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2072 - val_loss: 0.2193 - lr: 1.5625e-05\n",
      "Epoch 191/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2072 - val_loss: 0.2193 - lr: 1.5625e-05\n",
      "Epoch 192/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2072 - val_loss: 0.2198 - lr: 1.5625e-05\n",
      "Epoch 193/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2072 - val_loss: 0.2193 - lr: 1.5625e-05\n",
      "Epoch 194/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2072 - val_loss: 0.2195 - lr: 1.5625e-05\n",
      "Epoch 195/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2072 - val_loss: 0.2195 - lr: 1.5625e-05\n",
      "Epoch 196/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2072 - val_loss: 0.2193 - lr: 1.5625e-05\n",
      "Epoch 197/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2072 - val_loss: 0.2198 - lr: 1.5625e-05\n",
      "Epoch 198/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2071 - val_loss: 0.2195 - lr: 1.5625e-05\n",
      "Epoch 199/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2072 - val_loss: 0.2194 - lr: 1.5625e-05\n",
      "Epoch 200/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2071 - val_loss: 0.2193 - lr: 7.8125e-06\n",
      "Epoch 201/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2071 - val_loss: 0.2194 - lr: 7.8125e-06\n",
      "Epoch 202/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2071 - val_loss: 0.2194 - lr: 7.8125e-06\n",
      "Epoch 203/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2071 - val_loss: 0.2194 - lr: 7.8125e-06\n",
      "Epoch 204/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2071 - val_loss: 0.2192 - lr: 7.8125e-06\n",
      "Epoch 205/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2071 - val_loss: 0.2193 - lr: 7.8125e-06\n",
      "Epoch 206/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2071 - val_loss: 0.2194 - lr: 7.8125e-06\n",
      "Epoch 207/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2071 - val_loss: 0.2193 - lr: 7.8125e-06\n",
      "Epoch 208/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2071 - val_loss: 0.2194 - lr: 7.8125e-06\n",
      "Epoch 209/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2071 - val_loss: 0.2194 - lr: 7.8125e-06\n",
      "Epoch 210/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2071 - val_loss: 0.2193 - lr: 7.8125e-06\n",
      "Epoch 211/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2071 - val_loss: 0.2197 - lr: 7.8125e-06\n",
      "Epoch 212/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2071 - val_loss: 0.2192 - lr: 7.8125e-06\n",
      "Epoch 213/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2071 - val_loss: 0.2196 - lr: 7.8125e-06\n",
      "Epoch 214/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2071 - val_loss: 0.2194 - lr: 7.8125e-06\n",
      "Epoch 215/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2070 - val_loss: 0.2193 - lr: 3.9063e-06\n",
      "Epoch 216/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2071 - val_loss: 0.2193 - lr: 3.9063e-06\n",
      "Epoch 217/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2070 - val_loss: 0.2194 - lr: 3.9063e-06\n",
      "Epoch 218/1000\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2071 - val_loss: 0.2193 - lr: 3.9063e-06\n",
      "Epoch 219/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2071 - val_loss: 0.2194 - lr: 3.9063e-06\n",
      "Epoch 220/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2071 - val_loss: 0.2195 - lr: 3.9063e-06\n",
      "Epoch 221/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2070 - val_loss: 0.2194 - lr: 3.9063e-06\n",
      "Epoch 222/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2071 - val_loss: 0.2193 - lr: 3.9063e-06\n",
      "Epoch 223/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2071 - val_loss: 0.2193 - lr: 3.9063e-06\n",
      "Epoch 224/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2070 - val_loss: 0.2194 - lr: 3.9063e-06\n",
      "Epoch 225/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2070 - val_loss: 0.2194 - lr: 3.9063e-06\n",
      "Epoch 226/1000\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.2070 - val_loss: 0.2192 - lr: 3.9063e-06\n",
      "Epoch 227/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2070 - val_loss: 0.2193 - lr: 3.9063e-06\n",
      "Epoch 228/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2070 - val_loss: 0.2194 - lr: 3.9063e-06\n",
      "Epoch 229/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2071 - val_loss: 0.2192 - lr: 3.9063e-06\n",
      "Epoch 230/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2070 - val_loss: 0.2194 - lr: 1.9531e-06\n",
      "Epoch 231/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2070 - val_loss: 0.2193 - lr: 1.9531e-06\n",
      "Epoch 232/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2070 - val_loss: 0.2193 - lr: 1.9531e-06\n",
      "Epoch 233/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2070 - val_loss: 0.2193 - lr: 1.9531e-06\n",
      "Epoch 234/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2070 - val_loss: 0.2193 - lr: 1.9531e-06\n",
      "Epoch 235/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2070 - val_loss: 0.2193 - lr: 1.9531e-06\n",
      "Epoch 236/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2070 - val_loss: 0.2193 - lr: 1.9531e-06\n",
      "Epoch 237/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2070 - val_loss: 0.2194 - lr: 1.9531e-06\n",
      "Epoch 238/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2070 - val_loss: 0.2193 - lr: 1.9531e-06\n",
      "Epoch 239/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2070 - val_loss: 0.2193 - lr: 1.9531e-06\n",
      "Epoch 240/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2070 - val_loss: 0.2193 - lr: 1.9531e-06\n",
      "Epoch 241/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2070 - val_loss: 0.2193 - lr: 1.9531e-06\n",
      "Epoch 242/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2070 - val_loss: 0.2193 - lr: 1.9531e-06\n",
      "Epoch 243/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2070 - val_loss: 0.2193 - lr: 1.9531e-06\n",
      "Epoch 244/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2070 - val_loss: 0.2193 - lr: 1.9531e-06\n",
      "Epoch 245/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2070 - val_loss: 0.2193 - lr: 9.7656e-07\n",
      "Epoch 246/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2070 - val_loss: 0.2193 - lr: 9.7656e-07\n",
      "Epoch 247/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2070 - val_loss: 0.2193 - lr: 9.7656e-07\n",
      "Epoch 248/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2070 - val_loss: 0.2193 - lr: 9.7656e-07\n",
      "Epoch 249/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2070 - val_loss: 0.2193 - lr: 9.7656e-07\n",
      "Epoch 250/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2070 - val_loss: 0.2193 - lr: 9.7656e-07\n",
      "Epoch 251/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2070 - val_loss: 0.2193 - lr: 9.7656e-07\n",
      "Epoch 252/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2070 - val_loss: 0.2193 - lr: 9.7656e-07\n",
      "Epoch 253/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2070 - val_loss: 0.2193 - lr: 9.7656e-07\n",
      "Epoch 254/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2070 - val_loss: 0.2193 - lr: 9.7656e-07\n",
      "Epoch 255/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2070 - val_loss: 0.2193 - lr: 9.7656e-07\n",
      "Epoch 256/1000\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.2070 - val_loss: 0.2193 - lr: 9.7656e-07\n",
      "Epoch 257/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2070 - val_loss: 0.2193 - lr: 9.7656e-07\n",
      "Epoch 258/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2070 - val_loss: 0.2193 - lr: 9.7656e-07\n",
      "Epoch 259/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2070 - val_loss: 0.2193 - lr: 9.7656e-07\n",
      "Epoch 260/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2070 - val_loss: 0.2193 - lr: 4.8828e-07\n",
      "Epoch 261/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2070 - val_loss: 0.2193 - lr: 4.8828e-07\n",
      "Epoch 262/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2070 - val_loss: 0.2193 - lr: 4.8828e-07\n",
      "2681/2681 [==============================] - 5s 2ms/step\n",
      "Fold 3 NN: 0.2192\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "Training fold 4\n",
      "(343145, 18)\n",
      "(85787, 18)\n",
      "Epoch 1/1000\n",
      "168/168 [==============================] - 5s 15ms/step - loss: 0.3697 - val_loss: 0.2521 - lr: 0.0010\n",
      "Epoch 2/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2629 - val_loss: 0.3678 - lr: 0.0010\n",
      "Epoch 3/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2746 - val_loss: 0.2362 - lr: 0.0010\n",
      "Epoch 4/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2418 - val_loss: 0.2328 - lr: 0.0010\n",
      "Epoch 5/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2403 - val_loss: 0.2262 - lr: 0.0010\n",
      "Epoch 6/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2443 - val_loss: 0.2302 - lr: 0.0010\n",
      "Epoch 7/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2424 - val_loss: 0.2256 - lr: 0.0010\n",
      "Epoch 8/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2498 - val_loss: 0.2407 - lr: 0.0010\n",
      "Epoch 9/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2354 - val_loss: 0.2245 - lr: 0.0010\n",
      "Epoch 10/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2329 - val_loss: 0.2233 - lr: 0.0010\n",
      "Epoch 11/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2371 - val_loss: 0.2217 - lr: 0.0010\n",
      "Epoch 12/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2432 - val_loss: 0.2215 - lr: 0.0010\n",
      "Epoch 13/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2438 - val_loss: 0.2263 - lr: 0.0010\n",
      "Epoch 14/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2319 - val_loss: 0.2207 - lr: 0.0010\n",
      "Epoch 15/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2299 - val_loss: 0.2236 - lr: 0.0010\n",
      "Epoch 16/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2292 - val_loss: 0.2490 - lr: 0.0010\n",
      "Epoch 17/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2358 - val_loss: 0.2610 - lr: 0.0010\n",
      "Epoch 18/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2332 - val_loss: 0.2212 - lr: 0.0010\n",
      "Epoch 19/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2292 - val_loss: 0.2196 - lr: 0.0010\n",
      "Epoch 20/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2418 - val_loss: 0.2608 - lr: 0.0010\n",
      "Epoch 21/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2268 - val_loss: 0.2223 - lr: 0.0010\n",
      "Epoch 22/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2274 - val_loss: 0.2198 - lr: 0.0010\n",
      "Epoch 23/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2299 - val_loss: 0.2231 - lr: 0.0010\n",
      "Epoch 24/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2289 - val_loss: 0.2189 - lr: 0.0010\n",
      "Epoch 25/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2367 - val_loss: 0.2227 - lr: 0.0010\n",
      "Epoch 26/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2257 - val_loss: 0.2217 - lr: 0.0010\n",
      "Epoch 27/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2268 - val_loss: 0.2284 - lr: 0.0010\n",
      "Epoch 28/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2269 - val_loss: 0.2211 - lr: 0.0010\n",
      "Epoch 29/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2348 - val_loss: 0.2212 - lr: 0.0010\n",
      "Epoch 30/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2305 - val_loss: 0.2195 - lr: 0.0010\n",
      "Epoch 31/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2267 - val_loss: 0.2183 - lr: 0.0010\n",
      "Epoch 32/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2266 - val_loss: 0.2191 - lr: 0.0010\n",
      "Epoch 33/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2240 - val_loss: 0.2183 - lr: 0.0010\n",
      "Epoch 34/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2312 - val_loss: 0.2168 - lr: 0.0010\n",
      "Epoch 35/1000\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.2269 - val_loss: 0.2236 - lr: 0.0010\n",
      "Epoch 36/1000\n",
      "168/168 [==============================] - 3s 18ms/step - loss: 0.2228 - val_loss: 0.2180 - lr: 0.0010\n",
      "Epoch 37/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2236 - val_loss: 0.2343 - lr: 0.0010\n",
      "Epoch 38/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2235 - val_loss: 0.2518 - lr: 0.0010\n",
      "Epoch 39/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2219 - val_loss: 0.2378 - lr: 0.0010\n",
      "Epoch 40/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2234 - val_loss: 0.2245 - lr: 0.0010\n",
      "Epoch 41/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2236 - val_loss: 0.2222 - lr: 0.0010\n",
      "Epoch 42/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2225 - val_loss: 0.2165 - lr: 0.0010\n",
      "Epoch 43/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2231 - val_loss: 0.2156 - lr: 0.0010\n",
      "Epoch 44/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2224 - val_loss: 0.2188 - lr: 0.0010\n",
      "Epoch 45/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2253 - val_loss: 0.2166 - lr: 0.0010\n",
      "Epoch 46/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2255 - val_loss: 0.2153 - lr: 0.0010\n",
      "Epoch 47/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2213 - val_loss: 0.2184 - lr: 0.0010\n",
      "Epoch 48/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2221 - val_loss: 0.2150 - lr: 0.0010\n",
      "Epoch 49/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2248 - val_loss: 0.2161 - lr: 0.0010\n",
      "Epoch 50/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2184 - val_loss: 0.2151 - lr: 0.0010\n",
      "Epoch 51/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2208 - val_loss: 0.2145 - lr: 0.0010\n",
      "Epoch 52/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2203 - val_loss: 0.2150 - lr: 0.0010\n",
      "Epoch 53/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2195 - val_loss: 0.2127 - lr: 0.0010\n",
      "Epoch 54/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2204 - val_loss: 0.2152 - lr: 0.0010\n",
      "Epoch 55/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2201 - val_loss: 0.2131 - lr: 0.0010\n",
      "Epoch 56/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2201 - val_loss: 0.2123 - lr: 0.0010\n",
      "Epoch 57/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2207 - val_loss: 0.2133 - lr: 0.0010\n",
      "Epoch 58/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2248 - val_loss: 0.2169 - lr: 0.0010\n",
      "Epoch 59/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2186 - val_loss: 0.2134 - lr: 0.0010\n",
      "Epoch 60/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2166 - val_loss: 0.2193 - lr: 0.0010\n",
      "Epoch 61/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2224 - val_loss: 0.2145 - lr: 0.0010\n",
      "Epoch 62/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2188 - val_loss: 0.2133 - lr: 0.0010\n",
      "Epoch 63/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2201 - val_loss: 0.2191 - lr: 0.0010\n",
      "Epoch 64/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2196 - val_loss: 0.2228 - lr: 0.0010\n",
      "Epoch 65/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2174 - val_loss: 0.2125 - lr: 0.0010\n",
      "Epoch 66/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2173 - val_loss: 0.2243 - lr: 0.0010\n",
      "Epoch 67/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2182 - val_loss: 0.2138 - lr: 0.0010\n",
      "Epoch 68/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2160 - val_loss: 0.2161 - lr: 0.0010\n",
      "Epoch 69/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2173 - val_loss: 0.2145 - lr: 0.0010\n",
      "Epoch 70/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2185 - val_loss: 0.2172 - lr: 0.0010\n",
      "Epoch 71/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2169 - val_loss: 0.2167 - lr: 0.0010\n",
      "Epoch 72/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2141 - val_loss: 0.2116 - lr: 5.0000e-04\n",
      "Epoch 73/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2131 - val_loss: 0.2116 - lr: 5.0000e-04\n",
      "Epoch 74/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2140 - val_loss: 0.2135 - lr: 5.0000e-04\n",
      "Epoch 75/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2142 - val_loss: 0.2113 - lr: 5.0000e-04\n",
      "Epoch 76/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2140 - val_loss: 0.2120 - lr: 5.0000e-04\n",
      "Epoch 77/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2140 - val_loss: 0.2138 - lr: 5.0000e-04\n",
      "Epoch 78/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2139 - val_loss: 0.2118 - lr: 5.0000e-04\n",
      "Epoch 79/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2141 - val_loss: 0.2167 - lr: 5.0000e-04\n",
      "Epoch 80/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2147 - val_loss: 0.2115 - lr: 5.0000e-04\n",
      "Epoch 81/1000\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.2132 - val_loss: 0.2169 - lr: 5.0000e-04\n",
      "Epoch 82/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2140 - val_loss: 0.2135 - lr: 5.0000e-04\n",
      "Epoch 83/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2135 - val_loss: 0.2164 - lr: 5.0000e-04\n",
      "Epoch 84/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2137 - val_loss: 0.2127 - lr: 5.0000e-04\n",
      "Epoch 85/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2131 - val_loss: 0.2118 - lr: 5.0000e-04\n",
      "Epoch 86/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2135 - val_loss: 0.2139 - lr: 5.0000e-04\n",
      "Epoch 87/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2141 - val_loss: 0.2107 - lr: 5.0000e-04\n",
      "Epoch 88/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2136 - val_loss: 0.2135 - lr: 5.0000e-04\n",
      "Epoch 89/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2146 - val_loss: 0.2113 - lr: 5.0000e-04\n",
      "Epoch 90/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2128 - val_loss: 0.2197 - lr: 5.0000e-04\n",
      "Epoch 91/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2147 - val_loss: 0.2123 - lr: 5.0000e-04\n",
      "Epoch 92/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2142 - val_loss: 0.2116 - lr: 5.0000e-04\n",
      "Epoch 93/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2132 - val_loss: 0.2109 - lr: 5.0000e-04\n",
      "Epoch 94/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2130 - val_loss: 0.2214 - lr: 5.0000e-04\n",
      "Epoch 95/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2149 - val_loss: 0.2112 - lr: 5.0000e-04\n",
      "Epoch 96/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2130 - val_loss: 0.2153 - lr: 5.0000e-04\n",
      "Epoch 97/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2133 - val_loss: 0.2114 - lr: 5.0000e-04\n",
      "Epoch 98/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2130 - val_loss: 0.2155 - lr: 5.0000e-04\n",
      "Epoch 99/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2128 - val_loss: 0.2106 - lr: 5.0000e-04\n",
      "Epoch 100/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2127 - val_loss: 0.2132 - lr: 5.0000e-04\n",
      "Epoch 101/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2127 - val_loss: 0.2121 - lr: 5.0000e-04\n",
      "Epoch 102/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2131 - val_loss: 0.2135 - lr: 5.0000e-04\n",
      "Epoch 103/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2133 - val_loss: 0.2115 - lr: 5.0000e-04\n",
      "Epoch 104/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2127 - val_loss: 0.2117 - lr: 5.0000e-04\n",
      "Epoch 105/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2129 - val_loss: 0.2132 - lr: 5.0000e-04\n",
      "Epoch 106/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2133 - val_loss: 0.2130 - lr: 5.0000e-04\n",
      "Epoch 107/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2124 - val_loss: 0.2163 - lr: 5.0000e-04\n",
      "Epoch 108/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2127 - val_loss: 0.2126 - lr: 5.0000e-04\n",
      "Epoch 109/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2134 - val_loss: 0.2110 - lr: 5.0000e-04\n",
      "Epoch 110/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2126 - val_loss: 0.2139 - lr: 5.0000e-04\n",
      "Epoch 111/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2121 - val_loss: 0.2119 - lr: 5.0000e-04\n",
      "Epoch 112/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2120 - val_loss: 0.2102 - lr: 5.0000e-04\n",
      "Epoch 113/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2123 - val_loss: 0.2135 - lr: 5.0000e-04\n",
      "Epoch 114/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2120 - val_loss: 0.2151 - lr: 5.0000e-04\n",
      "Epoch 115/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2127 - val_loss: 0.2104 - lr: 5.0000e-04\n",
      "Epoch 116/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2117 - val_loss: 0.2106 - lr: 5.0000e-04\n",
      "Epoch 117/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2114 - val_loss: 0.2126 - lr: 5.0000e-04\n",
      "Epoch 118/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2121 - val_loss: 0.2110 - lr: 5.0000e-04\n",
      "Epoch 119/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2119 - val_loss: 0.2128 - lr: 5.0000e-04\n",
      "Epoch 120/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2129 - val_loss: 0.2158 - lr: 5.0000e-04\n",
      "Epoch 121/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2122 - val_loss: 0.2124 - lr: 5.0000e-04\n",
      "Epoch 122/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2117 - val_loss: 0.2150 - lr: 5.0000e-04\n",
      "Epoch 123/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2126 - val_loss: 0.2114 - lr: 5.0000e-04\n",
      "Epoch 124/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2117 - val_loss: 0.2113 - lr: 5.0000e-04\n",
      "Epoch 125/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2113 - val_loss: 0.2111 - lr: 5.0000e-04\n",
      "Epoch 126/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2119 - val_loss: 0.2146 - lr: 5.0000e-04\n",
      "Epoch 127/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2115 - val_loss: 0.2103 - lr: 5.0000e-04\n",
      "Epoch 128/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2103 - val_loss: 0.2105 - lr: 2.5000e-04\n",
      "Epoch 129/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2102 - val_loss: 0.2105 - lr: 2.5000e-04\n",
      "Epoch 130/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2104 - val_loss: 0.2102 - lr: 2.5000e-04\n",
      "Epoch 131/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2102 - val_loss: 0.2106 - lr: 2.5000e-04\n",
      "Epoch 132/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2102 - val_loss: 0.2113 - lr: 2.5000e-04\n",
      "Epoch 133/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2103 - val_loss: 0.2100 - lr: 2.5000e-04\n",
      "Epoch 134/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2103 - val_loss: 0.2135 - lr: 2.5000e-04\n",
      "Epoch 135/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2104 - val_loss: 0.2107 - lr: 2.5000e-04\n",
      "Epoch 136/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2101 - val_loss: 0.2110 - lr: 2.5000e-04\n",
      "Epoch 137/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2106 - val_loss: 0.2110 - lr: 2.5000e-04\n",
      "Epoch 138/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2101 - val_loss: 0.2111 - lr: 2.5000e-04\n",
      "Epoch 139/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2106 - val_loss: 0.2113 - lr: 2.5000e-04\n",
      "Epoch 140/1000\n",
      "168/168 [==============================] - 3s 17ms/step - loss: 0.2105 - val_loss: 0.2132 - lr: 2.5000e-04\n",
      "Epoch 141/1000\n",
      "168/168 [==============================] - 3s 18ms/step - loss: 0.2099 - val_loss: 0.2101 - lr: 2.5000e-04\n",
      "Epoch 142/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2100 - val_loss: 0.2102 - lr: 2.5000e-04\n",
      "Epoch 143/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2102 - val_loss: 0.2183 - lr: 2.5000e-04\n",
      "Epoch 144/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2108 - val_loss: 0.2115 - lr: 2.5000e-04\n",
      "Epoch 145/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2099 - val_loss: 0.2105 - lr: 2.5000e-04\n",
      "Epoch 146/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2102 - val_loss: 0.2109 - lr: 2.5000e-04\n",
      "Epoch 147/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2110 - val_loss: 0.2111 - lr: 2.5000e-04\n",
      "Epoch 148/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2104 - val_loss: 0.2147 - lr: 2.5000e-04\n",
      "Epoch 149/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2095 - val_loss: 0.2098 - lr: 1.2500e-04\n",
      "Epoch 150/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2096 - val_loss: 0.2109 - lr: 1.2500e-04\n",
      "Epoch 151/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2093 - val_loss: 0.2099 - lr: 1.2500e-04\n",
      "Epoch 152/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2093 - val_loss: 0.2101 - lr: 1.2500e-04\n",
      "Epoch 153/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2093 - val_loss: 0.2100 - lr: 1.2500e-04\n",
      "Epoch 154/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2094 - val_loss: 0.2106 - lr: 1.2500e-04\n",
      "Epoch 155/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2096 - val_loss: 0.2106 - lr: 1.2500e-04\n",
      "Epoch 156/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2093 - val_loss: 0.2099 - lr: 1.2500e-04\n",
      "Epoch 157/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2094 - val_loss: 0.2107 - lr: 1.2500e-04\n",
      "Epoch 158/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2095 - val_loss: 0.2110 - lr: 1.2500e-04\n",
      "Epoch 159/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2093 - val_loss: 0.2107 - lr: 1.2500e-04\n",
      "Epoch 160/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2096 - val_loss: 0.2116 - lr: 1.2500e-04\n",
      "Epoch 161/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2093 - val_loss: 0.2102 - lr: 1.2500e-04\n",
      "Epoch 162/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2092 - val_loss: 0.2101 - lr: 1.2500e-04\n",
      "Epoch 163/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2093 - val_loss: 0.2104 - lr: 1.2500e-04\n",
      "Epoch 164/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2092 - val_loss: 0.2102 - lr: 1.2500e-04\n",
      "Epoch 165/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2089 - val_loss: 0.2106 - lr: 6.2500e-05\n",
      "Epoch 166/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2091 - val_loss: 0.2109 - lr: 6.2500e-05\n",
      "Epoch 167/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2089 - val_loss: 0.2120 - lr: 6.2500e-05\n",
      "Epoch 168/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2090 - val_loss: 0.2109 - lr: 6.2500e-05\n",
      "Epoch 169/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2089 - val_loss: 0.2097 - lr: 6.2500e-05\n",
      "Epoch 170/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2089 - val_loss: 0.2099 - lr: 6.2500e-05\n",
      "Epoch 171/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2090 - val_loss: 0.2112 - lr: 6.2500e-05\n",
      "Epoch 172/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2089 - val_loss: 0.2104 - lr: 6.2500e-05\n",
      "Epoch 173/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2090 - val_loss: 0.2101 - lr: 6.2500e-05\n",
      "Epoch 174/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2089 - val_loss: 0.2102 - lr: 6.2500e-05\n",
      "Epoch 175/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2088 - val_loss: 0.2101 - lr: 6.2500e-05\n",
      "Epoch 176/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2088 - val_loss: 0.2107 - lr: 6.2500e-05\n",
      "Epoch 177/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2089 - val_loss: 0.2102 - lr: 6.2500e-05\n",
      "Epoch 178/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2088 - val_loss: 0.2105 - lr: 6.2500e-05\n",
      "Epoch 179/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2088 - val_loss: 0.2100 - lr: 6.2500e-05\n",
      "Epoch 180/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2087 - val_loss: 0.2099 - lr: 3.1250e-05\n",
      "Epoch 181/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2086 - val_loss: 0.2102 - lr: 3.1250e-05\n",
      "Epoch 182/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2087 - val_loss: 0.2103 - lr: 3.1250e-05\n",
      "Epoch 183/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2086 - val_loss: 0.2102 - lr: 3.1250e-05\n",
      "Epoch 184/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2087 - val_loss: 0.2103 - lr: 3.1250e-05\n",
      "Epoch 185/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2086 - val_loss: 0.2099 - lr: 3.1250e-05\n",
      "Epoch 186/1000\n",
      "168/168 [==============================] - 3s 16ms/step - loss: 0.2086 - val_loss: 0.2106 - lr: 3.1250e-05\n",
      "Epoch 187/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2087 - val_loss: 0.2101 - lr: 3.1250e-05\n",
      "Epoch 188/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2088 - val_loss: 0.2100 - lr: 3.1250e-05\n",
      "Epoch 189/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2086 - val_loss: 0.2099 - lr: 3.1250e-05\n",
      "Epoch 190/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2086 - val_loss: 0.2101 - lr: 3.1250e-05\n",
      "Epoch 191/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2086 - val_loss: 0.2099 - lr: 3.1250e-05\n",
      "Epoch 192/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2086 - val_loss: 0.2117 - lr: 3.1250e-05\n",
      "Epoch 193/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2086 - val_loss: 0.2101 - lr: 3.1250e-05\n",
      "Epoch 194/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2086 - val_loss: 0.2099 - lr: 3.1250e-05\n",
      "Epoch 195/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2085 - val_loss: 0.2101 - lr: 1.5625e-05\n",
      "Epoch 196/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2085 - val_loss: 0.2100 - lr: 1.5625e-05\n",
      "Epoch 197/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2085 - val_loss: 0.2101 - lr: 1.5625e-05\n",
      "Epoch 198/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2085 - val_loss: 0.2099 - lr: 1.5625e-05\n",
      "Epoch 199/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2085 - val_loss: 0.2101 - lr: 1.5625e-05\n",
      "Epoch 200/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2085 - val_loss: 0.2104 - lr: 1.5625e-05\n",
      "Epoch 201/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2085 - val_loss: 0.2103 - lr: 1.5625e-05\n",
      "Epoch 202/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2085 - val_loss: 0.2102 - lr: 1.5625e-05\n",
      "Epoch 203/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2085 - val_loss: 0.2102 - lr: 1.5625e-05\n",
      "Epoch 204/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2085 - val_loss: 0.2101 - lr: 1.5625e-05\n",
      "Epoch 205/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2085 - val_loss: 0.2103 - lr: 1.5625e-05\n",
      "Epoch 206/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2085 - val_loss: 0.2099 - lr: 1.5625e-05\n",
      "Epoch 207/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2085 - val_loss: 0.2106 - lr: 1.5625e-05\n",
      "Epoch 208/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2085 - val_loss: 0.2100 - lr: 1.5625e-05\n",
      "Epoch 209/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2085 - val_loss: 0.2101 - lr: 1.5625e-05\n",
      "Epoch 210/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2084 - val_loss: 0.2101 - lr: 7.8125e-06\n",
      "Epoch 211/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2084 - val_loss: 0.2106 - lr: 7.8125e-06\n",
      "Epoch 212/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2084 - val_loss: 0.2100 - lr: 7.8125e-06\n",
      "Epoch 213/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2084 - val_loss: 0.2106 - lr: 7.8125e-06\n",
      "Epoch 214/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2084 - val_loss: 0.2100 - lr: 7.8125e-06\n",
      "Epoch 215/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2084 - val_loss: 0.2102 - lr: 7.8125e-06\n",
      "Epoch 216/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2084 - val_loss: 0.2101 - lr: 7.8125e-06\n",
      "Epoch 217/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2084 - val_loss: 0.2102 - lr: 7.8125e-06\n",
      "Epoch 218/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2084 - val_loss: 0.2103 - lr: 7.8125e-06\n",
      "Epoch 219/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2084 - val_loss: 0.2102 - lr: 7.8125e-06\n",
      "2681/2681 [==============================] - 5s 2ms/step\n",
      "Fold 4 NN: 0.20973\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "Training fold 5\n",
      "(343145, 18)\n",
      "(85787, 18)\n",
      "Epoch 1/1000\n",
      "168/168 [==============================] - 6s 15ms/step - loss: 0.3837 - val_loss: 0.2677 - lr: 0.0010\n",
      "Epoch 2/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2712 - val_loss: 0.2536 - lr: 0.0010\n",
      "Epoch 3/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2544 - val_loss: 0.2348 - lr: 0.0010\n",
      "Epoch 4/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2516 - val_loss: 0.2427 - lr: 0.0010\n",
      "Epoch 5/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2408 - val_loss: 0.2324 - lr: 0.0010\n",
      "Epoch 6/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2420 - val_loss: 0.2317 - lr: 0.0010\n",
      "Epoch 7/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2381 - val_loss: 0.2400 - lr: 0.0010\n",
      "Epoch 8/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2375 - val_loss: 0.2347 - lr: 0.0010\n",
      "Epoch 9/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2380 - val_loss: 0.2323 - lr: 0.0010\n",
      "Epoch 10/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2473 - val_loss: 0.2240 - lr: 0.0010\n",
      "Epoch 11/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2336 - val_loss: 0.2305 - lr: 0.0010\n",
      "Epoch 12/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2311 - val_loss: 0.2462 - lr: 0.0010\n",
      "Epoch 13/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2383 - val_loss: 0.2352 - lr: 0.0010\n",
      "Epoch 14/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2423 - val_loss: 0.2792 - lr: 0.0010\n",
      "Epoch 15/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2332 - val_loss: 0.2260 - lr: 0.0010\n",
      "Epoch 16/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2361 - val_loss: 0.2225 - lr: 0.0010\n",
      "Epoch 17/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2287 - val_loss: 0.2208 - lr: 0.0010\n",
      "Epoch 18/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2340 - val_loss: 0.2321 - lr: 0.0010\n",
      "Epoch 19/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2291 - val_loss: 0.2321 - lr: 0.0010\n",
      "Epoch 20/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2269 - val_loss: 0.2307 - lr: 0.0010\n",
      "Epoch 21/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2270 - val_loss: 0.2387 - lr: 0.0010\n",
      "Epoch 22/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2291 - val_loss: 0.2221 - lr: 0.0010\n",
      "Epoch 23/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2285 - val_loss: 0.2192 - lr: 0.0010\n",
      "Epoch 24/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2296 - val_loss: 0.2187 - lr: 0.0010\n",
      "Epoch 25/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2274 - val_loss: 0.2754 - lr: 0.0010\n",
      "Epoch 26/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2348 - val_loss: 0.2209 - lr: 0.0010\n",
      "Epoch 27/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2256 - val_loss: 0.2274 - lr: 0.0010\n",
      "Epoch 28/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2304 - val_loss: 0.2192 - lr: 0.0010\n",
      "Epoch 29/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2241 - val_loss: 0.2387 - lr: 0.0010\n",
      "Epoch 30/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2261 - val_loss: 0.2329 - lr: 0.0010\n",
      "Epoch 31/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2297 - val_loss: 0.2228 - lr: 0.0010\n",
      "Epoch 32/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2285 - val_loss: 0.2173 - lr: 0.0010\n",
      "Epoch 33/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2249 - val_loss: 0.2192 - lr: 0.0010\n",
      "Epoch 34/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2260 - val_loss: 0.2195 - lr: 0.0010\n",
      "Epoch 35/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2224 - val_loss: 0.2375 - lr: 0.0010\n",
      "Epoch 36/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2238 - val_loss: 0.2168 - lr: 0.0010\n",
      "Epoch 37/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2229 - val_loss: 0.2170 - lr: 0.0010\n",
      "Epoch 38/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2216 - val_loss: 0.2163 - lr: 0.0010\n",
      "Epoch 39/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2217 - val_loss: 0.2432 - lr: 0.0010\n",
      "Epoch 40/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2233 - val_loss: 0.2170 - lr: 0.0010\n",
      "Epoch 41/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2206 - val_loss: 0.2171 - lr: 0.0010\n",
      "Epoch 42/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2222 - val_loss: 0.2164 - lr: 0.0010\n",
      "Epoch 43/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2214 - val_loss: 0.2183 - lr: 0.0010\n",
      "Epoch 44/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2323 - val_loss: 0.2159 - lr: 0.0010\n",
      "Epoch 45/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2230 - val_loss: 0.2163 - lr: 0.0010\n",
      "Epoch 46/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2196 - val_loss: 0.2183 - lr: 0.0010\n",
      "Epoch 47/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2201 - val_loss: 0.2193 - lr: 0.0010\n",
      "Epoch 48/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2210 - val_loss: 0.2232 - lr: 0.0010\n",
      "Epoch 49/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2188 - val_loss: 0.2181 - lr: 0.0010\n",
      "Epoch 50/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2198 - val_loss: 0.2173 - lr: 0.0010\n",
      "Epoch 51/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2264 - val_loss: 0.2167 - lr: 0.0010\n",
      "Epoch 52/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2227 - val_loss: 0.2168 - lr: 0.0010\n",
      "Epoch 53/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2188 - val_loss: 0.2208 - lr: 0.0010\n",
      "Epoch 54/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2176 - val_loss: 0.2197 - lr: 0.0010\n",
      "Epoch 55/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2186 - val_loss: 0.2246 - lr: 0.0010\n",
      "Epoch 56/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2193 - val_loss: 0.2158 - lr: 0.0010\n",
      "Epoch 57/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2172 - val_loss: 0.2199 - lr: 0.0010\n",
      "Epoch 58/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2205 - val_loss: 0.2181 - lr: 0.0010\n",
      "Epoch 59/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2191 - val_loss: 0.2230 - lr: 0.0010\n",
      "Epoch 60/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2183 - val_loss: 0.2205 - lr: 0.0010\n",
      "Epoch 61/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2207 - val_loss: 0.2170 - lr: 0.0010\n",
      "Epoch 62/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2174 - val_loss: 0.2160 - lr: 0.0010\n",
      "Epoch 63/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2168 - val_loss: 0.2158 - lr: 0.0010\n",
      "Epoch 64/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2162 - val_loss: 0.2200 - lr: 0.0010\n",
      "Epoch 65/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2210 - val_loss: 0.2346 - lr: 0.0010\n",
      "Epoch 66/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2166 - val_loss: 0.2175 - lr: 0.0010\n",
      "Epoch 67/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2174 - val_loss: 0.2145 - lr: 0.0010\n",
      "Epoch 68/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2170 - val_loss: 0.2169 - lr: 0.0010\n",
      "Epoch 69/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2170 - val_loss: 0.2166 - lr: 0.0010\n",
      "Epoch 70/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2163 - val_loss: 0.2153 - lr: 0.0010\n",
      "Epoch 71/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2179 - val_loss: 0.2208 - lr: 0.0010\n",
      "Epoch 72/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2154 - val_loss: 0.2223 - lr: 0.0010\n",
      "Epoch 73/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2184 - val_loss: 0.2178 - lr: 0.0010\n",
      "Epoch 74/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2167 - val_loss: 0.2188 - lr: 0.0010\n",
      "Epoch 75/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2160 - val_loss: 0.2204 - lr: 0.0010\n",
      "Epoch 76/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2168 - val_loss: 0.2171 - lr: 0.0010\n",
      "Epoch 77/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2161 - val_loss: 0.2150 - lr: 0.0010\n",
      "Epoch 78/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2142 - val_loss: 0.2146 - lr: 0.0010\n",
      "Epoch 79/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2157 - val_loss: 0.2146 - lr: 0.0010\n",
      "Epoch 80/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2146 - val_loss: 0.2144 - lr: 0.0010\n",
      "Epoch 81/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2140 - val_loss: 0.2159 - lr: 0.0010\n",
      "Epoch 82/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2130 - val_loss: 0.2162 - lr: 0.0010\n",
      "Epoch 83/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2151 - val_loss: 0.2214 - lr: 0.0010\n",
      "Epoch 84/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2148 - val_loss: 0.2140 - lr: 0.0010\n",
      "Epoch 85/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2145 - val_loss: 0.2153 - lr: 0.0010\n",
      "Epoch 86/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2147 - val_loss: 0.2200 - lr: 0.0010\n",
      "Epoch 87/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2176 - val_loss: 0.2144 - lr: 0.0010\n",
      "Epoch 88/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2135 - val_loss: 0.2153 - lr: 0.0010\n",
      "Epoch 89/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2132 - val_loss: 0.2167 - lr: 0.0010\n",
      "Epoch 90/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2142 - val_loss: 0.2143 - lr: 0.0010\n",
      "Epoch 91/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2164 - val_loss: 0.2164 - lr: 0.0010\n",
      "Epoch 92/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2147 - val_loss: 0.2153 - lr: 0.0010\n",
      "Epoch 93/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2126 - val_loss: 0.2161 - lr: 0.0010\n",
      "Epoch 94/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2133 - val_loss: 0.2139 - lr: 0.0010\n",
      "Epoch 95/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2134 - val_loss: 0.2157 - lr: 0.0010\n",
      "Epoch 96/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2141 - val_loss: 0.2147 - lr: 0.0010\n",
      "Epoch 97/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2152 - val_loss: 0.2150 - lr: 0.0010\n",
      "Epoch 98/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2133 - val_loss: 0.2135 - lr: 0.0010\n",
      "Epoch 99/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2141 - val_loss: 0.2133 - lr: 0.0010\n",
      "Epoch 100/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2137 - val_loss: 0.2192 - lr: 0.0010\n",
      "Epoch 101/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2138 - val_loss: 0.2222 - lr: 0.0010\n",
      "Epoch 102/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2122 - val_loss: 0.2138 - lr: 0.0010\n",
      "Epoch 103/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2138 - val_loss: 0.2179 - lr: 0.0010\n",
      "Epoch 104/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2174 - val_loss: 0.2171 - lr: 0.0010\n",
      "Epoch 105/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2121 - val_loss: 0.2131 - lr: 0.0010\n",
      "Epoch 106/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2128 - val_loss: 0.2156 - lr: 0.0010\n",
      "Epoch 107/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2119 - val_loss: 0.2142 - lr: 0.0010\n",
      "Epoch 108/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2122 - val_loss: 0.2187 - lr: 0.0010\n",
      "Epoch 109/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2154 - val_loss: 0.2196 - lr: 0.0010\n",
      "Epoch 110/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2125 - val_loss: 0.2137 - lr: 0.0010\n",
      "Epoch 111/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2135 - val_loss: 0.2151 - lr: 0.0010\n",
      "Epoch 112/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2118 - val_loss: 0.2325 - lr: 0.0010\n",
      "Epoch 113/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2115 - val_loss: 0.2143 - lr: 0.0010\n",
      "Epoch 114/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2129 - val_loss: 0.2150 - lr: 0.0010\n",
      "Epoch 115/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2129 - val_loss: 0.2129 - lr: 0.0010\n",
      "Epoch 116/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2124 - val_loss: 0.2149 - lr: 0.0010\n",
      "Epoch 117/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2124 - val_loss: 0.2177 - lr: 0.0010\n",
      "Epoch 118/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2118 - val_loss: 0.2128 - lr: 0.0010\n",
      "Epoch 119/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2126 - val_loss: 0.2171 - lr: 0.0010\n",
      "Epoch 120/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2121 - val_loss: 0.2133 - lr: 0.0010\n",
      "Epoch 121/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2122 - val_loss: 0.2147 - lr: 0.0010\n",
      "Epoch 122/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2117 - val_loss: 0.2398 - lr: 0.0010\n",
      "Epoch 123/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2143 - val_loss: 0.2132 - lr: 0.0010\n",
      "Epoch 124/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2124 - val_loss: 0.2140 - lr: 0.0010\n",
      "Epoch 125/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2110 - val_loss: 0.2129 - lr: 0.0010\n",
      "Epoch 126/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2104 - val_loss: 0.2131 - lr: 0.0010\n",
      "Epoch 127/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2125 - val_loss: 0.2120 - lr: 0.0010\n",
      "Epoch 128/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2109 - val_loss: 0.2121 - lr: 0.0010\n",
      "Epoch 129/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2112 - val_loss: 0.2124 - lr: 0.0010\n",
      "Epoch 130/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2103 - val_loss: 0.2135 - lr: 0.0010\n",
      "Epoch 131/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2110 - val_loss: 0.2128 - lr: 0.0010\n",
      "Epoch 132/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2110 - val_loss: 0.2130 - lr: 0.0010\n",
      "Epoch 133/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2108 - val_loss: 0.2144 - lr: 0.0010\n",
      "Epoch 134/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2104 - val_loss: 0.2121 - lr: 0.0010\n",
      "Epoch 135/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2098 - val_loss: 0.2131 - lr: 0.0010\n",
      "Epoch 136/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2119 - val_loss: 0.2134 - lr: 0.0010\n",
      "Epoch 137/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2103 - val_loss: 0.2131 - lr: 0.0010\n",
      "Epoch 138/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2106 - val_loss: 0.2183 - lr: 0.0010\n",
      "Epoch 139/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2107 - val_loss: 0.2159 - lr: 0.0010\n",
      "Epoch 140/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2103 - val_loss: 0.2120 - lr: 0.0010\n",
      "Epoch 141/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2109 - val_loss: 0.2131 - lr: 0.0010\n",
      "Epoch 142/1000\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2098 - val_loss: 0.2217 - lr: 0.0010\n",
      "Epoch 143/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2095 - val_loss: 0.2122 - lr: 5.0000e-04\n",
      "Epoch 144/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2084 - val_loss: 0.2120 - lr: 5.0000e-04\n",
      "Epoch 145/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2080 - val_loss: 0.2124 - lr: 5.0000e-04\n",
      "Epoch 146/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2085 - val_loss: 0.2142 - lr: 5.0000e-04\n",
      "Epoch 147/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2083 - val_loss: 0.2122 - lr: 5.0000e-04\n",
      "Epoch 148/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2086 - val_loss: 0.2124 - lr: 5.0000e-04\n",
      "Epoch 149/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2079 - val_loss: 0.2130 - lr: 5.0000e-04\n",
      "Epoch 150/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2080 - val_loss: 0.2117 - lr: 5.0000e-04\n",
      "Epoch 151/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2085 - val_loss: 0.2120 - lr: 5.0000e-04\n",
      "Epoch 152/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2082 - val_loss: 0.2121 - lr: 5.0000e-04\n",
      "Epoch 153/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2080 - val_loss: 0.2136 - lr: 5.0000e-04\n",
      "Epoch 154/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2084 - val_loss: 0.2134 - lr: 5.0000e-04\n",
      "Epoch 155/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2080 - val_loss: 0.2123 - lr: 5.0000e-04\n",
      "Epoch 156/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2086 - val_loss: 0.2142 - lr: 5.0000e-04\n",
      "Epoch 157/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2086 - val_loss: 0.2127 - lr: 5.0000e-04\n",
      "Epoch 158/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2077 - val_loss: 0.2131 - lr: 5.0000e-04\n",
      "Epoch 159/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2076 - val_loss: 0.2123 - lr: 5.0000e-04\n",
      "Epoch 160/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2077 - val_loss: 0.2126 - lr: 5.0000e-04\n",
      "Epoch 161/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2084 - val_loss: 0.2121 - lr: 5.0000e-04\n",
      "Epoch 162/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2079 - val_loss: 0.2133 - lr: 5.0000e-04\n",
      "Epoch 163/1000\n",
      "168/168 [==============================] - 3s 17ms/step - loss: 0.2077 - val_loss: 0.2126 - lr: 5.0000e-04\n",
      "Epoch 164/1000\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.2080 - val_loss: 0.2124 - lr: 5.0000e-04\n",
      "Epoch 165/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2078 - val_loss: 0.2120 - lr: 5.0000e-04\n",
      "Epoch 166/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2068 - val_loss: 0.2121 - lr: 2.5000e-04\n",
      "Epoch 167/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2071 - val_loss: 0.2117 - lr: 2.5000e-04\n",
      "Epoch 168/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2067 - val_loss: 0.2126 - lr: 2.5000e-04\n",
      "Epoch 169/1000\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2069 - val_loss: 0.2121 - lr: 2.5000e-04\n",
      "Epoch 170/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2068 - val_loss: 0.2121 - lr: 2.5000e-04\n",
      "Epoch 171/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2066 - val_loss: 0.2125 - lr: 2.5000e-04\n",
      "Epoch 172/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2068 - val_loss: 0.2121 - lr: 2.5000e-04\n",
      "Epoch 173/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2067 - val_loss: 0.2119 - lr: 2.5000e-04\n",
      "Epoch 174/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2070 - val_loss: 0.2120 - lr: 2.5000e-04\n",
      "Epoch 175/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2068 - val_loss: 0.2123 - lr: 2.5000e-04\n",
      "Epoch 176/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2068 - val_loss: 0.2121 - lr: 2.5000e-04\n",
      "Epoch 177/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2066 - val_loss: 0.2121 - lr: 2.5000e-04\n",
      "Epoch 178/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2068 - val_loss: 0.2128 - lr: 2.5000e-04\n",
      "Epoch 179/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2066 - val_loss: 0.2127 - lr: 2.5000e-04\n",
      "Epoch 180/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2065 - val_loss: 0.2123 - lr: 2.5000e-04\n",
      "Epoch 181/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2062 - val_loss: 0.2119 - lr: 1.2500e-04\n",
      "Epoch 182/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2062 - val_loss: 0.2121 - lr: 1.2500e-04\n",
      "Epoch 183/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2061 - val_loss: 0.2120 - lr: 1.2500e-04\n",
      "Epoch 184/1000\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.2061 - val_loss: 0.2117 - lr: 1.2500e-04\n",
      "Epoch 185/1000\n",
      "168/168 [==============================] - 3s 16ms/step - loss: 0.2062 - val_loss: 0.2117 - lr: 1.2500e-04\n",
      "Epoch 186/1000\n",
      "168/168 [==============================] - 3s 16ms/step - loss: 0.2061 - val_loss: 0.2121 - lr: 1.2500e-04\n",
      "Epoch 187/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2062 - val_loss: 0.2122 - lr: 1.2500e-04\n",
      "Epoch 188/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2061 - val_loss: 0.2119 - lr: 1.2500e-04\n",
      "Epoch 189/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2061 - val_loss: 0.2119 - lr: 1.2500e-04\n",
      "Epoch 190/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2062 - val_loss: 0.2138 - lr: 1.2500e-04\n",
      "Epoch 191/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2060 - val_loss: 0.2120 - lr: 1.2500e-04\n",
      "Epoch 192/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2060 - val_loss: 0.2118 - lr: 1.2500e-04\n",
      "Epoch 193/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2062 - val_loss: 0.2118 - lr: 1.2500e-04\n",
      "Epoch 194/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2061 - val_loss: 0.2123 - lr: 1.2500e-04\n",
      "Epoch 195/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2059 - val_loss: 0.2126 - lr: 1.2500e-04\n",
      "Epoch 196/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2060 - val_loss: 0.2118 - lr: 6.2500e-05\n",
      "Epoch 197/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2058 - val_loss: 0.2119 - lr: 6.2500e-05\n",
      "Epoch 198/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2057 - val_loss: 0.2127 - lr: 6.2500e-05\n",
      "Epoch 199/1000\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.2059 - val_loss: 0.2118 - lr: 6.2500e-05\n",
      "Epoch 200/1000\n",
      "168/168 [==============================] - 3s 16ms/step - loss: 0.2057 - val_loss: 0.2119 - lr: 6.2500e-05\n",
      "Epoch 201/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2058 - val_loss: 0.2122 - lr: 6.2500e-05\n",
      "Epoch 202/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2057 - val_loss: 0.2127 - lr: 6.2500e-05\n",
      "Epoch 203/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2057 - val_loss: 0.2119 - lr: 6.2500e-05\n",
      "Epoch 204/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2058 - val_loss: 0.2121 - lr: 6.2500e-05\n",
      "Epoch 205/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2057 - val_loss: 0.2120 - lr: 6.2500e-05\n",
      "Epoch 206/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2057 - val_loss: 0.2120 - lr: 6.2500e-05\n",
      "Epoch 207/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2057 - val_loss: 0.2118 - lr: 6.2500e-05\n",
      "Epoch 208/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2057 - val_loss: 0.2119 - lr: 6.2500e-05\n",
      "Epoch 209/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2056 - val_loss: 0.2121 - lr: 6.2500e-05\n",
      "Epoch 210/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2056 - val_loss: 0.2119 - lr: 6.2500e-05\n",
      "Epoch 211/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2055 - val_loss: 0.2118 - lr: 3.1250e-05\n",
      "Epoch 212/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2056 - val_loss: 0.2120 - lr: 3.1250e-05\n",
      "Epoch 213/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2055 - val_loss: 0.2119 - lr: 3.1250e-05\n",
      "Epoch 214/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2056 - val_loss: 0.2123 - lr: 3.1250e-05\n",
      "Epoch 215/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2055 - val_loss: 0.2118 - lr: 3.1250e-05\n",
      "Epoch 216/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2056 - val_loss: 0.2123 - lr: 3.1250e-05\n",
      "Epoch 217/1000\n",
      "168/168 [==============================] - 3s 16ms/step - loss: 0.2055 - val_loss: 0.2119 - lr: 3.1250e-05\n",
      "2681/2681 [==============================] - 4s 2ms/step\n",
      "Fold 5 NN: 0.21167\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "RMSPE NN: Folds: [0.21173, 0.2136, 0.2192, 0.20973, 0.21167]\n"
     ]
    }
   ],
   "source": [
    "for fold, (trn_ind, val_ind) in enumerate(skf):\n",
    "    print(f'Training fold {fold + 1}')\n",
    "    \n",
    "    x_train = train.iloc[trn_ind].copy()\n",
    "    x_val = train.iloc[val_ind].copy()\n",
    "    print(x_train.shape)\n",
    "    print(x_val.shape)\n",
    "    y_train, y_val = y.iloc[trn_ind], y.iloc[val_ind]\n",
    "    tt = test.copy()\n",
    "    \n",
    "    #############################################################################################\n",
    "    # NN\n",
    "    #############################################################################################\n",
    "    \n",
    "    x_train.loc[:, feats_nostock] = x_train.loc[:, feats_nostock].fillna(x_train.groupby('stock_id')[feats_nostock].transform('median')).values\n",
    "    for i in x_val.stock_id.unique():\n",
    "        x_val.loc[x_val.stock_id==i, feats_nostock] = x_val.loc[x_val.stock_id==i, feats_nostock].fillna(x_train.loc[x_train.stock_id==i, feats_nostock].median()).values\n",
    "    for i in tt.stock_id.unique():\n",
    "        tt.loc[tt.stock_id==i, feats_nostock] = tt.loc[tt.stock_id==i, feats_nostock].fillna(x_train.loc[x_train.stock_id==i, feats_nostock].median()).values\n",
    "    \n",
    "    x_train_agg_time, agg_feats_time = get_time_agg(x_train)\n",
    "    x_train = x_train.merge(x_train_agg_time, how='left', on='time_id')\n",
    "    x_val_agg_time, _ = get_time_agg(x_val)\n",
    "    x_val = x_val.merge(x_val_agg_time, how='left', on='time_id')\n",
    "    test_agg_time, _ = get_time_agg(tt)\n",
    "    tt = tt.merge(test_agg_time, how='left', on='time_id')\n",
    "    del x_train_agg_time,  x_val_agg_time, test_agg_time\n",
    "    gc.collect()\n",
    "\n",
    "    traincols = feats_nostock+agg_feats_time\n",
    "\n",
    "    for i in tt.stock_id.unique():\n",
    "        tt.loc[tt.stock_id==i, traincols] = tt.loc[tt.stock_id==i, traincols].fillna(x_train.loc[x_train.stock_id==i, traincols].median()).values\n",
    "\n",
    "    num_trans = Pipeline([('qt', QuantileTransformer(n_quantiles=2000, output_distribution='normal')),\n",
    "                         ('numscaler', MinMaxScaler())])\n",
    "    agg_trans = Pipeline([('aggscaler', MinMaxScaler())])\n",
    "    preprocessor = ColumnTransformer(transformers=[('num', num_trans, feats_nostock),\n",
    "                                                    ('agg', agg_trans, agg_feats_time)])\n",
    "    pipe = Pipeline([('pp', preprocessor)])\n",
    "    pipe.fit(x_train[traincols])\n",
    "    \n",
    "    model = base_model(len(traincols))\n",
    "    model.compile(\n",
    "        keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss=root_mean_squared_per_error\n",
    "    )\n",
    "    \n",
    "    model.fit([x_train['stock_id'], pipe.transform(x_train[traincols])], \n",
    "              y_train,               \n",
    "              batch_size=2048,\n",
    "              epochs=1000,\n",
    "              validation_data=([x_val['stock_id'], pipe.transform(x_val[traincols])], y_val),\n",
    "              callbacks=[es, plateau],\n",
    "              validation_batch_size=len(y_val),\n",
    "              shuffle=True,\n",
    "              verbose = 1)\n",
    "\n",
    "    preds = model.predict([x_val['stock_id'], pipe.transform(x_val[traincols])]).reshape(1,-1)[0]\n",
    "    \n",
    "    score = round(rmspe(y_true = y_val, y_pred = preds), 5)\n",
    "    print('Fold {} {}: {}'.format(counter, model_name, score))\n",
    "    scores_folds[model_name].append(score)\n",
    "    \n",
    "    predictions_nn_1 += np.abs(model.predict([tt['stock_id'], pipe.transform(tt[traincols])]).reshape(1,-1)[0]) / kf\n",
    "    \n",
    "\n",
    "    counter += 1\n",
    "    \n",
    "print('RMSPE {}: Folds: {}'.format(model_name, scores_folds[model_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5b1ede36",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-25T06:00:34.687367Z",
     "iopub.status.busy": "2023-05-25T06:00:34.686754Z",
     "iopub.status.idle": "2023-05-25T06:00:35.250624Z",
     "shell.execute_reply": "2023-05-25T06:00:35.249467Z"
    },
    "papermill": {
     "duration": 5.063618,
     "end_time": "2023-05-25T06:00:35.256011",
     "exception": false,
     "start_time": "2023-05-25T06:00:30.192393",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6191"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d85782",
   "metadata": {
    "papermill": {
     "duration": 4.378745,
     "end_time": "2023-05-25T06:00:43.961803",
     "exception": false,
     "start_time": "2023-05-25T06:00:39.583058",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Tabnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "466e3df8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-25T06:00:52.755641Z",
     "iopub.status.busy": "2023-05-25T06:00:52.754725Z",
     "iopub.status.idle": "2023-05-25T06:03:02.352040Z",
     "shell.execute_reply": "2023-05-25T06:03:02.350111Z"
    },
    "papermill": {
     "duration": 133.959861,
     "end_time": "2023-05-25T06:03:02.355517",
     "exception": false,
     "start_time": "2023-05-25T06:00:48.395656",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /kaggle/input/optiver-pytorch-tabnet/torch-1.12.1-cp310-cp310-manylinux1_x86_64.whl\r\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch==1.12.1) (4.5.0)\r\n",
      "Installing collected packages: torch\r\n",
      "  Attempting uninstall: torch\r\n",
      "    Found existing installation: torch 2.0.0+cpu\r\n",
      "    Uninstalling torch-2.0.0+cpu:\r\n",
      "      Successfully uninstalled torch-2.0.0+cpu\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "torchvision 0.15.1+cpu requires torch==2.0.0, but you have torch 1.12.1 which is incompatible.\r\n",
      "torchtext 0.15.1+cpu requires torch==2.0.0, but you have torch 1.12.1 which is incompatible.\r\n",
      "torchdata 0.6.0 requires torch==2.0.0, but you have torch 1.12.1 which is incompatible.\r\n",
      "torchaudio 2.0.1+cpu requires torch==2.0.0, but you have torch 1.12.1 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed torch-1.12.1\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0mProcessing /kaggle/input/optiver-pytorch-tabnet/pytorch_tabnet-4.0-py3-none-any.whl\r\n",
      "Requirement already satisfied: numpy<2.0,>=1.17 in /opt/conda/lib/python3.10/site-packages (from pytorch-tabnet==4.0) (1.23.5)\r\n",
      "Requirement already satisfied: scikit_learn>0.21 in /opt/conda/lib/python3.10/site-packages (from pytorch-tabnet==4.0) (1.2.2)\r\n",
      "Requirement already satisfied: torch<2.0,>=1.2 in /opt/conda/lib/python3.10/site-packages (from pytorch-tabnet==4.0) (1.12.1)\r\n",
      "Requirement already satisfied: scipy>1.4 in /opt/conda/lib/python3.10/site-packages (from pytorch-tabnet==4.0) (1.9.3)\r\n",
      "Requirement already satisfied: tqdm<5.0,>=4.36 in /opt/conda/lib/python3.10/site-packages (from pytorch-tabnet==4.0) (4.64.1)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit_learn>0.21->pytorch-tabnet==4.0) (3.1.0)\r\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit_learn>0.21->pytorch-tabnet==4.0) (1.2.0)\r\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch<2.0,>=1.2->pytorch-tabnet==4.0) (4.5.0)\r\n",
      "Installing collected packages: pytorch-tabnet\r\n",
      "Successfully installed pytorch-tabnet-4.0\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install ../input/optiver-pytorch-tabnet/torch-1.12.1-cp310-cp310-manylinux1_x86_64.whl\n",
    "!pip install ../input/optiver-pytorch-tabnet/pytorch_tabnet-4.0-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5f11cae4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-25T06:03:10.967256Z",
     "iopub.status.busy": "2023-05-25T06:03:10.965661Z",
     "iopub.status.idle": "2023-05-25T10:37:34.163893Z",
     "shell.execute_reply": "2023-05-25T10:37:34.162134Z"
    },
    "papermill": {
     "duration": 16471.757906,
     "end_time": "2023-05-25T10:37:38.430728",
     "exception": false,
     "start_time": "2023-05-25T06:03:06.672822",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done 112 out of 112 | elapsed: 57.2min finished\n",
      "[Parallel(n_jobs=-1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 253.02449| val_0_rmspe: 124.59943|  0:00:58s\n",
      "epoch 10 | loss: 0.65357 | val_0_rmspe: 0.54652 |  0:10:51s\n",
      "epoch 20 | loss: 0.37874 | val_0_rmspe: 0.54944 |  0:20:58s\n",
      "epoch 30 | loss: 0.32257 | val_0_rmspe: 0.28653 |  0:31:15s\n",
      "epoch 40 | loss: 0.26039 | val_0_rmspe: 0.24408 |  0:42:01s\n",
      "epoch 50 | loss: 0.23874 | val_0_rmspe: 0.23217 |  0:52:35s\n",
      "epoch 60 | loss: 0.22602 | val_0_rmspe: 0.22507 |  1:03:21s\n",
      "epoch 70 | loss: 0.22255 | val_0_rmspe: 0.24207 |  1:14:22s\n",
      "epoch 80 | loss: 0.22114 | val_0_rmspe: 0.22504 |  1:25:03s\n",
      "epoch 90 | loss: 0.21669 | val_0_rmspe: 0.21559 |  1:35:22s\n",
      "epoch 100| loss: 0.21508 | val_0_rmspe: 0.21646 |  1:45:40s\n",
      "epoch 110| loss: 0.21177 | val_0_rmspe: 0.21233 |  1:56:00s\n",
      "epoch 120| loss: 0.20999 | val_0_rmspe: 0.21123 |  2:06:15s\n",
      "epoch 130| loss: 0.20886 | val_0_rmspe: 0.21032 |  2:16:08s\n",
      "epoch 140| loss: 0.20837 | val_0_rmspe: 0.20996 |  2:26:09s\n",
      "epoch 150| loss: 0.20749 | val_0_rmspe: 0.2096  |  2:36:14s\n",
      "epoch 160| loss: 0.20693 | val_0_rmspe: 0.20966 |  2:46:15s\n",
      "epoch 170| loss: 0.20646 | val_0_rmspe: 0.20955 |  2:56:30s\n",
      "epoch 180| loss: 0.20596 | val_0_rmspe: 0.20943 |  3:06:13s\n",
      "epoch 190| loss: 0.20612 | val_0_rmspe: 0.20917 |  3:16:38s\n",
      "Stop training because you reached max_epochs = 200 with best_epoch = 183 and best_val_0_rmspe = 0.20908\n",
      "Successfully saved model at ./tabnet.zip\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from joblib import Parallel, delayed\n",
    "from pytorch_tabnet.metrics import Metric\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def calc_wap1(df):\n",
    "    # 一价\n",
    "    wap = (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']) / (\n",
    "                df['bid_size1'] + df['ask_size1'])\n",
    "    return wap\n",
    "\n",
    "\n",
    "def calc_wap2(df):\n",
    "    # 二价\n",
    "    wap = (df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2']) / (\n",
    "                df['bid_size2'] + df['ask_size2'])\n",
    "    return wap\n",
    "\n",
    "\n",
    "def log_return(series):\n",
    "    # 对数回报\n",
    "    return np.log(series).diff()\n",
    "\n",
    "\n",
    "def realized_volatility(series):\n",
    "    return np.sqrt(np.sum(series ** 2))\n",
    "\n",
    "\n",
    "def count_unique(series):\n",
    "    # 计数\n",
    "    return len(np.unique(series))\n",
    "\n",
    "\n",
    "def book_preprocessor(file_path):\n",
    "    # 订单处理\n",
    "    df = pd.read_parquet(file_path)\n",
    "\n",
    "    df['wap1'] = calc_wap1(df)\n",
    "    df['wap2'] = calc_wap2(df)\n",
    "\n",
    "    df['log_return1'] = df.groupby(['time_id'],group_keys=False)['wap1'].apply(log_return)\n",
    "    df['log_return2'] = df.groupby(['time_id'],group_keys=False)['wap2'].apply(log_return)\n",
    "\n",
    "    df['wap_balance'] = abs(df['wap1'] - df['wap2'])\n",
    "\n",
    "    df['price_spread'] = (df['ask_price1'] - df['bid_price1']) / ((df['ask_price1'] + df['bid_price1']) / 2)\n",
    "    df['price_spread2'] = (df['ask_price2'] - df['bid_price2']) / ((df['ask_price2'] + df['bid_price2']) / 2)\n",
    "    df['bid_spread'] = df['bid_price1'] - df['bid_price2']\n",
    "    df['ask_spread'] = df['ask_price1'] - df['ask_price2']\n",
    "    df[\"bid_ask_spread\"] = abs(df['bid_spread'] - df['ask_spread'])\n",
    "    df['total_volume'] = (df['ask_size1'] + df['ask_size2']) + (df['bid_size1'] + df['bid_size2'])\n",
    "    df['volume_imbalance'] = abs((df['ask_size1'] + df['ask_size2']) - (df['bid_size1'] + df['bid_size2']))\n",
    "\n",
    "    create_feature_dict = {\n",
    "        'wap1': [np.sum, np.mean, np.std],\n",
    "        'wap2': [np.sum, np.mean, np.std],\n",
    "        'log_return1': [np.sum, realized_volatility, np.mean, np.std],\n",
    "        'log_return2': [np.sum, realized_volatility, np.mean, np.std],\n",
    "        'wap_balance': [np.sum, np.mean, np.std],\n",
    "        'price_spread': [np.sum, np.mean, np.std],\n",
    "        'price_spread2': [np.sum, np.mean, np.std],\n",
    "        'bid_spread': [np.sum, np.mean, np.std],\n",
    "        'ask_spread': [np.sum, np.mean, np.std],\n",
    "        'total_volume': [np.sum, np.mean, np.std],\n",
    "        'volume_imbalance': [np.sum, np.mean, np.std],\n",
    "        \"bid_ask_spread\": [np.sum, np.mean, np.std],\n",
    "    }\n",
    "\n",
    "    def get_stats_window(seconds_in_bucket, add_suffix=False):\n",
    "        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(\n",
    "            create_feature_dict).reset_index()\n",
    "        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n",
    "        if add_suffix:\n",
    "            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n",
    "        return df_feature\n",
    "\n",
    "    df_feature = get_stats_window(seconds_in_bucket=0, add_suffix=False)\n",
    "    df_feature_400 = get_stats_window(seconds_in_bucket=400, add_suffix=True)\n",
    "    df_feature_300 = get_stats_window(seconds_in_bucket=300, add_suffix=True)\n",
    "    df_feature_200 = get_stats_window(seconds_in_bucket=200, add_suffix=True)\n",
    "\n",
    "    df_feature = df_feature.merge(df_feature_400, how='left', left_on='time_id_', right_on='time_id__400')\n",
    "    df_feature = df_feature.merge(df_feature_300, how='left', left_on='time_id_', right_on='time_id__300')\n",
    "    df_feature = df_feature.merge(df_feature_200, how='left', left_on='time_id_', right_on='time_id__200')\n",
    "\n",
    "    df_feature.drop(['time_id__400', 'time_id__300', 'time_id__200'], axis=1, inplace=True)\n",
    "\n",
    "    stock_id = file_path.split('=')[1]\n",
    "    df_feature['row_id'] = df_feature['time_id_'].apply(lambda x: f'{stock_id}-{x}')\n",
    "    df_feature.drop(['time_id_'], axis=1, inplace=True)\n",
    "\n",
    "    return df_feature\n",
    "\n",
    "\n",
    "def trade_preprocessor(file_path):\n",
    "    df = pd.read_parquet(file_path)\n",
    "    df['log_return'] = df.groupby('time_id',group_keys=False)['price'].apply(log_return)\n",
    "\n",
    "    create_feature_dict = {\n",
    "        'log_return': [realized_volatility],\n",
    "        'seconds_in_bucket': [count_unique],\n",
    "        'size': [np.sum, realized_volatility, np.mean, np.std, np.max, np.min],\n",
    "        'order_count': [np.mean, np.sum, np.max],\n",
    "    }\n",
    "\n",
    "    def get_stats_window(seconds_in_bucket, add_suffix=False):\n",
    "        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(\n",
    "            create_feature_dict).reset_index()\n",
    "        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n",
    "        if add_suffix:\n",
    "            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n",
    "        return df_feature\n",
    "\n",
    "    df_feature = get_stats_window(seconds_in_bucket=0, add_suffix=False)\n",
    "    df_feature_400 = get_stats_window(seconds_in_bucket=400, add_suffix=True)\n",
    "    df_feature_300 = get_stats_window(seconds_in_bucket=300, add_suffix=True)\n",
    "    df_feature_200 = get_stats_window(seconds_in_bucket=200, add_suffix=True)\n",
    "\n",
    "    def tendency(price, vol):\n",
    "        df_diff = np.diff(price)\n",
    "        val = (df_diff / price[1:]) * 100\n",
    "        power = np.sum(val * vol[1:])\n",
    "        return (power)\n",
    "\n",
    "    lis = []\n",
    "    for n_time_id in df['time_id'].unique():\n",
    "        df_id = df[df['time_id'] == n_time_id]\n",
    "        tendencyV = tendency(df_id['price'].values, df_id['size'].values)\n",
    "        f_max = np.sum(df_id['price'].values > np.mean(df_id['price'].values))\n",
    "        f_min = np.sum(df_id['price'].values < np.mean(df_id['price'].values))\n",
    "        df_max = np.sum(np.diff(df_id['price'].values) > 0)\n",
    "        df_min = np.sum(np.diff(df_id['price'].values) < 0)\n",
    "        abs_diff = np.median(np.abs(df_id['price'].values - np.mean(df_id['price'].values)))\n",
    "        energy = np.mean(df_id['price'].values ** 2)\n",
    "        iqr_p = np.percentile(df_id['price'].values, 75) - np.percentile(df_id['price'].values, 25)\n",
    "        abs_diff_v = np.median(np.abs(df_id['size'].values - np.mean(df_id['size'].values)))\n",
    "        energy_v = np.sum(df_id['size'].values ** 2)\n",
    "        iqr_p_v = np.percentile(df_id['size'].values, 75) - np.percentile(df_id['size'].values, 25)\n",
    "\n",
    "        lis.append({'time_id': n_time_id, 'tendency': tendencyV, 'f_max': f_max, 'f_min': f_min, 'df_max': df_max,\n",
    "                    'df_min': df_min,\n",
    "                    'abs_diff': abs_diff, 'energy': energy, 'iqr_p': iqr_p, 'abs_diff_v': abs_diff_v,\n",
    "                    'energy_v': energy_v, 'iqr_p_v': iqr_p_v})\n",
    "\n",
    "    df_lr = pd.DataFrame(lis)\n",
    "\n",
    "    df_feature = df_feature.merge(df_lr, how='left', left_on='time_id_', right_on='time_id')\n",
    "\n",
    "    # Merge all\n",
    "    df_feature = df_feature.merge(df_feature_400, how='left', left_on='time_id_', right_on='time_id__400')\n",
    "    df_feature = df_feature.merge(df_feature_300, how='left', left_on='time_id_', right_on='time_id__300')\n",
    "    df_feature = df_feature.merge(df_feature_200, how='left', left_on='time_id_', right_on='time_id__200')\n",
    "\n",
    "    # Drop unnecesary time_ids\n",
    "    df_feature.drop(['time_id__400', 'time_id__300', 'time_id__200', 'time_id'], axis=1, inplace=True)\n",
    "    df_feature = df_feature.add_prefix('trade_')\n",
    "    stock_id = file_path.split('=')[1]\n",
    "    df_feature['row_id'] = df_feature['trade_time_id_'].apply(lambda x: f'{stock_id}-{x}')\n",
    "    df_feature.drop(['trade_time_id_'], axis=1, inplace=True)\n",
    "\n",
    "    def order_sum(df, sec: str):\n",
    "        new_col = 'size_tau' + sec\n",
    "        bucket_col = 'trade_seconds_in_bucket_count_unique' + sec\n",
    "        df[new_col] = np.sqrt(1 / df[bucket_col])\n",
    "\n",
    "        new_col2 = 'size_tau2' + sec\n",
    "        order_col = 'trade_order_count_sum' + sec\n",
    "        df[new_col2] = np.sqrt(1 / df[order_col])\n",
    "\n",
    "        if sec == '400_':\n",
    "            df['size_tau2_d'] = df['size_tau2_400'] - df['size_tau2']\n",
    "\n",
    "    for sec in ['', '_200', '_300', '_400']:\n",
    "        order_sum(df_feature, sec)\n",
    "\n",
    "    df_feature['size_tau2_d'] = df_feature['size_tau2_400'] - df_feature['size_tau2']\n",
    "\n",
    "    return df_feature\n",
    "\n",
    "\n",
    "def get_time_stock(df):\n",
    "    vol_cols = ['log_return1_realized_volatility', 'log_return2_realized_volatility',\n",
    "                'log_return1_realized_volatility_400', 'log_return2_realized_volatility_400',\n",
    "                'log_return1_realized_volatility_300', 'log_return2_realized_volatility_300',\n",
    "                'log_return1_realized_volatility_200', 'log_return2_realized_volatility_200',\n",
    "                'trade_log_return_realized_volatility', 'trade_log_return_realized_volatility_400',\n",
    "                'trade_log_return_realized_volatility_300', 'trade_log_return_realized_volatility_200']\n",
    "\n",
    "    df_stock_id = df.groupby(['stock_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n",
    "    df_stock_id.columns = ['_'.join(col) for col in df_stock_id.columns]\n",
    "    df_stock_id = df_stock_id.add_suffix('_' + 'stock')\n",
    "\n",
    "    df_time_id = df.groupby(['time_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n",
    "\n",
    "    df_time_id.columns = ['_'.join(col) for col in df_time_id.columns]\n",
    "    df_time_id = df_time_id.add_suffix('_' + 'time')\n",
    "\n",
    "    df = df.merge(df_stock_id, how='left', left_on=['stock_id'], right_on=['stock_id__stock'])\n",
    "    df = df.merge(df_time_id, how='left', left_on=['time_id'], right_on=['time_id__time'])\n",
    "    df.drop(['stock_id__stock', 'time_id__time'], axis=1, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def create_agg_features(train, test):\n",
    "    train_p = pd.read_csv('../input/optiver-realized-volatility-prediction/train.csv')\n",
    "    train_p = train_p.pivot(index='time_id', columns='stock_id', values='target')\n",
    "    corr = train_p.corr()\n",
    "    ids = corr.index\n",
    "    kmeans = KMeans(n_clusters=7, random_state=0).fit(corr.values)\n",
    "    l = []\n",
    "    for n in range(7):\n",
    "        l.append([(x - 1) for x in ((ids + 1) * (kmeans.labels_ == n)) if x > 0])\n",
    "\n",
    "    mat = []\n",
    "    mat_test = []\n",
    "    n = 0\n",
    "    for ind in l:\n",
    "        new_df = train.loc[train['stock_id'].isin(ind)]\n",
    "        new_df = new_df.groupby(['time_id']).agg(np.nanmean)\n",
    "        new_df.loc[:, 'stock_id'] = str(n) + 'c1'\n",
    "        mat.append(new_df)\n",
    "        new_df = test.loc[test['stock_id'].isin(ind)]\n",
    "        new_df = new_df.groupby(['time_id']).agg(np.nanmean)\n",
    "        new_df.loc[:, 'stock_id'] = str(n) + 'c1'\n",
    "        mat_test.append(new_df)\n",
    "        n += 1\n",
    "\n",
    "    mat1 = pd.concat(mat).reset_index()\n",
    "    mat1.drop(columns=['target'], inplace=True)\n",
    "    mat2 = pd.concat(mat_test).reset_index()\n",
    "\n",
    "    mat2 = pd.concat([mat2, mat1.loc[mat1.time_id == 5]])\n",
    "\n",
    "    mat1 = mat1.pivot(index='time_id', columns='stock_id')\n",
    "    mat1.columns = [\"_\".join(x) for x in mat1.columns.ravel()]\n",
    "    mat1.reset_index(inplace=True)\n",
    "\n",
    "    mat2 = mat2.pivot(index='time_id', columns='stock_id')\n",
    "    mat2.columns = [\"_\".join(x) for x in mat2.columns.ravel()]\n",
    "    mat2.reset_index(inplace=True)\n",
    "\n",
    "    prefix = ['log_return1_realized_volatility', 'total_volume_mean', 'trade_size_mean', 'trade_order_count_mean',\n",
    "              'price_spread_mean', 'bid_spread_mean', 'ask_spread_mean',\n",
    "              'volume_imbalance_mean', 'bid_ask_spread_mean', 'size_tau2']\n",
    "    selected_cols = mat1.filter(regex='|'.join(f'^{x}.(0|1|3|4|6)c1' for x in prefix)).columns.tolist()\n",
    "    selected_cols.append('time_id')\n",
    "\n",
    "    train_m = pd.merge(train, mat1[selected_cols], how='left', on='time_id')\n",
    "    test_m = pd.merge(test, mat2[selected_cols], how='left', on='time_id')\n",
    "\n",
    "    features = [col for col in train_m.columns.tolist() if col not in ['time_id', 'target', 'row_id']]\n",
    "    train_m[features] = train_m[features].fillna(train_m[features].mean())\n",
    "    test_m[features] = test_m[features].fillna(train_m[features].mean())\n",
    "\n",
    "    return train_m, test_m\n",
    "\n",
    "\n",
    "def preprocessor(list_stock_ids, is_train=True):\n",
    "    def for_joblib(stock_id):\n",
    "        if is_train:\n",
    "            file_path_book = data_dir + \"book_train.parquet/stock_id=\" + str(stock_id)\n",
    "            file_path_trade = data_dir + \"trade_train.parquet/stock_id=\" + str(stock_id)\n",
    "        else:\n",
    "            file_path_book = data_dir + \"book_test.parquet/stock_id=\" + str(stock_id)\n",
    "            file_path_trade = data_dir + \"trade_test.parquet/stock_id=\" + str(stock_id)\n",
    "\n",
    "        df_tmp = pd.merge(book_preprocessor(file_path_book), trade_preprocessor(file_path_trade), on='row_id',\n",
    "                          how='left')\n",
    "\n",
    "        return df_tmp\n",
    "\n",
    "    df = Parallel(n_jobs=-1, verbose=1)(delayed(for_joblib)(stock_id) for stock_id in list_stock_ids)\n",
    "    df = pd.concat(df, ignore_index=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "train = pd.read_csv('../input/optiver-realized-volatility-prediction/train.csv')\n",
    "test = pd.read_csv('../input/optiver-realized-volatility-prediction/test.csv')\n",
    "\n",
    "train['row_id'] = train['stock_id'].astype(str) + '-' + train['time_id'].astype(str)\n",
    "test['row_id'] = test['stock_id'].astype(str) + '-' + test['time_id'].astype(str)\n",
    "\n",
    "# data directory\n",
    "data_dir = '../input/optiver-realized-volatility-prediction/'\n",
    "\n",
    "train_stock_ids = train['stock_id'].unique()\n",
    "train_ = preprocessor(train_stock_ids, is_train=True)\n",
    "train = train.merge(train_, on=['row_id'], how='left')\n",
    "train.to_csv('train.csv', index=False)\n",
    "\n",
    "test_stock_ids = test['stock_id'].unique()\n",
    "test_ = preprocessor(test_stock_ids, is_train=False)\n",
    "test = test.merge(test_, on=['row_id'], how='left')\n",
    "\n",
    "train = get_time_stock(train)\n",
    "test = get_time_stock(test)\n",
    "\n",
    "train.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "test.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "train.to_csv('train_tab.csv', index=False)\n",
    "# train = pd.read_csv(f'../input/optiver-tabnet-kmeans-model-csv/train.csv')\n",
    "\n",
    "\n",
    "train, test = create_agg_features(train, test)\n",
    "\n",
    "X = train.drop(['row_id', 'target', 'time_id'], axis=1)\n",
    "y = train['target']\n",
    "X_test = test.copy()\n",
    "X_test.drop(['time_id', 'row_id'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "def rmspe(y_true, y_pred):\n",
    "    return np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n",
    "\n",
    "\n",
    "class RmspeClz(Metric):\n",
    "    def __init__(self):\n",
    "        self._name = \"rmspe\"\n",
    "        self._maximize = False\n",
    "\n",
    "    def __call__(self, y_true, y_score):\n",
    "        return np.sqrt(np.mean(np.square((y_true - y_score) / y_true)))\n",
    "\n",
    "\n",
    "def RmspeLoss(y_pred, y_true):\n",
    "    return torch.sqrt(torch.mean(((y_true - y_pred) / y_true) ** 2)).clone()\n",
    "\n",
    "\n",
    "nunique = X.nunique()\n",
    "types = X.dtypes\n",
    "\n",
    "categorical_columns = []\n",
    "categorical_dims = {}\n",
    "\n",
    "for col in X.columns:\n",
    "    if col == 'stock_id':\n",
    "        l_enc = LabelEncoder()\n",
    "        X[col] = l_enc.fit_transform(X[col].values)\n",
    "        X_test[col] = l_enc.transform(X_test[col].values)\n",
    "        categorical_columns.append(col)\n",
    "        categorical_dims[col] = len(l_enc.classes_)\n",
    "    else:\n",
    "        scaler = StandardScaler()\n",
    "        X[col] = scaler.fit_transform(X[col].values.reshape(-1, 1))\n",
    "        X_test[col] = scaler.transform(X_test[col].values.reshape(-1, 1))\n",
    "\n",
    "cat_idxs = [i for i, f in enumerate(X.columns.tolist()) if f in categorical_columns]\n",
    "cat_dims = [categorical_dims[f] for i, f in enumerate(X.columns.tolist()) if f in categorical_columns]\n",
    "\n",
    "tabnet_params = dict(\n",
    "    cat_idxs=cat_idxs,\n",
    "    cat_dims=cat_dims,\n",
    "    cat_emb_dim=1,\n",
    "    n_d=16,\n",
    "    n_a=16,\n",
    "    n_steps=2,\n",
    "    gamma=2,\n",
    "    n_independent=2,\n",
    "    n_shared=2,\n",
    "    lambda_sparse=0,\n",
    "    optimizer_fn=Adam,\n",
    "    optimizer_params=dict(lr=(2e-2)),\n",
    "    mask_type=\"entmax\",\n",
    "    scheduler_params=dict(T_0=200, T_mult=1, eta_min=1e-4, last_epoch=-1, verbose=False),\n",
    "    scheduler_fn=CosineAnnealingWarmRestarts,\n",
    "    seed=42,\n",
    "    verbose=10\n",
    ")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gc\n",
    "gc.collect()\n",
    "# 划分训练集和验证集\n",
    "X_train, X_val, y_train, y_val = train_test_split(X.values, y.values.reshape(-1, 1), test_size=0.3, random_state=2023)\n",
    "\n",
    "clf = TabNetRegressor(**tabnet_params)\n",
    "clf.fit(\n",
    "  X_train, y_train,\n",
    " eval_set=[(X_val, y_val)],\n",
    "  max_epochs=200,\n",
    "  patience=50,\n",
    "  batch_size=1024 * 20,\n",
    "  virtual_batch_size=128 * 20,\n",
    "  num_workers=4,\n",
    "  drop_last=False,\n",
    "  eval_metric=[RmspeClz],\n",
    "  loss_fn=RmspeLoss\n",
    ")\n",
    "\n",
    "saving_path_name = \"./tabnet\"\n",
    "saved_filepath = clf.save_model(saving_path_name)\n",
    "\n",
    "#     clf = TabNetRegressor()\n",
    "#     clf.load_model(f'../input/optiver-tabnet-kmeans-model-csv/fold{fold}')\n",
    "\n",
    "test_predictions = clf.predict(X_test.values).flatten()\n",
    "test_predictions = np.abs(test_predictions)\n",
    "\n",
    "# kfold = KFold(n_splits=5, random_state=42, shuffle=True)\n",
    "# oof_predictions = np.zeros((X.shape[0], 1))\n",
    "# test_predictions = np.zeros(X_test.shape[0])\n",
    "\n",
    "# for fold, (trn_ind, val_ind) in enumerate(kfold.split(X)):\n",
    "#     print(f'Training fold {fold + 1}')\n",
    "#     X_train, X_val = X.iloc[trn_ind].values, X.iloc[val_ind].values\n",
    "#     y_train, y_val = y.iloc[trn_ind].values.reshape(-1, 1), y.iloc[val_ind].values.reshape(-1, 1)\n",
    "\n",
    "#     clf = TabNetRegressor(**tabnet_params)\n",
    "#     clf.fit(\n",
    "#       X_train, y_train,\n",
    "#      eval_set=[(X_val, y_val)],\n",
    "#       max_epochs=200,\n",
    "#       patience=50,\n",
    "#       batch_size=1024 * 20,\n",
    "#       virtual_batch_size=128 * 20,\n",
    "#       num_workers=4,\n",
    "#       drop_last=False,\n",
    "#       eval_metric=[RmspeClz],\n",
    "#       loss_fn=RmspeLoss\n",
    "#     )\n",
    "\n",
    "#     saving_path_name = f\"./fold{fold}\"\n",
    "#     saved_filepath = clf.save_model(saving_path_name)\n",
    "\n",
    "# #     clf = TabNetRegressor()\n",
    "# #     clf.load_model(f'../input/optiver-tabnet-kmeans-model-csv/fold{fold}')\n",
    "\n",
    "#     oof_predictions[val_ind] = clf.predict(X_val)\n",
    "#     test_predictions += clf.predict(X_test.values).flatten() / 5\n",
    "\n",
    "# print(rmspe(y, oof_predictions.flatten()))\n",
    "\n",
    "test_predictions_tabnet = test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a032eacf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-25T10:37:46.993409Z",
     "iopub.status.busy": "2023-05-25T10:37:46.992930Z",
     "iopub.status.idle": "2023-05-25T10:37:47.382393Z",
     "shell.execute_reply": "2023-05-25T10:37:47.381197Z"
    },
    "papermill": {
     "duration": 4.665341,
     "end_time": "2023-05-25T10:37:47.385488",
     "exception": false,
     "start_time": "2023-05-25T10:37:42.720147",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2370d55e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-25T10:37:55.959459Z",
     "iopub.status.busy": "2023-05-25T10:37:55.958755Z",
     "iopub.status.idle": "2023-05-25T10:37:55.984799Z",
     "shell.execute_reply": "2023-05-25T10:37:55.983703Z"
    },
    "papermill": {
     "duration": 4.326993,
     "end_time": "2023-05-25T10:37:55.987678",
     "exception": false,
     "start_time": "2023-05-25T10:37:51.660685",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test['target'] = 0.25*test_predictions_l+0.25*test_predictions_c + 0.25*predictions_nn_1+0.25*test_predictions_tabnet\n",
    "test[['row_id', 'target']].to_csv('submission.csv',index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 23019.767689,
   "end_time": "2023-05-25T10:38:03.551367",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-05-25T04:14:23.783678",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
